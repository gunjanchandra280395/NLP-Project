{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTLK Project: Analysis of Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this project is to design an approach that makes use of Google and msn snippet in order to compute the semantic similarity between sentences. Given two sentences S1 and S2, the key is to input each of the sentences to the search engine and investigate the overlapping that may exist between the generated snippets. \n",
    "\n",
    "Seminar report date: 11.12.2018.\n",
    "Project delivery deadline: 7.1.2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tDefine two sentences S1 and S2.\n",
    "\n",
    "  S1: \"Several research groups have discovered new pharmaceuticals from nordic berries.\"\n",
    "  \n",
    "  S2: \"New substances discovered from fruits, vegetables and berries have positive health effects.\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Academic sentence - short example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Keywords and their placing versus highly defined featured snippets from Google are more important for getting traffic on webpage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to build up super snippets for new web content?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Webpage will earn new featured snippets from Google if it includes the same keywords like competitors high ranked featured snippets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Most of the regions in Finland will enjoy white snow and temperatures below freezing point on Christmas Eve.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      Academic sentence - short example\n",
       "0  Keywords and their placing versus highly defined featured snippets from Google are more important for getting traffic on webpage.   \n",
       "1  How to build up super snippets for new web content?                                                                                 \n",
       "2  Webpage will earn new featured snippets from Google if it includes the same keywords like competitors high ranked featured snippets.\n",
       "3  Most of the regions in Finland will enjoy white snow and temperatures below freezing point on Christmas Eve.                        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lower case sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keywords and their placing versus highly defined featured snippets from google are more important for getting traffic on webpage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how to build up super snippets for new web content?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>webpage will earn new featured snippets from google if it includes the same keywords like competitors high ranked featured snippets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>most of the regions in finland will enjoy white snow and temperatures below freezing point on christmas eve.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                    Lower case sentence\n",
       "0  keywords and their placing versus highly defined featured snippets from google are more important for getting traffic on webpage.   \n",
       "1  how to build up super snippets for new web content?                                                                                 \n",
       "2  webpage will earn new featured snippets from google if it includes the same keywords like competitors high ranked featured snippets.\n",
       "3  most of the regions in finland will enjoy white snow and temperatures below freezing point on christmas eve.                        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence tokenized into words   - string form and comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keywords, and, their, placing, versus, highly, defined, featured, snippets, from, google, are, more, important, for, getting, traffic, on, webpage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how, to, build, up, super, snippets, for, new, web, content?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>webpage, will, earn, new, featured, snippets, from, google, if, it, includes, the, same, keywords, like, competitors, high, ranked, featured, snippets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>most, of, the, regions, in, finland, will, enjoy, white, snow, and, temperatures, below, freezing, point, on, christmas, eve.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             Sentence tokenized into words   - string form and comma separated for display\n",
       "0  keywords, and, their, placing, versus, highly, defined, featured, snippets, from, google, are, more, important, for, getting, traffic, on, webpage.    \n",
       "1  how, to, build, up, super, snippets, for, new, web, content?                                                                                           \n",
       "2  webpage, will, earn, new, featured, snippets, from, google, if, it, includes, the, same, keywords, like, competitors, high, ranked, featured, snippets.\n",
       "3  most, of, the, regions, in, finland, will, enjoy, white, snow, and, temperatures, below, freezing, point, on, christmas, eve.                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "import math \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "pd.set_option('display.max_columns', None)      # or 1000\n",
    "pd.set_option('display.max_rows', None)         # or 1000\n",
    "pd.set_option('display.max_colwidth', -1)       # or 199\n",
    "\n",
    "\n",
    "\n",
    "# snippets / sentences examples ##########\n",
    "\n",
    "document_0 = \"Keywords and their placing versus highly defined featured snippets from Google are more important for getting traffic on webpage.\"\n",
    "document_1 = \"How to build up super snippets for new web content?\"\n",
    "document_2 = \"Webpage will earn new featured snippets from Google if it includes the same keywords like competitors high ranked featured snippets.\"\n",
    "document_3 = \"Most of the regions in Finland will enjoy white snow and temperatures below freezing point on Christmas Eve.\"\n",
    "\n",
    "all_documents = [document_0, document_1, document_2, document_3]\n",
    "\n",
    "data_all = pd.DataFrame(all_documents)\n",
    "data_all.columns = ['Academic sentence - short example']\n",
    "display(data_all.head())  \n",
    "\n",
    "low_documents = []\n",
    "for document in all_documents:\n",
    "    low_documents.append(document.lower())\n",
    "    \n",
    "data_low = pd.DataFrame(low_documents)\n",
    "data_low.columns = ['Lower case sentence']\n",
    "display(data_low.head())  \n",
    "    \n",
    "# tokenization by split # Sentences Tokenized into Words - split by whitespace\n",
    "\n",
    "sentences_documents = []\n",
    "#document_counter = 0\n",
    "for document in low_documents:\n",
    "    sentences_documents.append(document.split())\n",
    "\n",
    "printableList1 = []\n",
    "for sentence1 in sentences_documents:\n",
    "    sentence1AsString = ''\n",
    "    for idx1, aWord1 in enumerate(sentence1):        \n",
    "        if idx1 == len(sentence1) - 1:\n",
    "            sentence1AsString = sentence1AsString + aWord1\n",
    "        else:\n",
    "            str1 = aWord1 + ', '\n",
    "            sentence1AsString = sentence1AsString + str1\n",
    "    printableList1.append(sentence1AsString)\n",
    "\n",
    "data_sentences1 = pd.DataFrame(printableList1)\n",
    "data_sentences1.columns = ['Sentence tokenized into words   - string form and comma separated for display']\n",
    "display(data_sentences1.head())     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single words\n",
      "\n",
      "[['keywords', 'and', 'their', 'placing', 'versus', 'highly', 'defined', 'featured', 'snippets', 'from', 'google', 'are', 'more', 'important', 'for', 'getting', 'traffic', 'on', 'webpage.'], ['how', 'to', 'build', 'up', 'super', 'snippets', 'for', 'new', 'web', 'content?'], ['webpage', 'will', 'earn', 'new', 'featured', 'snippets', 'from', 'google', 'if', 'it', 'includes', 'the', 'same', 'keywords', 'like', 'competitors', 'high', 'ranked', 'featured', 'snippets.'], ['most', 'of', 'the', 'regions', 'in', 'finland', 'will', 'enjoy', 'white', 'snow', 'and', 'temperatures', 'below', 'freezing', 'point', 'on', 'christmas', 'eve.']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Single words   - string form and comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keywords, and, their, placing, versus, highly, defined, featured, snippets, from, google, are, more, important, for, getting, traffic, on, webpage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how, to, build, up, super, snippets, for, new, web, content?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>webpage, will, earn, new, featured, snippets, from, google, if, it, includes, the, same, keywords, like, competitors, high, ranked, featured, snippets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>most, of, the, regions, in, finland, will, enjoy, white, snow, and, temperatures, below, freezing, point, on, christmas, eve.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              Single words   - string form and comma separated for display\n",
       "0  keywords, and, their, placing, versus, highly, defined, featured, snippets, from, google, are, more, important, for, getting, traffic, on, webpage.    \n",
       "1  how, to, build, up, super, snippets, for, new, web, content?                                                                                           \n",
       "2  webpage, will, earn, new, featured, snippets, from, google, if, it, includes, the, same, keywords, like, competitors, high, ranked, featured, snippets.\n",
       "3  most, of, the, regions, in, finland, will, enjoy, white, snow, and, temperatures, below, freezing, point, on, christmas, eve.                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized with alphabetic chars only\n",
      "\n",
      "[['keywords', 'and', 'their', 'placing', 'versus', 'highly', 'defined', 'featured', 'snippets', 'from', 'google', 'are', 'more', 'important', 'for', 'getting', 'traffic', 'on', 'webpage'], ['how', 'to', 'build', 'up', 'super', 'snippets', 'for', 'new', 'web', 'content'], ['webpage', 'will', 'earn', 'new', 'featured', 'snippets', 'from', 'google', 'if', 'it', 'includes', 'the', 'same', 'keywords', 'like', 'competitors', 'high', 'ranked', 'featured', 'snippets'], ['most', 'of', 'the', 'regions', 'in', 'finland', 'will', 'enjoy', 'white', 'snow', 'and', 'temperatures', 'below', 'freezing', 'point', 'on', 'christmas', 'eve']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized with alphabetic chars only   - string form and comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keywords, and, their, placing, versus, highly, defined, featured, snippets, from, google, are, more, important, for, getting, traffic, on, webpage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how, to, build, up, super, snippets, for, new, web, content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>webpage, will, earn, new, featured, snippets, from, google, if, it, includes, the, same, keywords, like, competitors, high, ranked, featured, snippets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>most, of, the, regions, in, finland, will, enjoy, white, snow, and, temperatures, below, freezing, point, on, christmas, eve</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     Tokenized with alphabetic chars only   - string form and comma separated for display\n",
       "0  keywords, and, their, placing, versus, highly, defined, featured, snippets, from, google, are, more, important, for, getting, traffic, on, webpage    \n",
       "1  how, to, build, up, super, snippets, for, new, web, content                                                                                           \n",
       "2  webpage, will, earn, new, featured, snippets, from, google, if, it, includes, the, same, keywords, like, competitors, high, ranked, featured, snippets\n",
       "3  most, of, the, regions, in, finland, will, enjoy, white, snow, and, temperatures, below, freezing, point, on, christmas, eve                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English stopwords filtered tokens\n",
      "\n",
      "[['keywords', 'placing', 'versus', 'highly', 'defined', 'featured', 'snippets', 'google', 'important', 'getting', 'traffic', 'webpage'], ['build', 'super', 'snippets', 'new', 'web', 'content'], ['webpage', 'earn', 'new', 'featured', 'snippets', 'google', 'includes', 'keywords', 'like', 'competitors', 'high', 'ranked', 'featured', 'snippets'], ['regions', 'finland', 'enjoy', 'white', 'snow', 'temperatures', 'freezing', 'point', 'christmas', 'eve']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English stopwords filtered tokens   - comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keywords, placing, versus, highly, defined, featured, snippets, google, important, getting, traffic, webpage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>build, super, snippets, new, web, content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>webpage, earn, new, featured, snippets, google, includes, keywords, like, competitors, high, ranked, featured, snippets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>regions, finland, enjoy, white, snow, temperatures, freezing, point, christmas, eve</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         English stopwords filtered tokens   - comma separated for display\n",
       "0  keywords, placing, versus, highly, defined, featured, snippets, google, important, getting, traffic, webpage           \n",
       "1  build, super, snippets, new, web, content                                                                              \n",
       "2  webpage, earn, new, featured, snippets, google, includes, keywords, like, competitors, high, ranked, featured, snippets\n",
       "3  regions, finland, enjoy, white, snow, temperatures, freezing, point, christmas, eve                                    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Stemming by PorterStemmer\n",
      "\n",
      "[['keyword', 'place', 'versu', 'highli', 'defin', 'featur', 'snippet', 'googl', 'import', 'get', 'traffic', 'webpag'], ['build', 'super', 'snippet', 'new', 'web', 'content'], ['webpag', 'earn', 'new', 'featur', 'snippet', 'googl', 'includ', 'keyword', 'like', 'competitor', 'high', 'rank', 'featur', 'snippet'], ['region', 'finland', 'enjoy', 'white', 'snow', 'temperatur', 'freez', 'point', 'christma', 'eve']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Stemming by PorterStemmer   - comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>keyword, place, versu, highli, defin, featur, snippet, googl, import, get, traffic, webpag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>build, super, snippet, new, web, content</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>webpag, earn, new, featur, snippet, googl, includ, keyword, like, competitor, high, rank, featur, snippet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>region, finland, enjoy, white, snow, temperatur, freez, point, christma, eve</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Word Stemming by PorterStemmer   - comma separated for display\n",
       "0  keyword, place, versu, highli, defin, featur, snippet, googl, import, get, traffic, webpag               \n",
       "1  build, super, snippet, new, web, content                                                                 \n",
       "2  webpag, earn, new, featur, snippet, googl, includ, keyword, like, competitor, high, rank, featur, snippet\n",
       "3  region, finland, enjoy, white, snow, temperatur, freez, point, christma, eve                             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# change compound words to separate words ie. 'conditional-statements' -> 'conditional', 'statements' \n",
    "print(\"\\n\" 'Single words' \"\\n\")\n",
    "single_word_documents = []\n",
    "for sentence_words in sentences_documents:\n",
    "    single_word_list = []\n",
    "    for word in sentence_words:\n",
    "        regex = re.compile(\"[-_]\")\n",
    "        trimmed = regex.sub(' ', word)\n",
    "        separate = trimmed.split( )\n",
    "        for item in separate:\n",
    "            single_word_list.append(item)        \n",
    "    single_word_documents.append(single_word_list)\n",
    "print(single_word_documents)\n",
    "\n",
    "printableList2 = []\n",
    "for sentence2 in single_word_documents:\n",
    "    sentence2AsString = ''\n",
    "    for idx2, aWord2 in enumerate(sentence2):        \n",
    "        if idx2 == len(sentence2) - 1:\n",
    "            sentence2AsString = sentence2AsString + aWord2\n",
    "        else:\n",
    "            str2 = aWord2 + ', '\n",
    "            sentence2AsString = sentence2AsString + str2\n",
    "    printableList2.append(sentence2AsString)\n",
    "\n",
    "data_sentences2 = pd.DataFrame(printableList2)\n",
    "data_sentences2.columns = ['Single words   - string form and comma separated for display']\n",
    "display(data_sentences2.head())      \n",
    "    \n",
    "    \n",
    "    \n",
    "# remove all tokens that are not alphabetic #############\n",
    "print(\"\\n\" 'Tokenized with alphabetic chars only' \"\\n\")\n",
    "alpha_documents = []\n",
    "for single_word_sentence in single_word_documents:\n",
    "    cleaned_list = []\n",
    "    for single_word in single_word_sentence:\n",
    "        regex = re.compile('[^a-zA-Z]')\n",
    "        #First parameter is the replacement, second parameter is your input string\n",
    "        nonAlphaRemoved = regex.sub('', single_word)\n",
    "        # add string to list only if it has content\n",
    "        if nonAlphaRemoved:\n",
    "            cleaned_list.append(nonAlphaRemoved)\n",
    "    alpha_documents.append(cleaned_list)\n",
    "print(alpha_documents)\n",
    "\n",
    "printableList3 = []\n",
    "for sentence3 in alpha_documents:\n",
    "    sentence3AsString = ''\n",
    "    for idx3, aWord3 in enumerate(sentence3):        \n",
    "        if idx3 == len(sentence3) - 1:\n",
    "            sentence3AsString = sentence3AsString + aWord3\n",
    "        else:\n",
    "            str3 = aWord3 + ', '\n",
    "            sentence3AsString = sentence3AsString + str3\n",
    "    printableList3.append(sentence3AsString)\n",
    "\n",
    "data_sentences3 = pd.DataFrame(printableList3)\n",
    "data_sentences3.columns = ['Tokenized with alphabetic chars only   - string form and comma separated for display']\n",
    "display(data_sentences3.head())     \n",
    "\n",
    "\n",
    "# filter out stopwords ########\n",
    "print(\"\\n\" 'English stopwords filtered tokens' \"\\n\")\n",
    "stop_filtered_tokens = []\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for fword in alpha_documents:\n",
    "    fword_list = []\n",
    "    for sword in fword:\n",
    "        #fword_list = [sword for sword in alpha_documents if not sword in english_stop_words]\n",
    "        if not sword in english_stop_words:\n",
    "            fword_list.append(sword)\n",
    "    stop_filtered_tokens.append(fword_list)\n",
    "print(stop_filtered_tokens)  \n",
    "\n",
    "\n",
    "printableList4 = []\n",
    "for sentence4 in stop_filtered_tokens:\n",
    "    sentence4AsString = ''\n",
    "    for idx4, aWord4 in enumerate(sentence4):        \n",
    "        if idx4 == len(sentence4) - 1:\n",
    "            sentence4AsString = sentence4AsString + aWord4\n",
    "        else:\n",
    "            str4 = aWord4 + ', '\n",
    "            sentence4AsString = sentence4AsString + str4\n",
    "    printableList4.append(sentence4AsString)\n",
    "\n",
    "data_sentences4 = pd.DataFrame(printableList4)\n",
    "data_sentences4.columns = ['English stopwords filtered tokens   - comma separated for display']\n",
    "display(data_sentences4.head())     \n",
    "\n",
    "\n",
    "# tokenization by PorterStemmer ############\n",
    "print(\"\\n\" 'Word Stemming by PorterStemmer' \"\\n\")\n",
    "porter_documents = []\n",
    "for ps_word_list in stop_filtered_tokens:\n",
    "    PS = PorterStemmer()\n",
    "    porter_list = []\n",
    "    for ps_word in ps_word_list:\n",
    "        porter_list.append(PS.stem(ps_word))\n",
    "    porter_documents.append(porter_list)\n",
    "print(porter_documents)\n",
    "\n",
    "printableList5 = []\n",
    "for sentence5 in porter_documents:\n",
    "    sentence5AsString = ''\n",
    "    for idx5, aWord5 in enumerate(sentence5):        \n",
    "        if idx5 == len(sentence5) - 1:\n",
    "            sentence5AsString = sentence5AsString + aWord5\n",
    "        else:\n",
    "            str5 = aWord5 + ', '\n",
    "            sentence5AsString = sentence5AsString + str5\n",
    "    printableList5.append(sentence5AsString)\n",
    "\n",
    "data_sentences5 = pd.DataFrame(printableList5)\n",
    "data_sentences5.columns = ['Word Stemming by PorterStemmer   - comma separated for display']\n",
    "display(data_sentences5.head())     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard similarity\n",
      "0.0588\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First Index</th>\n",
       "      <th>First Sentence</th>\n",
       "      <th>Second Index</th>\n",
       "      <th>Second Sentence</th>\n",
       "      <th>Jaccard similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>keyword, place, versu, highli, defin, featur, snippet, googl, import, get, traffic, webpag</td>\n",
       "      <td>1</td>\n",
       "      <td>[build, super, snippet, new, web, content]</td>\n",
       "      <td>0.0588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>keyword, place, versu, highli, defin, featur, snippet, googl, import, get, traffic, webpag</td>\n",
       "      <td>2</td>\n",
       "      <td>[webpag, earn, new, featur, snippet, googl, includ, keyword, like, competitor, high, rank, featur, snippet]</td>\n",
       "      <td>0.2632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>keyword, place, versu, highli, defin, featur, snippet, googl, import, get, traffic, webpag</td>\n",
       "      <td>3</td>\n",
       "      <td>[region, finland, enjoy, white, snow, temperatur, freez, point, christma, eve]</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   First Index  \\\n",
       "0  0             \n",
       "1  0             \n",
       "2  0             \n",
       "\n",
       "                                                                               First Sentence  \\\n",
       "0  keyword, place, versu, highli, defin, featur, snippet, googl, import, get, traffic, webpag   \n",
       "1  keyword, place, versu, highli, defin, featur, snippet, googl, import, get, traffic, webpag   \n",
       "2  keyword, place, versu, highli, defin, featur, snippet, googl, import, get, traffic, webpag   \n",
       "\n",
       "   Second Index  \\\n",
       "0  1              \n",
       "1  2              \n",
       "2  3              \n",
       "\n",
       "                                                                                               Second Sentence  \\\n",
       "0  [build, super, snippet, new, web, content]                                                                    \n",
       "1  [webpag, earn, new, featur, snippet, googl, includ, keyword, like, competitor, high, rank, featur, snippet]   \n",
       "2  [region, finland, enjoy, white, snow, temperatur, freez, point, christma, eve]                                \n",
       "\n",
       "  Jaccard similarity  \n",
       "0  0.0588             \n",
       "1  0.2632             \n",
       "2  0.0000             "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define jaccard similarity for python ################\n",
    "def jaccard_similarity(query, jdoc):\n",
    "    intersection = set(query).intersection(set(jdoc))\n",
    "    union = set(query).union(set(jdoc))\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "# calculate jaccard similarity\n",
    "print('Jaccard similarity')\n",
    "result = jaccard_similarity(porter_documents[0], porter_documents[1])\n",
    "j_string = \"{:.4f}\".format(result)\n",
    "print(j_string)\n",
    "\n",
    "\n",
    "#data_table1 = pd.DataFrame(tableJaccSim1)\n",
    "# data_table1.columns = ['n', 'sentence', 'n', 'sentence', 'JaccardSim'  ]\n",
    "#display(data_table1.head())\n",
    "\n",
    "def listToString (sourceList):\n",
    "    subListAsString = ''    \n",
    "    for listIndex, listWord in enumerate(sourceList):        \n",
    "        if listIndex == len(sourceList) - 1:\n",
    "            subListAsString = subListAsString + listWord\n",
    "        else:\n",
    "            strWithComma = listWord + ', '\n",
    "            subListAsString = subListAsString + strWithComma        \n",
    "    return subListAsString\n",
    "\n",
    "# compare the first porter document to the rest in the porter docs list\n",
    "def printJaccardSimilarities (porterDocs):    \n",
    "    printableJaccardList = []    \n",
    "    for porterIndex, porter in enumerate(porterDocs):\n",
    "        if porterIndex > 0:            \n",
    "            jresult = jaccard_similarity(porterDocs[0], porter)    \n",
    "            j_string = \"{:.4f}\".format(jresult)\n",
    "            porterParamStr1 = listToString(porterDocs[0])\n",
    "            porterParamStr2 = listToString(porter)\n",
    "            data = [0, porterParamStr1, porterIndex, porter, j_string]\n",
    "            printableJaccardList.append(data)\n",
    "    df = pd.DataFrame(printableJaccardList,columns=['First Index','First Sentence','Second Index','Second Sentence','Jaccard similarity'])\n",
    "    display(df.head()) \n",
    "\n",
    "printJaccardSimilarities(porter_documents)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tUse Google search API and msn search API to generate the first ten snippets associated to each sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Google search snippets for search terms : Keywords and their placing versus highly defined featured snippets from Google are more important for getting traffic on webpage. : Total Result count : 54600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aug 23, 2017 ... You have chances to get featured for the terms your pages are already ... health \\nor finance, you have the highest probability of getting featured ... When \\nperforming keyword research with featured snippets in mind, note that: .... queries \\ndetermine the subheadings of the article and thus define its structure.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>May 29, 2017 ... Google is actively rolling out featured snippets for a wide variety of search \\nqueries because they ... I mean, let's say that you rank #7 for some keyword. ... \\nBut if there's a featured snippet opportunity, you can get there almost instantly \\nwith just a ... They occupy more real estate, and they clearly catch the eye.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aug 15, 2018 ... The Google featured snippet was first designed when the Google ... a highlighted \\nbox that sometimes appears at the very top of a Google ... types in a question or \\nkeywords that Google recognizes as a question. ... featured-snippets-traffic ... \\nglance would mean millions of more eyes on your website's name.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feb 14, 2018 ... A website's traffic reflects how well a business is doing online. It is also an ... To \\nbe successful at this, you need to understand how Google defines better content. \\n... (PS: doing this well will increase your ability to get featured snippets too) ... \\nThis is the most important, most effective SEO tactic or tips there is.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obtaining traffic from Google means that you've got to keep up with all of \\nGoogle's ... Featured snippets are the first thing that most people see when \\nsearching for a word ... or finance, your chances of getting featured are higher \\nthan if you publish .... Plus, if you already rank high for a particular keyword, it's \\nworth finding out if ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                           Google search snippets for search terms : Keywords and their placing versus highly defined featured snippets from Google are more important for getting traffic on webpage. : Total Result count : 54600\n",
       "0  Aug 23, 2017 ... You have chances to get featured for the terms your pages are already ... health \\nor finance, you have the highest probability of getting featured ... When \\nperforming keyword research with featured snippets in mind, note that: .... queries \\ndetermine the subheadings of the article and thus define its structure.                   \n",
       "1  May 29, 2017 ... Google is actively rolling out featured snippets for a wide variety of search \\nqueries because they ... I mean, let's say that you rank #7 for some keyword. ... \\nBut if there's a featured snippet opportunity, you can get there almost instantly \\nwith just a ... They occupy more real estate, and they clearly catch the eye.          \n",
       "2  Aug 15, 2018 ... The Google featured snippet was first designed when the Google ... a highlighted \\nbox that sometimes appears at the very top of a Google ... types in a question or \\nkeywords that Google recognizes as a question. ... featured-snippets-traffic ... \\nglance would mean millions of more eyes on your website's name.                      \n",
       "3  Feb 14, 2018 ... A website's traffic reflects how well a business is doing online. It is also an ... To \\nbe successful at this, you need to understand how Google defines better content. \\n... (PS: doing this well will increase your ability to get featured snippets too) ... \\nThis is the most important, most effective SEO tactic or tips there is.    \n",
       "4  Obtaining traffic from Google means that you've got to keep up with all of \\nGoogle's ... Featured snippets are the first thing that most people see when \\nsearching for a word ... or finance, your chances of getting featured are higher \\nthan if you publish .... Plus, if you already rank high for a particular keyword, it's \\nworth finding out if ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Google search snippets for search terms : How to build up super snippets for new web content? : Total Result count : 1410000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jan 2, 2018 ... Here's how to configure WP Super Cache to serve up your site's content from the \\ncache ... WordPress can do quite a bit of work creating a page, from making a .... \\nWhen I'm setting up a site for a new client that isn't familiar with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nov 26, 2018 ... The meta description is a snippet of up to about 155 characters – a tag in HTML – \\nwhich summarizes a page's content. Search engines show the ... “Hello, we have \\nsuch and such new product, and you want it. Find out more!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Before publishing your new piece of content, reach out to an influencer or \\ninfluencers in your industry. ... Create 20+ Snippets for Mega Sharing on Social \\nMedia ... How to Build Credibility as a Start-Up SEO Agency – http://t.co/\\nHoiqDf0S4b ... Or, if they have a website, contact them via email or via a contact \\nform and ask them ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Feb 23, 2018 ... Artificial intelligence (AI) is getting much better at identifying the content of \\nimages and providing labels. So-called “generative” algorithms go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aug 16, 2016 ... ST3 makes it super easy to create a snippet. ... ST3 will typically place you in the \\nproper directory when you save your new snippet, ... we no longer have to scroll \\nup the page to see where the selector is ... Hit tab a second time to put your \\ncursor into a beautifuly tabbed spot within the bracket --&gt; &lt;content&gt;&lt;!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                             Google search snippets for search terms : How to build up super snippets for new web content? : Total Result count : 1410000\n",
       "0  Jan 2, 2018 ... Here's how to configure WP Super Cache to serve up your site's content from the \\ncache ... WordPress can do quite a bit of work creating a page, from making a .... \\nWhen I'm setting up a site for a new client that isn't familiar with ...                                                                                       \n",
       "1  Nov 26, 2018 ... The meta description is a snippet of up to about 155 characters – a tag in HTML – \\nwhich summarizes a page's content. Search engines show the ... “Hello, we have \\nsuch and such new product, and you want it. Find out more!                                                                                                      \n",
       "2  Before publishing your new piece of content, reach out to an influencer or \\ninfluencers in your industry. ... Create 20+ Snippets for Mega Sharing on Social \\nMedia ... How to Build Credibility as a Start-Up SEO Agency – http://t.co/\\nHoiqDf0S4b ... Or, if they have a website, contact them via email or via a contact \\nform and ask them ...\n",
       "3  Feb 23, 2018 ... Artificial intelligence (AI) is getting much better at identifying the content of \\nimages and providing labels. So-called “generative” algorithms go ...                                                                                                                                                                            \n",
       "4  Aug 16, 2016 ... ST3 makes it super easy to create a snippet. ... ST3 will typically place you in the \\nproper directory when you save your new snippet, ... we no longer have to scroll \\nup the page to see where the selector is ... Hit tab a second time to put your \\ncursor into a beautifuly tabbed spot within the bracket --> <content><!   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Google search snippets for search terms : Webpage will earn new featured snippets from Google if it includes the same keywords like competitors high ranked featured snippets. : Total Result count : 52000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On your site, you've done keyword research, written tons of blog posts, and ... But \\nthose high rankings don't mean as much if another competitor owns a ... Here's \\nhow you can steal your competitor's featured snippets to earn even better \\nrankings. ... It contains a summary of the content on the featured web page \\nrelated to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aug 23, 2017 ... Featured snippets are selected search results that are featured on top of Google's \\n... It helps if you use a keyword research tool that shows immediately whether a \\nquery ... those for which you or your competitors are already ranking high. But ... \\nas to how long each answer should be in order to get featured:.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apr 19, 2018 ... Do you want more traffic from Google, without creating new content ... If you're \\nwondering what featured snippets are, here's an example: ... featured snippet (\\nbut a competitor does): only 19.6% of all clicks will go to your page. ..... the same \\nSERP filters (as I mentioned earlier) directly in Keywords Explorer?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sep 4, 2018 ... Most companies with a presence on Google will have an interest in appearing at \\nthe ... some featured snippets, this blog post will give you some tips and \\nprocesses to get you started. ... When a page owns a featured snippet, it will sit at \\nthe top of the Search Engine Results Page (SERP), and look like this:.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>May 29, 2017 ... Google is actively rolling out featured snippets for a wide variety of ... It looks like \\nwe're now in the middle of the featured‐snippet‐geddon that ... one of the top‐\\nranking pages for that search query and includes that ... Performing a study on 14 \\nmillion keywords would be rather ..... Get notified of new articles.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              Google search snippets for search terms : Webpage will earn new featured snippets from Google if it includes the same keywords like competitors high ranked featured snippets. : Total Result count : 52000\n",
       "0  On your site, you've done keyword research, written tons of blog posts, and ... But \\nthose high rankings don't mean as much if another competitor owns a ... Here's \\nhow you can steal your competitor's featured snippets to earn even better \\nrankings. ... It contains a summary of the content on the featured web page \\nrelated to ...       \n",
       "1  Aug 23, 2017 ... Featured snippets are selected search results that are featured on top of Google's \\n... It helps if you use a keyword research tool that shows immediately whether a \\nquery ... those for which you or your competitors are already ranking high. But ... \\nas to how long each answer should be in order to get featured:.        \n",
       "2  Apr 19, 2018 ... Do you want more traffic from Google, without creating new content ... If you're \\nwondering what featured snippets are, here's an example: ... featured snippet (\\nbut a competitor does): only 19.6% of all clicks will go to your page. ..... the same \\nSERP filters (as I mentioned earlier) directly in Keywords Explorer?     \n",
       "3  Sep 4, 2018 ... Most companies with a presence on Google will have an interest in appearing at \\nthe ... some featured snippets, this blog post will give you some tips and \\nprocesses to get you started. ... When a page owns a featured snippet, it will sit at \\nthe top of the Search Engine Results Page (SERP), and look like this:.          \n",
       "4  May 29, 2017 ... Google is actively rolling out featured snippets for a wide variety of ... It looks like \\nwe're now in the middle of the featured‐snippet‐geddon that ... one of the top‐\\nranking pages for that search query and includes that ... Performing a study on 14 \\nmillion keywords would be rather ..... Get notified of new articles."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Google search snippets for search terms : Most of the regions in Finland will enjoy white snow and temperatures below freezing point on Christmas Eve. : Total Result count : 72600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In Finland the weather can vary greatly during one day, first sunshine, then rain. \\n... A layer of clean white snow increases the brightness by as much as 80%, .... for \\nthe start of the permanent snow cover in the Helsinki region is Christmas Day, ... \\ntemperature from 15 degrees below zero in the morning to above zero later in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When to go to Rovaniemi? ... for you to travel to the Official Hometown of Santa \\nClaus in Lapland, Finland! ... Winter - Christmas, the Polar Night and snowy \\nspring from December to March ... At the end of May, nights are white. ... \\nTemperatures start dropping below zero in October, and first snow usually .... \\nRead more here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Weather averages, seasons, and tips on the best time to visit. ... In winter, a light \\nsnow often falls, which may not even be counted in the statistics (if it ... Here, the \\ntemperature remains almost constantly around or below freezing (0 °C or 32 ... In \\nLapland, the northernmost part of Finland, the climate is cold for most of the year,\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As such, the climate of much of the Arctic is moderated by the ocean water, which \\ncan never have a temperature below −2 °C (28 °F). In winter, this relatively ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>But that's no reason to stay indoors - just gear up right and make the most of the \\nfresh white season. ... We can't argue below –30 isn't freezing, but enjoying the \\nFinnish winter is all about dressing right – and dressing right is all ... Well, the fact \\nis there's no real winter without snow and no snow without sub-zero temperatures\\n.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                               Google search snippets for search terms : Most of the regions in Finland will enjoy white snow and temperatures below freezing point on Christmas Eve. : Total Result count : 72600\n",
       "0  In Finland the weather can vary greatly during one day, first sunshine, then rain. \\n... A layer of clean white snow increases the brightness by as much as 80%, .... for \\nthe start of the permanent snow cover in the Helsinki region is Christmas Day, ... \\ntemperature from 15 degrees below zero in the morning to above zero later in ...              \n",
       "1  When to go to Rovaniemi? ... for you to travel to the Official Hometown of Santa \\nClaus in Lapland, Finland! ... Winter - Christmas, the Polar Night and snowy \\nspring from December to March ... At the end of May, nights are white. ... \\nTemperatures start dropping below zero in October, and first snow usually .... \\nRead more here.                \n",
       "2  Weather averages, seasons, and tips on the best time to visit. ... In winter, a light \\nsnow often falls, which may not even be counted in the statistics (if it ... Here, the \\ntemperature remains almost constantly around or below freezing (0 °C or 32 ... In \\nLapland, the northernmost part of Finland, the climate is cold for most of the year,\\n ...\n",
       "3  As such, the climate of much of the Arctic is moderated by the ocean water, which \\ncan never have a temperature below −2 °C (28 °F). In winter, this relatively ...                                                                                                                                                                                           \n",
       "4  But that's no reason to stay indoors - just gear up right and make the most of the \\nfresh white season. ... We can't argue below –30 isn't freezing, but enjoying the \\nFinnish winter is all about dressing right – and dressing right is all ... Well, the fact \\nis there's no real winter without snow and no snow without sub-zero temperatures\\n.       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "my_api_key = \"AIzaSyBN0zRiSDC_IdQrYWQaTcbCheyKLRopqOA\"\n",
    "my_cse_id = \"009592823161165690347:wrkvjhigeuw\"\n",
    "\n",
    "# searchTerms = 'build:featur:snippet:paragraph:question:new:content'\n",
    "searchTerms = 'Keywords and their placing versus highly defined featured snippets from Google are more important for getting traffic on webpage.' \n",
    "\n",
    "def google_search(search_term, api_key, cse_id, **kwargs):    \n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
    "    resultItems = res['items']\n",
    "    gQueries = res.get('queries', [])\n",
    "    gTotalResultCount = 0\n",
    "    gRequestObj = gQueries.get('request', [])\n",
    "    for gReqItems in gRequestObj:\n",
    "        gJsonItems = json.dumps(gReqItems)        \n",
    "        gJsonDict = json.loads(gJsonItems)\n",
    "        for key, value in gJsonDict.items():\n",
    "            if key == 'totalResults':\n",
    "                gTotalResultCount = value\n",
    "                         \n",
    "    resultDict = {'total':gTotalResultCount,'items':resultItems}    \n",
    "    return resultDict\n",
    "\n",
    "\n",
    "for google_doc in all_documents: \n",
    "    resultsDict = google_search(\n",
    "        google_doc, my_api_key, my_cse_id, num=10)\n",
    "    \n",
    "    googleSearchSnippetlist = []\n",
    "    for result in resultsDict['items']:\n",
    "        jsonResult = json.dumps(result)\n",
    "        jsonDict = json.loads(jsonResult)    \n",
    "        for key, value in jsonDict.items():\n",
    "            if key == 'snippet':\n",
    "                googleSearchSnippetlist.append(value)\n",
    "\n",
    "    gSnippetDf = pd.DataFrame(googleSearchSnippetlist, columns=['Google search snippets for search terms : ' + google_doc + ' : Total Result count : ' + resultsDict['total']])\n",
    "    display(gSnippetDf.head()) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "401 Client Error: Access Denied for url: https://api.cognitive.microsoft.com/bing/v7.0/search?q=build+and+featur+and+snippet+and+paragraph+and+question+and+new+and+content&textDecorations=True&textFormat=HTML",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-45b782f3abea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mparams\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"q\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msearch_term\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"textDecorations\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"textFormat\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"HTML\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0msearch_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Access Denied for url: https://api.cognitive.microsoft.com/bing/v7.0/search?q=build+and+featur+and+snippet+and+paragraph+and+question+and+new+and+content&textDecorations=True&textFormat=HTML"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "subscription_key = \"ec8557b875a046eb8f036276a87cd9b0\"\n",
    "assert subscription_key\n",
    "\n",
    "search_url = \"https://api.cognitive.microsoft.com/bing/v7.0/search\"\n",
    "search_term = \"build and featur and snippet and paragraph and question and new and content\"\n",
    "\n",
    "headers = {\"Ocp-Apim-Subscription-Key\" : subscription_key}\n",
    "params  = {\"q\": search_term, \"textDecorations\":True, \"textFormat\":\"HTML\"}\n",
    "response = requests.get(search_url, headers=headers, params=params)\n",
    "response.raise_for_status()\n",
    "search_results = response.json()\n",
    "\n",
    "bingSearchSnippetlist = []\n",
    "bingjsonResult = json.dumps(search_results)\n",
    "bingjsonDict = json.loads(bingjsonResult)\n",
    "\n",
    "for bingKey, bingValue in bingjsonDict.items():        \n",
    "        if bingKey == \"webPages\":\n",
    "            for webKey, webValueItems in bingValue.items():\n",
    "                if webKey == \"value\":\n",
    "                    for valueItems in webValueItems:\n",
    "                        for valueKey, valueItem in valueItems.items():\n",
    "                            if valueKey == \"snippet\":\n",
    "                                bingSearchSnippetlist.append(valueItem)\n",
    "                                \n",
    "bingdf = pd.DataFrame(bingSearchSnippetlist, columns=['Bing search snippets for search terms : ' + search_term])\n",
    "display(bingdf.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tDesign and implement a similarity measure that computes the number of overlapping words between the total terms of the ten snippets associated to the first sentence S1 and the second sentence S2. \n",
    "\n",
    "    Hint: use loop for S1 snippets and S2 snippets similarity measurement. The measurement should be conducted between each snippets for each sentence S1 and S2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 5. Compare the result with sentence semantic similarity that you have seen in Lab2.\n",
    "    \n",
    "    Hint: in lab2, WordNet was used to calculate sentence semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "Similarity(\"Cats are beautiful animals.\", \"Dogs are awesome.\") = 0.3333333333333333\n",
      "0.2222222222222222\n",
      "Similarity(\"Dogs are awesome.\", \"Cats are beautiful animals.\") = 0.2222222222222222\n",
      "0.23650793650793647\n",
      "Similarity(\"Cats are beautiful animals.\", \"Some gorgeous creatures are felines.\") = 0.23650793650793647\n",
      "0.41798941798941797\n",
      "Similarity(\"Some gorgeous creatures are felines.\", \"Cats are beautiful animals.\") = 0.41798941798941797\n",
      "0.17777777777777778\n",
      "Similarity(\"Cats are beautiful animals.\", \"Dolphins are swimming mammals.\") = 0.17777777777777778\n",
      "0.14027777777777778\n",
      "Similarity(\"Dolphins are swimming mammals.\", \"Cats are beautiful animals.\") = 0.14027777777777778\n",
      "0.41203703703703703\n",
      "Similarity(\"Cats are beautiful animals.\", \"Cats are beautiful animals.\") = 0.41203703703703703\n",
      "0.41203703703703703\n",
      "Similarity(\"Cats are beautiful animals.\", \"Cats are beautiful animals.\") = 0.41203703703703703\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "#example\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    " \n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    best_score = [0.0]\n",
    "    for ss1 in synsets1:\n",
    "        for ss2 in synsets2:\n",
    "            best1_score=ss1.path_similarity(ss2)\n",
    "        if best1_score is not None:\n",
    "            best_score.append(best1_score)\n",
    "        max1=max(best_score)\n",
    "        if best_score is not None:\n",
    "            score += max1\n",
    "        if max1 is not 0.0:\n",
    "            count += 1\n",
    "        best_score=[0.0]\n",
    "    print(score/count)      \n",
    "   \n",
    "    # Average the values\n",
    "    score /= count\n",
    "    return score\n",
    " \n",
    "sentences = [\n",
    "    \"Dogs are awesome.\",\n",
    "    \"Some gorgeous creatures are felines.\",\n",
    "    \"Dolphins are swimming mammals.\",\n",
    "    \"Cats are beautiful animals.\",\n",
    "]\n",
    " \n",
    "focus_sentence = \"Cats are beautiful animals.\"\n",
    " \n",
    "for sentence in sentences:\n",
    "    print (\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (focus_sentence, sentence, sentence_similarity(focus_sentence, sentence)))\n",
    "    print (\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (sentence, focus_sentence, sentence_similarity(sentence, focus_sentence)))\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Refine your code in order to expand the terms of each snippets to include all the hyponyms and hypernyms of the associated words by quering the WordNet database, and repeat the overlapping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Wikipedia based similarity.\n",
    "   Similarly, use Wikipedia dump files in order to design a program that search the Wikipedia documents for each Sentence. The similarity between the sentences is therefore measured as the number of common Wikipedia documents outputted by the queries (S1 and S2) over the total number of documents outputted by the two queries. Repeat the process of calculating the semantic similarity for your set of chosen academic examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Use a publicly available database of your choice in order to test the usefulness of this similarity measure (Snippets and Wikipedia based similarity) and compare the results with some state of art measures mentioned in the literature employing your chosen publicly database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n",
      "Videos:\n",
      " How to Optimize for Google's Featured Snippet Box\n",
      "How to Find Pages That Send Your Competitors Organic Search Traffic\n",
      "How to Do Keyword Research in 2018: Go Beyond Search Volume\n",
      "13 SEO Tips That ACTUALLY Work in 2018 and Beyond\n",
      "Google Analytics Individual Qualification Exam Answers 2018📊Live Exam Pass📊100% correct✅\n",
      "How does Google Search work?\n",
      "How to Advertise on Google For Beginners | Complete Google AdWords Tutorial for 2018!\n",
      "How to Get Website Traffic With Evergreen Content and Social Media Marketing\n",
      "How to Dominate Half of the Google Search Results Page - Google Featured Snippets\n",
      "SEO 2018 - What you need to know about Content, Schema, Semantically Related Keywords etc  [28:07] \n",
      "\n",
      "22507\n",
      "Videos:\n",
      " Stand out in search results with Rich Cards\n",
      "How to Optimize Google Snippets - MUST DO SEO\n",
      "Super Junior D&E, Can We See a Snippet of the Dance? [Yu Huiyeol’s Sketchbook Ep 407]\n",
      "Bootstrap 3 Tutorial 4 - Using Bootsnipp to Copy & Paste HTML5 Web Elements\n",
      "How-to Add A Custom Function To Your WordPress Website\n",
      "How To Appear In Google Search Featured Snippets\n",
      "Why & How To Add Schema To Your WordPress Website For Better SEO & Visibility\n",
      "Link Building: How to Get POWERFUL Backlinks in 2018\n",
      "Coffee shop - Web Design made easy with Sitemagic CMS 4.3\n",
      "Bootstrap 4 tutorial   44   extra plugins, templates and snippets \n",
      "\n",
      "395\n",
      "Videos:\n",
      " How to Find and Steal Google Featured Snippets [AMS-08]\n",
      "How to Appear in Google's Answer Boxes - Whiteboard Friday\n",
      "Google Zero Rank - How to RANK #0 in Google organic listings (Featured Snippets)\n",
      "How to Find Thousands of Keyword Ideas for SEO\n",
      "13 SEO Tips That ACTUALLY Work in 2018 and Beyond\n",
      "How to Earn Google Featured Snippets for Mobile: Large-scale Study\n",
      "How to Do an Effective Content Gap Analysis for SEO\n",
      "How to Get Your Content to Appear in Google's Featured Snippet\n",
      "How to win with featured snippets | International SEO\n",
      "How to Do Keyword Research in 2018: Go Beyond Search Volume \n",
      "\n",
      "47\n",
      "Videos:\n",
      " Why Winter Swimming Is Good For You\n",
      "Snow Tha Product - “Nights\" (feat. W. Darling)\n",
      "Winter Solstice in the Arctic. Fairbanks, Alaska. Time-lapse video.\n",
      "Admiral (2008) with English Subtitles (Full)\n",
      "Worlds Most Visited Ports For Cruise Ship Passengers Plus Viewers Favourite Cruise Destinations\n",
      "Uncharted Territory: David Thompson on the Columbia Plateau\n",
      "Money Magic Trick For Homeless\n",
      "Yelawolf - Daddy's Lambo (Official Music Video) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LET'S TRY YOUTUBE SNIPPETS\n",
    "\n",
    "# Please check in this code\n",
    "# python search.py / --q = surfing / --max-results = 10 / totalResults\n",
    "\n",
    "import argparse\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import json\n",
    "\n",
    "# NOTE: To use the sample, you must provide a developer key obtained in the Google APIs Console. \n",
    "# Search for DEVELOPER_KEY in this code to find the correct place to provide that key..\n",
    "# Please ensure that you have enabled the YouTube Data API for your project from your Google account.\n",
    "\n",
    "DEVELOPER_KEY = 'AIzaSyBN0zRiSDC_IdQrYWQaTcbCheyKLRopqOA'\n",
    "YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
    "YOUTUBE_API_VERSION = 'v3'\n",
    "\n",
    "# This code collects only ids and snippets of videos \n",
    "# (as chennels and playlists didn't include any suitable information for us)\n",
    "\n",
    "def youtube_search(searchItem, maxResults):\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "    # Call the search.list method to retrieve results matching the specified query term (at the end of the code).\n",
    "    search_response = youtube.search().list(q=searchItem, part='id,snippet', maxResults=maxResults).execute()\n",
    "\n",
    "    videos = []     #  channels = []  #  playlists = []\n",
    "\n",
    "    # Add each result to the appropriate list, and then display the lists of matching videos. (channels, and playlists)\n",
    "    \n",
    "#print(str(search_response.get('pageInfo')), '\\n\\n')\n",
    "#print(search_response)\n",
    "    \n",
    "    info = search_response.get('pageInfo', [])\n",
    "    total = info.get('totalResults')\n",
    "    print(total)\n",
    "    for search_result in search_response.get(\"items\", []):\n",
    "        if search_result['id']['kind'] == 'youtube#video':\n",
    "              videos.append('%s' % (search_result['snippet']['title']))\n",
    "\n",
    "        # Code below saved for later use\n",
    "        #                              search_result['id']['videoId']))\n",
    "        #      elif search_result['id']['kind'] == 'youtube#channel':\n",
    "        #            channels.append('%s (%s)' % (search_result['snippet']['title'],\n",
    "        #                                 search_result['id']['channelId']))\n",
    "        #      elif search_result['id']['kind'] == 'youtube#playlist':\n",
    "        #            playlists.append('%s (%s)' % (search_result['snippet']['title'],\n",
    "        #                                  search_result['id']['playlistId']))\n",
    "\n",
    "    print ('Videos:\\n', '\\n'.join(videos), '\\n')\n",
    "         #   print ('Channels:\\n', '\\n'.join(channels), '\\n')\n",
    "         #   print ('Playlists:\\n', '\\n'.join(playlists), '\\n')\n",
    "\n",
    "\n",
    "for youtube_doc in all_documents:\n",
    "    try:\n",
    "                        # youtube_search(args)\n",
    "                        # searchItem = \"How to build up super snippets for new web content?\"\n",
    "        maxResults = 10\n",
    "        youtube_search(youtube_doc, maxResults)\n",
    "    except HttpError as e:\n",
    "        print ('An HTTP error %d occurred:\\n%s' % (e.resp.status, e.content))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Design a simple GUI interface that allows you to demonstrate your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Source word lists---\n",
      "[['keywords', 'placing', 'versus', 'highly', 'defined', 'featured', 'snippets', 'google', 'important', 'getting', 'traffic', 'webpage'], ['build', 'super', 'snippets', 'new', 'web', 'content'], ['webpage', 'earn', 'new', 'featured', 'snippets', 'google', 'includes', 'keywords', 'like', 'competitors', 'high', 'ranked', 'featured', 'snippets'], ['regions', 'finland', 'enjoy', 'white', 'snow', 'temperatures', 'freezing', 'point', 'christmas', 'eve']]\n",
      "[[{'word': 'placing', 'hyponym': [Synset('belt_out.v.01'), Synset('chant.v.01'), Synset('choir.v.01'), Synset('croon.v.01'), Synset('descant.v.01'), Synset('descant_on.v.01'), Synset('harmonize.v.03'), Synset('hum.v.01'), Synset('minstrel.v.01'), Synset('place.v.16'), Synset('psalm.v.01'), Synset('sing.v.01'), Synset('sing_along.v.01'), Synset('solmizate.v.02'), Synset('treble.v.01'), Synset('troll.v.05'), Synset('tweedle.v.01'), Synset('vocalize.v.02'), Synset('warble.v.01'), Synset('yodel.v.01')]}, {'word': 'defined', 'hyponym': [Synset('adopt.v.01'), Synset('anoint.v.01'), Synset('assign.v.04'), Synset('cream_off.v.02'), Synset('cull_out.v.01'), Synset('dial.v.02'), Synset('draw.v.09'), Synset('elect.v.01'), Synset('elect.v.02'), Synset('empanel.v.02'), Synset('excerpt.v.01'), Synset('field.v.04'), Synset('nominate.v.02'), Synset('pick.v.01'), Synset('plump.v.04'), Synset('screen.v.03'), Synset('sieve.v.04'), Synset('sieve_out.v.01'), Synset('single_out.v.01'), Synset('specify.v.02'), Synset('think_of.v.06'), Synset('vote.v.01'), Synset('vote_in.v.01')]}, {'word': 'featured', 'hyponym': [Synset('abound.v.02'), Synset('bear.v.01'), Synset('brim.v.01'), Synset('bristle.v.03'), Synset('carry.v.02'), Synset('carry.v.18'), Synset('carry.v.22'), Synset('carry.v.35'), Synset('give_off.v.01'), Synset('imply.v.05'), Synset('possess.v.01'), Synset('read.v.02'), Synset('sport.v.01'), Synset('star.v.01'), Synset('unite.v.03'), Synset('wear.v.02'), Synset('wear.v.03'), Synset('wear.v.05')]}, {'word': 'snippets', 'hyponym': [Synset('beam.n.02'), Synset('piece_of_cloth.n.01'), Synset('piece_of_leather.n.01'), Synset('scrap.n.03'), Synset('shard.n.01'), Synset('snip.n.01'), Synset('spindle.n.02')]}, {'word': 'google', 'hyponym': [Synset('cast_about.v.01'), Synset('google.v.01'), Synset('mapquest.v.01'), Synset('prospect.v.02'), Synset('re-explore.v.01')]}, {'word': 'getting', 'hyponym': [Synset('arouse.v.01'), Synset('assemble.v.01'), Synset('bear.v.05'), Synset('beat.v.18'), Synset('beget.v.01'), Synset('blast.v.05'), Synset('bring.v.03'), Synset('build.v.03'), Synset('cause.v.01'), Synset('chop.v.03'), Synset('choreograph.v.01'), Synset('clear.v.02'), Synset('cleave.v.02'), Synset('compose.v.02'), Synset('construct.v.01'), Synset('copy.v.04'), Synset('create.v.05'), Synset('create_by_mental_act.v.01'), Synset('create_from_raw_material.v.01'), Synset('create_verbally.v.01'), Synset('cut.v.06'), Synset('cut.v.22'), Synset('derive.v.04'), Synset('direct.v.03'), Synset('distill.v.03'), Synset('establish.v.05'), Synset('film-make.v.01'), Synset('film.v.02'), Synset('form.v.01'), Synset('froth.v.02'), Synset('generate.v.01'), Synset('give.v.09'), Synset('grind.v.06'), Synset('incorporate.v.03'), Synset('institute.v.02'), Synset('lay_down.v.01'), Synset('manufacture.v.04'), Synset('offset.v.04'), Synset('originate.v.02'), Synset('prepare.v.03'), Synset('press.v.07'), Synset('produce.v.01'), Synset('produce.v.03'), Synset('puncture.v.02'), Synset('put_on.v.04'), Synset('raise.v.07'), Synset('raise.v.11'), Synset('re-create.v.01'), Synset('realize.v.03'), Synset('recreate.v.04'), Synset('regenerate.v.07'), Synset('reproduce.v.02'), Synset('scrape.v.02'), Synset('short-circuit.v.02'), Synset('strike.v.13'), Synset('style.v.02'), Synset('track.v.05'), Synset('twine.v.03')]}, {'word': 'traffic', 'hyponym': [Synset('arbitrage.v.01'), Synset('export.v.01'), Synset('import.v.01'), Synset('market.v.01'), Synset('run.v.25'), Synset('traffic.v.01'), Synset('traffic.v.02')]}, {'word': 'webpage', 'hyponym': [Synset('ascii_text_file.n.01'), Synset('web_page.n.01')]}], [{'word': 'build', 'hyponym': [Synset('accelerate.v.01'), Synset('build.v.10'), Synset('condense.v.05'), Synset('erupt.v.02'), Synset('redouble.v.03'), Synset('sharpen.v.03')]}, {'word': 'super', 'hyponym': [Synset('concierge.n.01'), Synset('sexton.n.02'), Synset('superintendent.n.02'), Synset('verger.n.01')]}, {'word': 'snippets', 'hyponym': [Synset('beam.n.02'), Synset('piece_of_cloth.n.01'), Synset('piece_of_leather.n.01'), Synset('scrap.n.03'), Synset('shard.n.01'), Synset('snip.n.01'), Synset('spindle.n.02')]}, {'word': 'web', 'hyponym': [Synset('braid.v.01'), Synset('brocade.v.01'), Synset('lace.v.03'), Synset('loom.v.04'), Synset('twill.v.01'), Synset('web.v.01')]}, {'word': 'content', 'hyponym': [Synset('content.v.02'), Synset('please.v.01'), Synset('please.v.03')]}], [{'word': 'webpage', 'hyponym': [Synset('ascii_text_file.n.01'), Synset('web_page.n.01')]}, {'word': 'earn', 'hyponym': [Synset('accept.v.02'), Synset('acquire.v.05'), Synset('borrow.v.01'), Synset('buy.v.01'), Synset('buy.v.04'), Synset('capture.v.06'), Synset('catch.v.10'), Synset('collect.v.05'), Synset('come_by.v.02'), Synset('earn.v.02'), Synset('enter_upon.v.01'), Synset('find.v.03'), Synset('find.v.10'), Synset('gain.v.08'), Synset('get.v.21'), Synset('glom.v.02'), Synset('inherit.v.01'), Synset('isolate.v.02'), Synset('lease.v.04'), Synset('line_up.v.02'), Synset('obtain.v.01'), Synset('partake.v.02'), Synset('pick_up.v.06'), Synset('poll.v.03'), Synset('preempt.v.01'), Synset('preempt.v.03'), Synset('press_out.v.03'), Synset('profit.v.01'), Synset('receive.v.01'), Synset('reclaim.v.01'), Synset('recover.v.01'), Synset('recover.v.04'), Synset('turn.v.18'), Synset('win_back.v.01')]}, {'word': 'featured', 'hyponym': [Synset('abound.v.02'), Synset('bear.v.01'), Synset('brim.v.01'), Synset('bristle.v.03'), Synset('carry.v.02'), Synset('carry.v.18'), Synset('carry.v.22'), Synset('carry.v.35'), Synset('give_off.v.01'), Synset('imply.v.05'), Synset('possess.v.01'), Synset('read.v.02'), Synset('sport.v.01'), Synset('star.v.01'), Synset('unite.v.03'), Synset('wear.v.02'), Synset('wear.v.03'), Synset('wear.v.05')]}, {'word': 'snippets', 'hyponym': [Synset('beam.n.02'), Synset('piece_of_cloth.n.01'), Synset('piece_of_leather.n.01'), Synset('scrap.n.03'), Synset('shard.n.01'), Synset('snip.n.01'), Synset('spindle.n.02')]}, {'word': 'google', 'hyponym': [Synset('cast_about.v.01'), Synset('google.v.01'), Synset('mapquest.v.01'), Synset('prospect.v.02'), Synset('re-explore.v.01')]}, {'word': 'includes', 'hyponym': [Synset('admit.v.02'), Synset('admit.v.03'), Synset('allow.v.10'), Synset('authorize.v.01'), Synset('digest.v.03'), Synset('furlough.v.02'), Synset('give.v.40'), Synset('legalize.v.01'), Synset('privilege.v.01'), Synset('trust.v.02')]}, {'word': 'like', 'hyponym': [Synset('ambition.v.01'), Synset('crave.v.01'), Synset('envy.v.02'), Synset('fancy.v.02'), Synset('feel_like.v.01'), Synset('hanker.v.01'), Synset('hope.v.02'), Synset('itch.v.04'), Synset('like.v.05'), Synset('lust_after.v.01'), Synset('miss.v.02'), Synset('seek.v.01'), Synset('wish.v.01'), Synset('wish.v.02'), Synset('wish.v.04')]}, {'word': 'competitors', 'hyponym': [Synset('agonist.n.02'), Synset('athlete.n.01'), Synset('defaulter.n.03'), Synset('entrant.n.04'), Synset('long_shot.n.02'), Synset('loser.n.01'), Synset('opposition.n.04'), Synset('outsider.n.02'), Synset('player.n.01'), Synset('pothunter.n.02'), Synset('qualifier.n.01'), Synset('rival.n.01'), Synset('starter.n.02'), Synset('winner.n.01'), Synset('withdrawer.n.04')]}, {'word': 'high', 'hyponym': [Synset('first_gear.n.01'), Synset('gearset.n.01'), Synset('high_gear.n.01'), Synset('park.n.06'), Synset('reverse.n.02'), Synset('second_gear.n.01'), Synset('steering_gear.n.01'), Synset('third_gear.n.01'), Synset('transmission.n.05')]}, {'word': 'ranked', 'hyponym': [Synset('exceed.v.02'), Synset('rank.v.03'), Synset('shine_at.v.01')]}, {'word': 'featured', 'hyponym': [Synset('abound.v.02'), Synset('bear.v.01'), Synset('brim.v.01'), Synset('bristle.v.03'), Synset('carry.v.02'), Synset('carry.v.18'), Synset('carry.v.22'), Synset('carry.v.35'), Synset('give_off.v.01'), Synset('imply.v.05'), Synset('possess.v.01'), Synset('read.v.02'), Synset('sport.v.01'), Synset('star.v.01'), Synset('unite.v.03'), Synset('wear.v.02'), Synset('wear.v.03'), Synset('wear.v.05')]}, {'word': 'snippets', 'hyponym': [Synset('beam.n.02'), Synset('piece_of_cloth.n.01'), Synset('piece_of_leather.n.01'), Synset('scrap.n.03'), Synset('shard.n.01'), Synset('snip.n.01'), Synset('spindle.n.02')]}], [{'word': 'regions', 'hyponym': [Synset('discipline.n.01'), Synset('region.n.05'), Synset('scientific_knowledge.n.01')]}, {'word': 'enjoy', 'hyponym': [Synset('come.v.20'), Synset('enjoy.v.04'), Synset('feel.v.06'), Synset('know.v.05'), Synset('suffer.v.01'), Synset('suffer.v.10'), Synset('witness.v.02')]}, {'word': 'white', 'hyponym': [Synset('blacken.v.01'), Synset('blue.v.01'), Synset('blush.v.01'), Synset('dye.v.01'), Synset('green.v.01'), Synset('grey.v.02'), Synset('pale.v.01'), Synset('purple.v.01'), Synset('redden.v.03'), Synset('silver.v.03'), Synset('sunburn.v.01'), Synset('tan.v.02'), Synset('tone.v.03'), Synset('turn.v.14'), Synset('whiten.v.01'), Synset('yellow.v.01')]}, {'word': 'snow', 'hyponym': [Synset('bamboozle.v.01'), Synset('gull.v.02'), Synset('pose.v.03')]}, {'word': 'temperatures', 'hyponym': [Synset('feeling.n.04'), Synset('pain.n.03'), Synset('pressure.n.05'), Synset('prickling.n.01'), Synset('temperature.n.02')]}, {'word': 'freezing', 'hyponym': [Synset('act.v.05'), Synset('act_involuntarily.v.01'), Synset('backslap.v.01'), Synset('break_down.v.03'), Synset('bungle.v.02'), Synset('dally.v.02'), Synset('fall_over_backwards.v.01'), Synset('follow.v.18'), Synset('footle.v.02'), Synset('freeze.v.10'), Synset('frivol.v.01'), Synset('hugger_mugger.v.01'), Synset('joke.v.02'), Synset('make.v.19'), Synset('make.v.48'), Synset('make_as_if.v.01'), Synset('menace.v.03'), Synset('optimize.v.03'), Synset('piffle.v.02'), Synset('play.v.16'), Synset('presume.v.04'), Synset('quack.v.02'), Synset('ramp.v.01'), Synset('relax.v.05'), Synset('romanticize.v.03'), Synset('sauce.v.01'), Synset('sentimentalise.v.03'), Synset('stooge.v.03'), Synset('swagger.v.03'), Synset('swell.v.02'), Synset('vulgarize.v.03'), Synset('wanton.v.06')]}, {'word': 'point', 'hyponym': [Synset('cobble.v.02'), Synset('darn.v.01'), Synset('fill.v.09'), Synset('heel.v.05'), Synset('patch.v.03'), Synset('piece.v.05'), Synset('point.v.14'), Synset('sole.v.01'), Synset('tinker.v.03'), Synset('trouble-shoot.v.01'), Synset('vamp.v.04')]}, {'word': 'christmas', 'hyponym': [Synset('christmas.n.02'), Synset('circumcision.n.01'), Synset('id_al-adha.n.01'), Synset('id_al-fitr.n.01'), Synset('movable_feast.n.01'), Synset('rosh_hashanah.n.01'), Synset('thanksgiving.n.01')]}, {'word': 'eve', 'hyponym': [Synset('afternoon.n.01'), Synset('evening.n.01'), Synset('midafternoon.n.01')]}]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Be sure you've unzipped the wordnet corpus at nltk_data/corpora/wordnet. \n",
    "# This will allow WordNetCorpusReader to access it.\n",
    "# nltk_data/corpora/wordnet\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "# we use stop_filtered_tokes -list of word lists that is defined above\n",
    "# as seed for wordnet\n",
    "print('---Source word lists---')\n",
    "print(stop_filtered_tokens)\n",
    "listOfSentences = []\n",
    "\n",
    "\n",
    "for sft in stop_filtered_tokens:\n",
    "    listOfTokenHyponymDictionaries = []\n",
    "    for t in sft:\n",
    "        synSetList = []\n",
    "        #print('--- word to Process --- ')\n",
    "        #print(t)\n",
    "        mySynSets = wordnet.synsets(t)\n",
    "        if mySynSets:\n",
    "            hyponymsForTokensDict = {}\n",
    "            for mySynset in mySynSets:\n",
    "                #print (mySynset)\n",
    "                if(mySynset.hypernyms()):\n",
    "                    #for aHyponym in mySynset.hypernyms()[0].hyponyms():\n",
    "                    #    print(aHyponym.name())\n",
    "                    hyponymsForTokensDict = {'word':t,'hyponym':mySynset.hypernyms()[0].hyponyms()}                    \n",
    "            if any(hyponymsForTokensDict):        \n",
    "                listOfTokenHyponymDictionaries.append(hyponymsForTokensDict)        \n",
    "    listOfSentences.append(listOfTokenHyponymDictionaries)\n",
    "print(listOfSentences)\n",
    "            #print(mySynset)\n",
    "            #print('----hypernyms----')\n",
    "            #print(mySynset.hypernyms())\n",
    "            #if mySynset \n",
    "                \n",
    "        #print(syn.name())\n",
    "        #print(syn.definition())\n",
    "#wordnet.synset('cookbook.n.01')\n",
    "\n",
    "#print(wordnet.synsets('cooking')[0].examples())        # wordnet.synsets(word)\n",
    "#syn.name()                                  # 'cookbook.n.01'\n",
    "#syn.definition()                            #'a book of recipes and cooking directions'\n",
    "#wordnet.synsets('snippet')[0].examples()\n",
    "\n",
    "#  ['cooking can be a great art', 'people are needed who have experience in cookery', 'he left the preparation of meals to his wife']\n",
    "\n",
    "# Synsets are organized in a structure similar to that of an inheritance tree. \n",
    "# More abstract terms are known as hypernyms and more specific terms are hyponyms. \n",
    "# This tree can be traced all the way up to a root hypernym\n",
    "#print(syn.hypernyms())\n",
    "#print(syn.hypernyms()[0].hyponyms())\n",
    "#print(syn.hypernym_paths())\n",
    "\n",
    "#“The hypernym_paths() method returns a list of lists, \n",
    "# where each list starts at the root hypernym and ends with the original Synset. \n",
    "# Most of the time, you'll only get one nested list of Synsets.”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
