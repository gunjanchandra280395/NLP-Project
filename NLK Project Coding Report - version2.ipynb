{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 18: Analysis of Snippets\n",
    "---Natural Language Processing and Text Mining 521158S "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The aim of this project is to design an approach that makes use of Google and msn snippet in order to compute the semantic similarity between sentences.Given two sentences S1 and S2, the key is to input each of the sentences to the search engine and investigate the overlapping that may exist between the generated snippets.\n",
    "  \n",
    "  Seminar report date: 11.12.2018. Project delivery deadline: 7.1.2018."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tUse Google search API and msn search API in order to generate snippets associated to a given sentence. Retrieve the first ten snippets for each sentence. Design and implement a similarity measure that computes the number of overlapping words between the total terms of the ten snippets associated to the first sentence S1 and the first ten snippets associated to the sentence S2. So the similarity here looks similar to Jaccard distance.  You may find the following Msc thesis useful http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2015/MSC/MSC-2015-16.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences\n",
    "# Sentence 1: AI and humans have always been friendly.\n",
    "# Sentence 2: AI is our friend and it has been friendly.\n",
    "# Note: more sentences without closing meanings will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "401 Client Error: Access Denied for url: https://api.cognitive.microsoft.com/bing/v7.0/search?q=build+and+featur+and+snippet+and+paragraph+and+question+and+new+and+content&textDecorations=True&textFormat=HTML",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-f699068e3745>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mparams\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"q\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msearch_term\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"textDecorations\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"textFormat\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"HTML\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0msearch_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\nltk\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Access Denied for url: https://api.cognitive.microsoft.com/bing/v7.0/search?q=build+and+featur+and+snippet+and+paragraph+and+question+and+new+and+content&textDecorations=True&textFormat=HTML"
     ]
    }
   ],
   "source": [
    "# Bing API search\n",
    "import requests\n",
    "\n",
    "subscription_key = \"ec8557b875a046eb8f036276a87cd9b0\"\n",
    "assert subscription_key\n",
    "\n",
    "search_url = \"https://api.cognitive.microsoft.com/bing/v7.0/search\"\n",
    "search_term = \"build and featur and snippet and paragraph and question and new and content\"\n",
    "\n",
    "headers = {\"Ocp-Apim-Subscription-Key\" : subscription_key}\n",
    "params  = {\"q\": search_term, \"textDecorations\":True, \"textFormat\":\"HTML\"}\n",
    "response = requests.get(search_url, headers=headers, params=params)\n",
    "response.raise_for_status()\n",
    "search_results = response.json()\n",
    "\n",
    "bingSearchSnippetlist = []\n",
    "bingjsonResult = json.dumps(search_results)\n",
    "bingjsonDict = json.loads(bingjsonResult)\n",
    "\n",
    "for bingKey, bingValue in bingjsonDict.items():        \n",
    "        if bingKey == \"webPages\":\n",
    "            for webKey, webValueItems in bingValue.items():\n",
    "                if webKey == \"value\":\n",
    "                    for valueItems in webValueItems:\n",
    "                        for valueKey, valueItem in valueItems.items():\n",
    "                            if valueKey == \"snippet\":\n",
    "                                bingSearchSnippetlist.append(valueItem)\n",
    "                                \n",
    "bingdf = pd.DataFrame(bingSearchSnippetlist, columns=['Bing search snippets for search terms : ' + search_term])\n",
    "display(bingdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: two lists of snippet were processed independently in this notebook. The loop will be updated in version3(now in testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Google search snippets for sentence_1 : AI and humans have always been friendly. : Total Result count : 28100000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>May 15, 2018 ... Sentence 1: AI is our friend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feb 7, 2018 ... Behaviors have always been asc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But the United States has been developing and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mar 7, 2018 ... I call this approach “human-ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dec 17, 2013 ... WHY WE NEED FRIENDLY AI. Luke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aug 29, 2017 ... A superintelligent AI will be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>May 14, 2018 ... A.G.I. enthusiasts have had d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jun 7, 2001 ... analyzes the ways in which AI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It was just a friendly little argument about t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jul 4, 2018 ... Sentence 1: AI is our friend a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Google search snippets for sentence_1 : AI and humans have always been friendly. : Total Result count : 28100000\n",
       "0  May 15, 2018 ... Sentence 1: AI is our friend ...                                                              \n",
       "1  Feb 7, 2018 ... Behaviors have always been asc...                                                              \n",
       "2  But the United States has been developing and ...                                                              \n",
       "3  Mar 7, 2018 ... I call this approach “human-ce...                                                              \n",
       "4  Dec 17, 2013 ... WHY WE NEED FRIENDLY AI. Luke...                                                              \n",
       "5  Aug 29, 2017 ... A superintelligent AI will be...                                                              \n",
       "6  May 14, 2018 ... A.G.I. enthusiasts have had d...                                                              \n",
       "7  Jun 7, 2001 ... analyzes the ways in which AI ...                                                              \n",
       "8  It was just a friendly little argument about t...                                                              \n",
       "9  Jul 4, 2018 ... Sentence 1: AI is our friend a...                                                              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Google search snippets for sentence_2 : AI is our friend and it has been friendly. : Total Result count : 31500000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It was just a friendly little argument about t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>May 15, 2018 ... I have already talked about c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feb 16, 2018 ... \"AI can be our friend,\" says ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>May 14, 2018 ... Tad Friend writes that thinki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jan 9, 2018 ... For example, in 2014 it was cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jul 4, 2018 ... Sentence 1: AI is our friend a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nov 18, 2018 ... Microsoft has built a new typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dec 7, 2017 ... MIT's Sherry Turkle has concer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jun 30, 2018 ... whether an environment is a f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jun 7, 2001 ... The Machine Intelligence Resea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Google search snippets for sentence_2 : AI is our friend and it has been friendly. : Total Result count : 31500000\n",
       "0  It was just a friendly little argument about t...                                                                \n",
       "1  May 15, 2018 ... I have already talked about c...                                                                \n",
       "2  Feb 16, 2018 ... \"AI can be our friend,\" says ...                                                                \n",
       "3  May 14, 2018 ... Tad Friend writes that thinki...                                                                \n",
       "4  Jan 9, 2018 ... For example, in 2014 it was cl...                                                                \n",
       "5  Jul 4, 2018 ... Sentence 1: AI is our friend a...                                                                \n",
       "6  Nov 18, 2018 ... Microsoft has built a new typ...                                                                \n",
       "7  Dec 7, 2017 ... MIT's Sherry Turkle has concer...                                                                \n",
       "8  Jun 30, 2018 ... whether an environment is a f...                                                                \n",
       "9  Jun 7, 2001 ... The Machine Intelligence Resea...                                                                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Google API search\n",
    "from googleapiclient.discovery import build\n",
    "import pprint\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "my_api_key = \"AIzaSyBN0zRiSDC_IdQrYWQaTcbCheyKLRopqOA\"\n",
    "my_cse_id = \"009592823161165690347:wrkvjhigeuw\"\n",
    "\n",
    "# searchTerms = 'build:featur:snippet:paragraph:question:new:content'\n",
    "searchTerms = 'Keywords and their placing versus highly defined featured snippets from Google are more important for getting traffic on webpage.' \n",
    "\n",
    "def google_search(search_term, api_key, cse_id, **kwargs):    \n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
    "    resultItems = res['items']\n",
    "    gQueries = res.get('queries', [])\n",
    "    gTotalResultCount = 0\n",
    "    gRequestObj = gQueries.get('request', [])\n",
    "    for gReqItems in gRequestObj:\n",
    "        gJsonItems = json.dumps(gReqItems)        \n",
    "        gJsonDict = json.loads(gJsonItems)\n",
    "        for key, value in gJsonDict.items():\n",
    "            if key == 'totalResults':\n",
    "                gTotalResultCount = value\n",
    "                         \n",
    "    resultDict = {'total':gTotalResultCount,'items':resultItems}    \n",
    "    return resultDict\n",
    "\n",
    "sentence_1 = \"AI and humans have always been friendly.\"\n",
    "\n",
    "sentence_2 = \"AI is our friend and it has been friendly.\"\n",
    "\n",
    "# Snippets list for sentence_1\n",
    "googleSearchSnippetlist_1 = []\n",
    "resultsDict_1 = google_search(sentence_1, my_api_key, my_cse_id, num=10)\n",
    "\n",
    "for result in resultsDict_1['items']:\n",
    "    jsonResult = json.dumps(result)\n",
    "    jsonDict = json.loads(jsonResult)    \n",
    "    for key, value in jsonDict.items():\n",
    "        if key == 'snippet':\n",
    "            googleSearchSnippetlist_1.append(value)\n",
    "gSnippetDf = pd.DataFrame(googleSearchSnippetlist_1, columns=['Google search snippets for sentence_1 : ' + sentence_1 + ' : Total Result count : ' + resultsDict_1['total']])\n",
    "display(gSnippetDf)\n",
    "\n",
    "# Snippets list for sentence_2\n",
    "googleSearchSnippetlist_2 = []\n",
    "resultsDict_2 = google_search(sentence_2, my_api_key, my_cse_id, num=10)\n",
    "\n",
    "for result in resultsDict_2['items']:\n",
    "    jsonResult = json.dumps(result)\n",
    "    jsonDict = json.loads(jsonResult)    \n",
    "    for key, value in jsonDict.items():\n",
    "        if key == 'snippet':\n",
    "            googleSearchSnippetlist_2.append(value)\n",
    "gSnippetDf = pd.DataFrame(googleSearchSnippetlist_2, columns=['Google search snippets for sentence_2 : ' + sentence_2 + ' : Total Result count : ' + resultsDict_2['total']])\n",
    "display(gSnippetDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Academic sentence - short example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>May 15, 2018 ... Sentence 1: AI is our friend and it has been friendly. Sentence 2: AI and humans \\nhave always been friendly. In order to calculate similarity using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Feb 7, 2018 ... Behaviors have always been ascribed to agents with biological drives ... at the \\nCenter for Human-Compatible AI, the big challenge is how to get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But the United States has been developing and deploying military robots with ... \\nnot be in the familiar ways humans are, for their psychology and reasons for \\naction will be quite unlike ours. Thus, Eliezer S. Yudkowsky, among the most \\nprominent of the Friendly AI .... Moral reasoning will always be essential but \\nunfinished.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mar 7, 2018 ... I call this approach “human-centered A.I.” It consists of three goals that can help \\n... on the world — for better or worse — will always be our responsibility. ... New \\nYork edition with the headline: How to Make A.I. Human-Friendly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dec 17, 2013 ... WHY WE NEED FRIENDLY AI. Luke Muehlhauser and Nick Bostrom. Humans \\nwill not always be the most intelligent agents on Earth, the ones ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Aug 29, 2017 ... A superintelligent AI will be extremely good at accomplishing its goals, ... pioneer \\nEliezer Yudkowsky has termed “friendly AI”: AI whose goals are aligned with ours\\n. ... We humans accomplish this so effortlessly that it's easy to forget how ... but \\nthe third wish is almost always the same: “please undo the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>May 14, 2018 ... A.G.I. enthusiasts have had decades to ponder this future, and yet their ... In the \\nNetflix show “Altered Carbon,” A.I. beings scorn humans as “a ..... Therefore, “to \\nprogram a friendly AI, we need to capture the meaning of life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Jun 7, 2001 ... analyzes the ways in which AI and human psychology are likely to differ, and the \\nways ..... of Friendly AI must be seen against that background. ..... It is always \\npossible to make engineering assumptions so conservative that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It was just a friendly little argument about the fate of humanity. ... Google's search \\nengine from the beginning has been dependent on A.I. All of these ... to \\neventually create flexible, self-teaching A.I. that will mirror human learning. .... “\\nAnd it's very hard to calibrate how much you are moving because it always looks \\nthe same.”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jul 4, 2018 ... Sentence 1: AI is our friend and it has been friendly. Sentence 2: AI and humans \\nhave always been friendly. In order to calculate similarity using ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                           Academic sentence - short example\n",
       "0  May 15, 2018 ... Sentence 1: AI is our friend and it has been friendly. Sentence 2: AI and humans \\nhave always been friendly. In order to calculate similarity using ...                                                                                                                                                                                \n",
       "1  Feb 7, 2018 ... Behaviors have always been ascribed to agents with biological drives ... at the \\nCenter for Human-Compatible AI, the big challenge is how to get ...                                                                                                                                                                                    \n",
       "2  But the United States has been developing and deploying military robots with ... \\nnot be in the familiar ways humans are, for their psychology and reasons for \\naction will be quite unlike ours. Thus, Eliezer S. Yudkowsky, among the most \\nprominent of the Friendly AI .... Moral reasoning will always be essential but \\nunfinished.            \n",
       "3  Mar 7, 2018 ... I call this approach “human-centered A.I.” It consists of three goals that can help \\n... on the world — for better or worse — will always be our responsibility. ... New \\nYork edition with the headline: How to Make A.I. Human-Friendly.                                                                                             \n",
       "4  Dec 17, 2013 ... WHY WE NEED FRIENDLY AI. Luke Muehlhauser and Nick Bostrom. Humans \\nwill not always be the most intelligent agents on Earth, the ones ...                                                                                                                                                                                              \n",
       "5  Aug 29, 2017 ... A superintelligent AI will be extremely good at accomplishing its goals, ... pioneer \\nEliezer Yudkowsky has termed “friendly AI”: AI whose goals are aligned with ours\\n. ... We humans accomplish this so effortlessly that it's easy to forget how ... but \\nthe third wish is almost always the same: “please undo the first ...    \n",
       "6  May 14, 2018 ... A.G.I. enthusiasts have had decades to ponder this future, and yet their ... In the \\nNetflix show “Altered Carbon,” A.I. beings scorn humans as “a ..... Therefore, “to \\nprogram a friendly AI, we need to capture the meaning of life.                                                                                               \n",
       "7  Jun 7, 2001 ... analyzes the ways in which AI and human psychology are likely to differ, and the \\nways ..... of Friendly AI must be seen against that background. ..... It is always \\npossible to make engineering assumptions so conservative that ...                                                                                                \n",
       "8  It was just a friendly little argument about the fate of humanity. ... Google's search \\nengine from the beginning has been dependent on A.I. All of these ... to \\neventually create flexible, self-teaching A.I. that will mirror human learning. .... “\\nAnd it's very hard to calibrate how much you are moving because it always looks \\nthe same.”.\n",
       "9  Jul 4, 2018 ... Sentence 1: AI is our friend and it has been friendly. Sentence 2: AI and humans \\nhave always been friendly. In order to calculate similarity using ...                                                                                                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                           Academic sentence - short example\n",
      "0  May 15, 2018 ... Sentence 1: AI is our friend and it has been friendly. Sentence 2: AI and humans \\nhave always been friendly. In order to calculate similarity using ...                                                                                                                                                                                \n",
      "1  Feb 7, 2018 ... Behaviors have always been ascribed to agents with biological drives ... at the \\nCenter for Human-Compatible AI, the big challenge is how to get ...                                                                                                                                                                                    \n",
      "2  But the United States has been developing and deploying military robots with ... \\nnot be in the familiar ways humans are, for their psychology and reasons for \\naction will be quite unlike ours. Thus, Eliezer S. Yudkowsky, among the most \\nprominent of the Friendly AI .... Moral reasoning will always be essential but \\nunfinished.            \n",
      "3  Mar 7, 2018 ... I call this approach “human-centered A.I.” It consists of three goals that can help \\n... on the world — for better or worse — will always be our responsibility. ... New \\nYork edition with the headline: How to Make A.I. Human-Friendly.                                                                                             \n",
      "4  Dec 17, 2013 ... WHY WE NEED FRIENDLY AI. Luke Muehlhauser and Nick Bostrom. Humans \\nwill not always be the most intelligent agents on Earth, the ones ...                                                                                                                                                                                              \n",
      "5  Aug 29, 2017 ... A superintelligent AI will be extremely good at accomplishing its goals, ... pioneer \\nEliezer Yudkowsky has termed “friendly AI”: AI whose goals are aligned with ours\\n. ... We humans accomplish this so effortlessly that it's easy to forget how ... but \\nthe third wish is almost always the same: “please undo the first ...    \n",
      "6  May 14, 2018 ... A.G.I. enthusiasts have had decades to ponder this future, and yet their ... In the \\nNetflix show “Altered Carbon,” A.I. beings scorn humans as “a ..... Therefore, “to \\nprogram a friendly AI, we need to capture the meaning of life.                                                                                               \n",
      "7  Jun 7, 2001 ... analyzes the ways in which AI and human psychology are likely to differ, and the \\nways ..... of Friendly AI must be seen against that background. ..... It is always \\npossible to make engineering assumptions so conservative that ...                                                                                                \n",
      "8  It was just a friendly little argument about the fate of humanity. ... Google's search \\nengine from the beginning has been dependent on A.I. All of these ... to \\neventually create flexible, self-teaching A.I. that will mirror human learning. .... “\\nAnd it's very hard to calibrate how much you are moving because it always looks \\nthe same.”.\n",
      "9  Jul 4, 2018 ... Sentence 1: AI is our friend and it has been friendly. Sentence 2: AI and humans \\nhave always been friendly. In order to calculate similarity using ...                                                                                                                                                                                 \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lower case sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>may 15, 2018 ... sentence 1: ai is our friend and it has been friendly. sentence 2: ai and humans \\nhave always been friendly. in order to calculate similarity using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feb 7, 2018 ... behaviors have always been ascribed to agents with biological drives ... at the \\ncenter for human-compatible ai, the big challenge is how to get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>but the united states has been developing and deploying military robots with ... \\nnot be in the familiar ways humans are, for their psychology and reasons for \\naction will be quite unlike ours. thus, eliezer s. yudkowsky, among the most \\nprominent of the friendly ai .... moral reasoning will always be essential but \\nunfinished.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar 7, 2018 ... i call this approach “human-centered a.i.” it consists of three goals that can help \\n... on the world — for better or worse — will always be our responsibility. ... new \\nyork edition with the headline: how to make a.i. human-friendly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dec 17, 2013 ... why we need friendly ai. luke muehlhauser and nick bostrom. humans \\nwill not always be the most intelligent agents on earth, the ones ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aug 29, 2017 ... a superintelligent ai will be extremely good at accomplishing its goals, ... pioneer \\neliezer yudkowsky has termed “friendly ai”: ai whose goals are aligned with ours\\n. ... we humans accomplish this so effortlessly that it's easy to forget how ... but \\nthe third wish is almost always the same: “please undo the first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>may 14, 2018 ... a.g.i. enthusiasts have had decades to ponder this future, and yet their ... in the \\nnetflix show “altered carbon,” a.i. beings scorn humans as “a ..... therefore, “to \\nprogram a friendly ai, we need to capture the meaning of life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jun 7, 2001 ... analyzes the ways in which ai and human psychology are likely to differ, and the \\nways ..... of friendly ai must be seen against that background. ..... it is always \\npossible to make engineering assumptions so conservative that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>it was just a friendly little argument about the fate of humanity. ... google's search \\nengine from the beginning has been dependent on a.i. all of these ... to \\neventually create flexible, self-teaching a.i. that will mirror human learning. .... “\\nand it's very hard to calibrate how much you are moving because it always looks \\nthe same.”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jul 4, 2018 ... sentence 1: ai is our friend and it has been friendly. sentence 2: ai and humans \\nhave always been friendly. in order to calculate similarity using ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                         Lower case sentence\n",
       "0  may 15, 2018 ... sentence 1: ai is our friend and it has been friendly. sentence 2: ai and humans \\nhave always been friendly. in order to calculate similarity using ...                                                                                                                                                                                \n",
       "1  feb 7, 2018 ... behaviors have always been ascribed to agents with biological drives ... at the \\ncenter for human-compatible ai, the big challenge is how to get ...                                                                                                                                                                                    \n",
       "2  but the united states has been developing and deploying military robots with ... \\nnot be in the familiar ways humans are, for their psychology and reasons for \\naction will be quite unlike ours. thus, eliezer s. yudkowsky, among the most \\nprominent of the friendly ai .... moral reasoning will always be essential but \\nunfinished.            \n",
       "3  mar 7, 2018 ... i call this approach “human-centered a.i.” it consists of three goals that can help \\n... on the world — for better or worse — will always be our responsibility. ... new \\nyork edition with the headline: how to make a.i. human-friendly.                                                                                             \n",
       "4  dec 17, 2013 ... why we need friendly ai. luke muehlhauser and nick bostrom. humans \\nwill not always be the most intelligent agents on earth, the ones ...                                                                                                                                                                                              \n",
       "5  aug 29, 2017 ... a superintelligent ai will be extremely good at accomplishing its goals, ... pioneer \\neliezer yudkowsky has termed “friendly ai”: ai whose goals are aligned with ours\\n. ... we humans accomplish this so effortlessly that it's easy to forget how ... but \\nthe third wish is almost always the same: “please undo the first ...    \n",
       "6  may 14, 2018 ... a.g.i. enthusiasts have had decades to ponder this future, and yet their ... in the \\nnetflix show “altered carbon,” a.i. beings scorn humans as “a ..... therefore, “to \\nprogram a friendly ai, we need to capture the meaning of life.                                                                                               \n",
       "7  jun 7, 2001 ... analyzes the ways in which ai and human psychology are likely to differ, and the \\nways ..... of friendly ai must be seen against that background. ..... it is always \\npossible to make engineering assumptions so conservative that ...                                                                                                \n",
       "8  it was just a friendly little argument about the fate of humanity. ... google's search \\nengine from the beginning has been dependent on a.i. all of these ... to \\neventually create flexible, self-teaching a.i. that will mirror human learning. .... “\\nand it's very hard to calibrate how much you are moving because it always looks \\nthe same.”.\n",
       "9  jul 4, 2018 ... sentence 1: ai is our friend and it has been friendly. sentence 2: ai and humans \\nhave always been friendly. in order to calculate similarity using ...                                                                                                                                                                                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence tokenized into words   - string form and comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>may, 15,, 2018, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly., sentence, 2:, ai, and, humans, have, always, been, friendly., in, order, to, calculate, similarity, using, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feb, 7,, 2018, ..., behaviors, have, always, been, ascribed, to, agents, with, biological, drives, ..., at, the, center, for, human-compatible, ai,, the, big, challenge, is, how, to, get, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>but, the, united, states, has, been, developing, and, deploying, military, robots, with, ..., not, be, in, the, familiar, ways, humans, are,, for, their, psychology, and, reasons, for, action, will, be, quite, unlike, ours., thus,, eliezer, s., yudkowsky,, among, the, most, prominent, of, the, friendly, ai, ...., moral, reasoning, will, always, be, essential, but, unfinished.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar, 7,, 2018, ..., i, call, this, approach, “human-centered, a.i.”, it, consists, of, three, goals, that, can, help, ..., on, the, world, —, for, better, or, worse, —, will, always, be, our, responsibility., ..., new, york, edition, with, the, headline:, how, to, make, a.i., human-friendly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dec, 17,, 2013, ..., why, we, need, friendly, ai., luke, muehlhauser, and, nick, bostrom., humans, will, not, always, be, the, most, intelligent, agents, on, earth,, the, ones, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aug, 29,, 2017, ..., a, superintelligent, ai, will, be, extremely, good, at, accomplishing, its, goals,, ..., pioneer, eliezer, yudkowsky, has, termed, “friendly, ai”:, ai, whose, goals, are, aligned, with, ours, ., ..., we, humans, accomplish, this, so, effortlessly, that, it's, easy, to, forget, how, ..., but, the, third, wish, is, almost, always, the, same:, “please, undo, the, first, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>may, 14,, 2018, ..., a.g.i., enthusiasts, have, had, decades, to, ponder, this, future,, and, yet, their, ..., in, the, netflix, show, “altered, carbon,”, a.i., beings, scorn, humans, as, “a, ....., therefore,, “to, program, a, friendly, ai,, we, need, to, capture, the, meaning, of, life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jun, 7,, 2001, ..., analyzes, the, ways, in, which, ai, and, human, psychology, are, likely, to, differ,, and, the, ways, ....., of, friendly, ai, must, be, seen, against, that, background., ....., it, is, always, possible, to, make, engineering, assumptions, so, conservative, that, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>it, was, just, a, friendly, little, argument, about, the, fate, of, humanity., ..., google's, search, engine, from, the, beginning, has, been, dependent, on, a.i., all, of, these, ..., to, eventually, create, flexible,, self-teaching, a.i., that, will, mirror, human, learning., ...., “, and, it's, very, hard, to, calibrate, how, much, you, are, moving, because, it, always, looks, the, same.”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jul, 4,, 2018, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly., sentence, 2:, ai, and, humans, have, always, been, friendly., in, order, to, calculate, similarity, using, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                 Sentence tokenized into words   - string form and comma separated for display\n",
       "0  may, 15,, 2018, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly., sentence, 2:, ai, and, humans, have, always, been, friendly., in, order, to, calculate, similarity, using, ...                                                                                                                                                                                                      \n",
       "1  feb, 7,, 2018, ..., behaviors, have, always, been, ascribed, to, agents, with, biological, drives, ..., at, the, center, for, human-compatible, ai,, the, big, challenge, is, how, to, get, ...                                                                                                                                                                                                            \n",
       "2  but, the, united, states, has, been, developing, and, deploying, military, robots, with, ..., not, be, in, the, familiar, ways, humans, are,, for, their, psychology, and, reasons, for, action, will, be, quite, unlike, ours., thus,, eliezer, s., yudkowsky,, among, the, most, prominent, of, the, friendly, ai, ...., moral, reasoning, will, always, be, essential, but, unfinished.                 \n",
       "3  mar, 7,, 2018, ..., i, call, this, approach, “human-centered, a.i.”, it, consists, of, three, goals, that, can, help, ..., on, the, world, —, for, better, or, worse, —, will, always, be, our, responsibility., ..., new, york, edition, with, the, headline:, how, to, make, a.i., human-friendly.                                                                                                       \n",
       "4  dec, 17,, 2013, ..., why, we, need, friendly, ai., luke, muehlhauser, and, nick, bostrom., humans, will, not, always, be, the, most, intelligent, agents, on, earth,, the, ones, ...                                                                                                                                                                                                                       \n",
       "5  aug, 29,, 2017, ..., a, superintelligent, ai, will, be, extremely, good, at, accomplishing, its, goals,, ..., pioneer, eliezer, yudkowsky, has, termed, “friendly, ai”:, ai, whose, goals, are, aligned, with, ours, ., ..., we, humans, accomplish, this, so, effortlessly, that, it's, easy, to, forget, how, ..., but, the, third, wish, is, almost, always, the, same:, “please, undo, the, first, ... \n",
       "6  may, 14,, 2018, ..., a.g.i., enthusiasts, have, had, decades, to, ponder, this, future,, and, yet, their, ..., in, the, netflix, show, “altered, carbon,”, a.i., beings, scorn, humans, as, “a, ....., therefore,, “to, program, a, friendly, ai,, we, need, to, capture, the, meaning, of, life.                                                                                                          \n",
       "7  jun, 7,, 2001, ..., analyzes, the, ways, in, which, ai, and, human, psychology, are, likely, to, differ,, and, the, ways, ....., of, friendly, ai, must, be, seen, against, that, background., ....., it, is, always, possible, to, make, engineering, assumptions, so, conservative, that, ...                                                                                                            \n",
       "8  it, was, just, a, friendly, little, argument, about, the, fate, of, humanity., ..., google's, search, engine, from, the, beginning, has, been, dependent, on, a.i., all, of, these, ..., to, eventually, create, flexible,, self-teaching, a.i., that, will, mirror, human, learning., ...., “, and, it's, very, hard, to, calibrate, how, much, you, are, moving, because, it, always, looks, the, same.”.\n",
       "9  jul, 4,, 2018, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly., sentence, 2:, ai, and, humans, have, always, been, friendly., in, order, to, calculate, similarity, using, ...                                                                                                                                                                                                       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single words\n",
      "\n",
      "[['may', '15,', '2018', '...', 'sentence', '1:', 'ai', 'is', 'our', 'friend', 'and', 'it', 'has', 'been', 'friendly.', 'sentence', '2:', 'ai', 'and', 'humans', 'have', 'always', 'been', 'friendly.', 'in', 'order', 'to', 'calculate', 'similarity', 'using', '...'], ['feb', '7,', '2018', '...', 'behaviors', 'have', 'always', 'been', 'ascribed', 'to', 'agents', 'with', 'biological', 'drives', '...', 'at', 'the', 'center', 'for', 'human', 'compatible', 'ai,', 'the', 'big', 'challenge', 'is', 'how', 'to', 'get', '...'], ['but', 'the', 'united', 'states', 'has', 'been', 'developing', 'and', 'deploying', 'military', 'robots', 'with', '...', 'not', 'be', 'in', 'the', 'familiar', 'ways', 'humans', 'are,', 'for', 'their', 'psychology', 'and', 'reasons', 'for', 'action', 'will', 'be', 'quite', 'unlike', 'ours.', 'thus,', 'eliezer', 's.', 'yudkowsky,', 'among', 'the', 'most', 'prominent', 'of', 'the', 'friendly', 'ai', '....', 'moral', 'reasoning', 'will', 'always', 'be', 'essential', 'but', 'unfinished.'], ['mar', '7,', '2018', '...', 'i', 'call', 'this', 'approach', '“human', 'centered', 'a.i.”', 'it', 'consists', 'of', 'three', 'goals', 'that', 'can', 'help', '...', 'on', 'the', 'world', '—', 'for', 'better', 'or', 'worse', '—', 'will', 'always', 'be', 'our', 'responsibility.', '...', 'new', 'york', 'edition', 'with', 'the', 'headline:', 'how', 'to', 'make', 'a.i.', 'human', 'friendly.'], ['dec', '17,', '2013', '...', 'why', 'we', 'need', 'friendly', 'ai.', 'luke', 'muehlhauser', 'and', 'nick', 'bostrom.', 'humans', 'will', 'not', 'always', 'be', 'the', 'most', 'intelligent', 'agents', 'on', 'earth,', 'the', 'ones', '...'], ['aug', '29,', '2017', '...', 'a', 'superintelligent', 'ai', 'will', 'be', 'extremely', 'good', 'at', 'accomplishing', 'its', 'goals,', '...', 'pioneer', 'eliezer', 'yudkowsky', 'has', 'termed', '“friendly', 'ai”:', 'ai', 'whose', 'goals', 'are', 'aligned', 'with', 'ours', '.', '...', 'we', 'humans', 'accomplish', 'this', 'so', 'effortlessly', 'that', \"it's\", 'easy', 'to', 'forget', 'how', '...', 'but', 'the', 'third', 'wish', 'is', 'almost', 'always', 'the', 'same:', '“please', 'undo', 'the', 'first', '...'], ['may', '14,', '2018', '...', 'a.g.i.', 'enthusiasts', 'have', 'had', 'decades', 'to', 'ponder', 'this', 'future,', 'and', 'yet', 'their', '...', 'in', 'the', 'netflix', 'show', '“altered', 'carbon,”', 'a.i.', 'beings', 'scorn', 'humans', 'as', '“a', '.....', 'therefore,', '“to', 'program', 'a', 'friendly', 'ai,', 'we', 'need', 'to', 'capture', 'the', 'meaning', 'of', 'life.'], ['jun', '7,', '2001', '...', 'analyzes', 'the', 'ways', 'in', 'which', 'ai', 'and', 'human', 'psychology', 'are', 'likely', 'to', 'differ,', 'and', 'the', 'ways', '.....', 'of', 'friendly', 'ai', 'must', 'be', 'seen', 'against', 'that', 'background.', '.....', 'it', 'is', 'always', 'possible', 'to', 'make', 'engineering', 'assumptions', 'so', 'conservative', 'that', '...'], ['it', 'was', 'just', 'a', 'friendly', 'little', 'argument', 'about', 'the', 'fate', 'of', 'humanity.', '...', \"google's\", 'search', 'engine', 'from', 'the', 'beginning', 'has', 'been', 'dependent', 'on', 'a.i.', 'all', 'of', 'these', '...', 'to', 'eventually', 'create', 'flexible,', 'self', 'teaching', 'a.i.', 'that', 'will', 'mirror', 'human', 'learning.', '....', '“', 'and', \"it's\", 'very', 'hard', 'to', 'calibrate', 'how', 'much', 'you', 'are', 'moving', 'because', 'it', 'always', 'looks', 'the', 'same.”.'], ['jul', '4,', '2018', '...', 'sentence', '1:', 'ai', 'is', 'our', 'friend', 'and', 'it', 'has', 'been', 'friendly.', 'sentence', '2:', 'ai', 'and', 'humans', 'have', 'always', 'been', 'friendly.', 'in', 'order', 'to', 'calculate', 'similarity', 'using', '...']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Single words   - string form and comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>may, 15,, 2018, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly., sentence, 2:, ai, and, humans, have, always, been, friendly., in, order, to, calculate, similarity, using, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feb, 7,, 2018, ..., behaviors, have, always, been, ascribed, to, agents, with, biological, drives, ..., at, the, center, for, human, compatible, ai,, the, big, challenge, is, how, to, get, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>but, the, united, states, has, been, developing, and, deploying, military, robots, with, ..., not, be, in, the, familiar, ways, humans, are,, for, their, psychology, and, reasons, for, action, will, be, quite, unlike, ours., thus,, eliezer, s., yudkowsky,, among, the, most, prominent, of, the, friendly, ai, ...., moral, reasoning, will, always, be, essential, but, unfinished.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar, 7,, 2018, ..., i, call, this, approach, “human, centered, a.i.”, it, consists, of, three, goals, that, can, help, ..., on, the, world, —, for, better, or, worse, —, will, always, be, our, responsibility., ..., new, york, edition, with, the, headline:, how, to, make, a.i., human, friendly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dec, 17,, 2013, ..., why, we, need, friendly, ai., luke, muehlhauser, and, nick, bostrom., humans, will, not, always, be, the, most, intelligent, agents, on, earth,, the, ones, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aug, 29,, 2017, ..., a, superintelligent, ai, will, be, extremely, good, at, accomplishing, its, goals,, ..., pioneer, eliezer, yudkowsky, has, termed, “friendly, ai”:, ai, whose, goals, are, aligned, with, ours, ., ..., we, humans, accomplish, this, so, effortlessly, that, it's, easy, to, forget, how, ..., but, the, third, wish, is, almost, always, the, same:, “please, undo, the, first, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>may, 14,, 2018, ..., a.g.i., enthusiasts, have, had, decades, to, ponder, this, future,, and, yet, their, ..., in, the, netflix, show, “altered, carbon,”, a.i., beings, scorn, humans, as, “a, ....., therefore,, “to, program, a, friendly, ai,, we, need, to, capture, the, meaning, of, life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jun, 7,, 2001, ..., analyzes, the, ways, in, which, ai, and, human, psychology, are, likely, to, differ,, and, the, ways, ....., of, friendly, ai, must, be, seen, against, that, background., ....., it, is, always, possible, to, make, engineering, assumptions, so, conservative, that, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>it, was, just, a, friendly, little, argument, about, the, fate, of, humanity., ..., google's, search, engine, from, the, beginning, has, been, dependent, on, a.i., all, of, these, ..., to, eventually, create, flexible,, self, teaching, a.i., that, will, mirror, human, learning., ...., “, and, it's, very, hard, to, calibrate, how, much, you, are, moving, because, it, always, looks, the, same.”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jul, 4,, 2018, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly., sentence, 2:, ai, and, humans, have, always, been, friendly., in, order, to, calculate, similarity, using, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                   Single words   - string form and comma separated for display\n",
       "0  may, 15,, 2018, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly., sentence, 2:, ai, and, humans, have, always, been, friendly., in, order, to, calculate, similarity, using, ...                                                                                                                                                                                                       \n",
       "1  feb, 7,, 2018, ..., behaviors, have, always, been, ascribed, to, agents, with, biological, drives, ..., at, the, center, for, human, compatible, ai,, the, big, challenge, is, how, to, get, ...                                                                                                                                                                                                            \n",
       "2  but, the, united, states, has, been, developing, and, deploying, military, robots, with, ..., not, be, in, the, familiar, ways, humans, are,, for, their, psychology, and, reasons, for, action, will, be, quite, unlike, ours., thus,, eliezer, s., yudkowsky,, among, the, most, prominent, of, the, friendly, ai, ...., moral, reasoning, will, always, be, essential, but, unfinished.                  \n",
       "3  mar, 7,, 2018, ..., i, call, this, approach, “human, centered, a.i.”, it, consists, of, three, goals, that, can, help, ..., on, the, world, —, for, better, or, worse, —, will, always, be, our, responsibility., ..., new, york, edition, with, the, headline:, how, to, make, a.i., human, friendly.                                                                                                      \n",
       "4  dec, 17,, 2013, ..., why, we, need, friendly, ai., luke, muehlhauser, and, nick, bostrom., humans, will, not, always, be, the, most, intelligent, agents, on, earth,, the, ones, ...                                                                                                                                                                                                                        \n",
       "5  aug, 29,, 2017, ..., a, superintelligent, ai, will, be, extremely, good, at, accomplishing, its, goals,, ..., pioneer, eliezer, yudkowsky, has, termed, “friendly, ai”:, ai, whose, goals, are, aligned, with, ours, ., ..., we, humans, accomplish, this, so, effortlessly, that, it's, easy, to, forget, how, ..., but, the, third, wish, is, almost, always, the, same:, “please, undo, the, first, ...  \n",
       "6  may, 14,, 2018, ..., a.g.i., enthusiasts, have, had, decades, to, ponder, this, future,, and, yet, their, ..., in, the, netflix, show, “altered, carbon,”, a.i., beings, scorn, humans, as, “a, ....., therefore,, “to, program, a, friendly, ai,, we, need, to, capture, the, meaning, of, life.                                                                                                           \n",
       "7  jun, 7,, 2001, ..., analyzes, the, ways, in, which, ai, and, human, psychology, are, likely, to, differ,, and, the, ways, ....., of, friendly, ai, must, be, seen, against, that, background., ....., it, is, always, possible, to, make, engineering, assumptions, so, conservative, that, ...                                                                                                             \n",
       "8  it, was, just, a, friendly, little, argument, about, the, fate, of, humanity., ..., google's, search, engine, from, the, beginning, has, been, dependent, on, a.i., all, of, these, ..., to, eventually, create, flexible,, self, teaching, a.i., that, will, mirror, human, learning., ...., “, and, it's, very, hard, to, calibrate, how, much, you, are, moving, because, it, always, looks, the, same.”.\n",
       "9  jul, 4,, 2018, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly., sentence, 2:, ai, and, humans, have, always, been, friendly., in, order, to, calculate, similarity, using, ...                                                                                                                                                                                                        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized with alphabetic chars only\n",
      "\n",
      "[['may', 'sentence', 'ai', 'is', 'our', 'friend', 'and', 'it', 'has', 'been', 'friendly', 'sentence', 'ai', 'and', 'humans', 'have', 'always', 'been', 'friendly', 'in', 'order', 'to', 'calculate', 'similarity', 'using'], ['feb', 'behaviors', 'have', 'always', 'been', 'ascribed', 'to', 'agents', 'with', 'biological', 'drives', 'at', 'the', 'center', 'for', 'human', 'compatible', 'ai', 'the', 'big', 'challenge', 'is', 'how', 'to', 'get'], ['but', 'the', 'united', 'states', 'has', 'been', 'developing', 'and', 'deploying', 'military', 'robots', 'with', 'not', 'be', 'in', 'the', 'familiar', 'ways', 'humans', 'are', 'for', 'their', 'psychology', 'and', 'reasons', 'for', 'action', 'will', 'be', 'quite', 'unlike', 'ours', 'thus', 'eliezer', 's', 'yudkowsky', 'among', 'the', 'most', 'prominent', 'of', 'the', 'friendly', 'ai', 'moral', 'reasoning', 'will', 'always', 'be', 'essential', 'but', 'unfinished'], ['mar', 'i', 'call', 'this', 'approach', 'human', 'centered', 'ai', 'it', 'consists', 'of', 'three', 'goals', 'that', 'can', 'help', 'on', 'the', 'world', 'for', 'better', 'or', 'worse', 'will', 'always', 'be', 'our', 'responsibility', 'new', 'york', 'edition', 'with', 'the', 'headline', 'how', 'to', 'make', 'ai', 'human', 'friendly'], ['dec', 'why', 'we', 'need', 'friendly', 'ai', 'luke', 'muehlhauser', 'and', 'nick', 'bostrom', 'humans', 'will', 'not', 'always', 'be', 'the', 'most', 'intelligent', 'agents', 'on', 'earth', 'the', 'ones'], ['aug', 'a', 'superintelligent', 'ai', 'will', 'be', 'extremely', 'good', 'at', 'accomplishing', 'its', 'goals', 'pioneer', 'eliezer', 'yudkowsky', 'has', 'termed', 'friendly', 'ai', 'ai', 'whose', 'goals', 'are', 'aligned', 'with', 'ours', 'we', 'humans', 'accomplish', 'this', 'so', 'effortlessly', 'that', 'its', 'easy', 'to', 'forget', 'how', 'but', 'the', 'third', 'wish', 'is', 'almost', 'always', 'the', 'same', 'please', 'undo', 'the', 'first'], ['may', 'agi', 'enthusiasts', 'have', 'had', 'decades', 'to', 'ponder', 'this', 'future', 'and', 'yet', 'their', 'in', 'the', 'netflix', 'show', 'altered', 'carbon', 'ai', 'beings', 'scorn', 'humans', 'as', 'a', 'therefore', 'to', 'program', 'a', 'friendly', 'ai', 'we', 'need', 'to', 'capture', 'the', 'meaning', 'of', 'life'], ['jun', 'analyzes', 'the', 'ways', 'in', 'which', 'ai', 'and', 'human', 'psychology', 'are', 'likely', 'to', 'differ', 'and', 'the', 'ways', 'of', 'friendly', 'ai', 'must', 'be', 'seen', 'against', 'that', 'background', 'it', 'is', 'always', 'possible', 'to', 'make', 'engineering', 'assumptions', 'so', 'conservative', 'that'], ['it', 'was', 'just', 'a', 'friendly', 'little', 'argument', 'about', 'the', 'fate', 'of', 'humanity', 'googles', 'search', 'engine', 'from', 'the', 'beginning', 'has', 'been', 'dependent', 'on', 'ai', 'all', 'of', 'these', 'to', 'eventually', 'create', 'flexible', 'self', 'teaching', 'ai', 'that', 'will', 'mirror', 'human', 'learning', 'and', 'its', 'very', 'hard', 'to', 'calibrate', 'how', 'much', 'you', 'are', 'moving', 'because', 'it', 'always', 'looks', 'the', 'same'], ['jul', 'sentence', 'ai', 'is', 'our', 'friend', 'and', 'it', 'has', 'been', 'friendly', 'sentence', 'ai', 'and', 'humans', 'have', 'always', 'been', 'friendly', 'in', 'order', 'to', 'calculate', 'similarity', 'using']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized with alphabetic chars only   - string form and comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>may, sentence, ai, is, our, friend, and, it, has, been, friendly, sentence, ai, and, humans, have, always, been, friendly, in, order, to, calculate, similarity, using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feb, behaviors, have, always, been, ascribed, to, agents, with, biological, drives, at, the, center, for, human, compatible, ai, the, big, challenge, is, how, to, get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>but, the, united, states, has, been, developing, and, deploying, military, robots, with, not, be, in, the, familiar, ways, humans, are, for, their, psychology, and, reasons, for, action, will, be, quite, unlike, ours, thus, eliezer, s, yudkowsky, among, the, most, prominent, of, the, friendly, ai, moral, reasoning, will, always, be, essential, but, unfinished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar, i, call, this, approach, human, centered, ai, it, consists, of, three, goals, that, can, help, on, the, world, for, better, or, worse, will, always, be, our, responsibility, new, york, edition, with, the, headline, how, to, make, ai, human, friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dec, why, we, need, friendly, ai, luke, muehlhauser, and, nick, bostrom, humans, will, not, always, be, the, most, intelligent, agents, on, earth, the, ones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aug, a, superintelligent, ai, will, be, extremely, good, at, accomplishing, its, goals, pioneer, eliezer, yudkowsky, has, termed, friendly, ai, ai, whose, goals, are, aligned, with, ours, we, humans, accomplish, this, so, effortlessly, that, its, easy, to, forget, how, but, the, third, wish, is, almost, always, the, same, please, undo, the, first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>may, agi, enthusiasts, have, had, decades, to, ponder, this, future, and, yet, their, in, the, netflix, show, altered, carbon, ai, beings, scorn, humans, as, a, therefore, to, program, a, friendly, ai, we, need, to, capture, the, meaning, of, life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jun, analyzes, the, ways, in, which, ai, and, human, psychology, are, likely, to, differ, and, the, ways, of, friendly, ai, must, be, seen, against, that, background, it, is, always, possible, to, make, engineering, assumptions, so, conservative, that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>it, was, just, a, friendly, little, argument, about, the, fate, of, humanity, googles, search, engine, from, the, beginning, has, been, dependent, on, ai, all, of, these, to, eventually, create, flexible, self, teaching, ai, that, will, mirror, human, learning, and, its, very, hard, to, calibrate, how, much, you, are, moving, because, it, always, looks, the, same</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jul, sentence, ai, is, our, friend, and, it, has, been, friendly, sentence, ai, and, humans, have, always, been, friendly, in, order, to, calculate, similarity, using</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                            Tokenized with alphabetic chars only   - string form and comma separated for display\n",
       "0  may, sentence, ai, is, our, friend, and, it, has, been, friendly, sentence, ai, and, humans, have, always, been, friendly, in, order, to, calculate, similarity, using                                                                                                                                                                                                       \n",
       "1  feb, behaviors, have, always, been, ascribed, to, agents, with, biological, drives, at, the, center, for, human, compatible, ai, the, big, challenge, is, how, to, get                                                                                                                                                                                                       \n",
       "2  but, the, united, states, has, been, developing, and, deploying, military, robots, with, not, be, in, the, familiar, ways, humans, are, for, their, psychology, and, reasons, for, action, will, be, quite, unlike, ours, thus, eliezer, s, yudkowsky, among, the, most, prominent, of, the, friendly, ai, moral, reasoning, will, always, be, essential, but, unfinished    \n",
       "3  mar, i, call, this, approach, human, centered, ai, it, consists, of, three, goals, that, can, help, on, the, world, for, better, or, worse, will, always, be, our, responsibility, new, york, edition, with, the, headline, how, to, make, ai, human, friendly                                                                                                               \n",
       "4  dec, why, we, need, friendly, ai, luke, muehlhauser, and, nick, bostrom, humans, will, not, always, be, the, most, intelligent, agents, on, earth, the, ones                                                                                                                                                                                                                 \n",
       "5  aug, a, superintelligent, ai, will, be, extremely, good, at, accomplishing, its, goals, pioneer, eliezer, yudkowsky, has, termed, friendly, ai, ai, whose, goals, are, aligned, with, ours, we, humans, accomplish, this, so, effortlessly, that, its, easy, to, forget, how, but, the, third, wish, is, almost, always, the, same, please, undo, the, first                 \n",
       "6  may, agi, enthusiasts, have, had, decades, to, ponder, this, future, and, yet, their, in, the, netflix, show, altered, carbon, ai, beings, scorn, humans, as, a, therefore, to, program, a, friendly, ai, we, need, to, capture, the, meaning, of, life                                                                                                                      \n",
       "7  jun, analyzes, the, ways, in, which, ai, and, human, psychology, are, likely, to, differ, and, the, ways, of, friendly, ai, must, be, seen, against, that, background, it, is, always, possible, to, make, engineering, assumptions, so, conservative, that                                                                                                                  \n",
       "8  it, was, just, a, friendly, little, argument, about, the, fate, of, humanity, googles, search, engine, from, the, beginning, has, been, dependent, on, ai, all, of, these, to, eventually, create, flexible, self, teaching, ai, that, will, mirror, human, learning, and, its, very, hard, to, calibrate, how, much, you, are, moving, because, it, always, looks, the, same\n",
       "9  jul, sentence, ai, is, our, friend, and, it, has, been, friendly, sentence, ai, and, humans, have, always, been, friendly, in, order, to, calculate, similarity, using                                                                                                                                                                                                       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English stopwords filtered tokens\n",
      "\n",
      "[['may', 'sentence', 'ai', 'friend', 'friendly', 'sentence', 'ai', 'humans', 'always', 'friendly', 'order', 'calculate', 'similarity', 'using'], ['feb', 'behaviors', 'always', 'ascribed', 'agents', 'biological', 'drives', 'center', 'human', 'compatible', 'ai', 'big', 'challenge', 'get'], ['united', 'states', 'developing', 'deploying', 'military', 'robots', 'familiar', 'ways', 'humans', 'psychology', 'reasons', 'action', 'quite', 'unlike', 'thus', 'eliezer', 'yudkowsky', 'among', 'prominent', 'friendly', 'ai', 'moral', 'reasoning', 'always', 'essential', 'unfinished'], ['mar', 'call', 'approach', 'human', 'centered', 'ai', 'consists', 'three', 'goals', 'help', 'world', 'better', 'worse', 'always', 'responsibility', 'new', 'york', 'edition', 'headline', 'make', 'ai', 'human', 'friendly'], ['dec', 'need', 'friendly', 'ai', 'luke', 'muehlhauser', 'nick', 'bostrom', 'humans', 'always', 'intelligent', 'agents', 'earth', 'ones'], ['aug', 'superintelligent', 'ai', 'extremely', 'good', 'accomplishing', 'goals', 'pioneer', 'eliezer', 'yudkowsky', 'termed', 'friendly', 'ai', 'ai', 'whose', 'goals', 'aligned', 'humans', 'accomplish', 'effortlessly', 'easy', 'forget', 'third', 'wish', 'almost', 'always', 'please', 'undo', 'first'], ['may', 'agi', 'enthusiasts', 'decades', 'ponder', 'future', 'yet', 'netflix', 'show', 'altered', 'carbon', 'ai', 'beings', 'scorn', 'humans', 'therefore', 'program', 'friendly', 'ai', 'need', 'capture', 'meaning', 'life'], ['jun', 'analyzes', 'ways', 'ai', 'human', 'psychology', 'likely', 'differ', 'ways', 'friendly', 'ai', 'must', 'seen', 'background', 'always', 'possible', 'make', 'engineering', 'assumptions', 'conservative'], ['friendly', 'little', 'argument', 'fate', 'humanity', 'googles', 'search', 'engine', 'beginning', 'dependent', 'ai', 'eventually', 'create', 'flexible', 'self', 'teaching', 'ai', 'mirror', 'human', 'learning', 'hard', 'calibrate', 'much', 'moving', 'always', 'looks'], ['jul', 'sentence', 'ai', 'friend', 'friendly', 'sentence', 'ai', 'humans', 'always', 'friendly', 'order', 'calculate', 'similarity', 'using']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English stopwords filtered tokens   - comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>may, sentence, ai, friend, friendly, sentence, ai, humans, always, friendly, order, calculate, similarity, using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feb, behaviors, always, ascribed, agents, biological, drives, center, human, compatible, ai, big, challenge, get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>united, states, developing, deploying, military, robots, familiar, ways, humans, psychology, reasons, action, quite, unlike, thus, eliezer, yudkowsky, among, prominent, friendly, ai, moral, reasoning, always, essential, unfinished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar, call, approach, human, centered, ai, consists, three, goals, help, world, better, worse, always, responsibility, new, york, edition, headline, make, ai, human, friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dec, need, friendly, ai, luke, muehlhauser, nick, bostrom, humans, always, intelligent, agents, earth, ones</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aug, superintelligent, ai, extremely, good, accomplishing, goals, pioneer, eliezer, yudkowsky, termed, friendly, ai, ai, whose, goals, aligned, humans, accomplish, effortlessly, easy, forget, third, wish, almost, always, please, undo, first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>may, agi, enthusiasts, decades, ponder, future, yet, netflix, show, altered, carbon, ai, beings, scorn, humans, therefore, program, friendly, ai, need, capture, meaning, life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jun, analyzes, ways, ai, human, psychology, likely, differ, ways, friendly, ai, must, seen, background, always, possible, make, engineering, assumptions, conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>friendly, little, argument, fate, humanity, googles, search, engine, beginning, dependent, ai, eventually, create, flexible, self, teaching, ai, mirror, human, learning, hard, calibrate, much, moving, always, looks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jul, sentence, ai, friend, friendly, sentence, ai, humans, always, friendly, order, calculate, similarity, using</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                  English stopwords filtered tokens   - comma separated for display\n",
       "0  may, sentence, ai, friend, friendly, sentence, ai, humans, always, friendly, order, calculate, similarity, using                                                                                                                                \n",
       "1  feb, behaviors, always, ascribed, agents, biological, drives, center, human, compatible, ai, big, challenge, get                                                                                                                                \n",
       "2  united, states, developing, deploying, military, robots, familiar, ways, humans, psychology, reasons, action, quite, unlike, thus, eliezer, yudkowsky, among, prominent, friendly, ai, moral, reasoning, always, essential, unfinished          \n",
       "3  mar, call, approach, human, centered, ai, consists, three, goals, help, world, better, worse, always, responsibility, new, york, edition, headline, make, ai, human, friendly                                                                   \n",
       "4  dec, need, friendly, ai, luke, muehlhauser, nick, bostrom, humans, always, intelligent, agents, earth, ones                                                                                                                                     \n",
       "5  aug, superintelligent, ai, extremely, good, accomplishing, goals, pioneer, eliezer, yudkowsky, termed, friendly, ai, ai, whose, goals, aligned, humans, accomplish, effortlessly, easy, forget, third, wish, almost, always, please, undo, first\n",
       "6  may, agi, enthusiasts, decades, ponder, future, yet, netflix, show, altered, carbon, ai, beings, scorn, humans, therefore, program, friendly, ai, need, capture, meaning, life                                                                  \n",
       "7  jun, analyzes, ways, ai, human, psychology, likely, differ, ways, friendly, ai, must, seen, background, always, possible, make, engineering, assumptions, conservative                                                                          \n",
       "8  friendly, little, argument, fate, humanity, googles, search, engine, beginning, dependent, ai, eventually, create, flexible, self, teaching, ai, mirror, human, learning, hard, calibrate, much, moving, always, looks                          \n",
       "9  jul, sentence, ai, friend, friendly, sentence, ai, humans, always, friendly, order, calculate, similarity, using                                                                                                                                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Stemming by PorterStemmer\n",
      "\n",
      "[['may', 'sentenc', 'ai', 'friend', 'friendli', 'sentenc', 'ai', 'human', 'alway', 'friendli', 'order', 'calcul', 'similar', 'use'], ['feb', 'behavior', 'alway', 'ascrib', 'agent', 'biolog', 'drive', 'center', 'human', 'compat', 'ai', 'big', 'challeng', 'get'], ['unit', 'state', 'develop', 'deploy', 'militari', 'robot', 'familiar', 'way', 'human', 'psycholog', 'reason', 'action', 'quit', 'unlik', 'thu', 'eliez', 'yudkowski', 'among', 'promin', 'friendli', 'ai', 'moral', 'reason', 'alway', 'essenti', 'unfinish'], ['mar', 'call', 'approach', 'human', 'center', 'ai', 'consist', 'three', 'goal', 'help', 'world', 'better', 'wors', 'alway', 'respons', 'new', 'york', 'edit', 'headlin', 'make', 'ai', 'human', 'friendli'], ['dec', 'need', 'friendli', 'ai', 'luke', 'muehlhaus', 'nick', 'bostrom', 'human', 'alway', 'intellig', 'agent', 'earth', 'one'], ['aug', 'superintellig', 'ai', 'extrem', 'good', 'accomplish', 'goal', 'pioneer', 'eliez', 'yudkowski', 'term', 'friendli', 'ai', 'ai', 'whose', 'goal', 'align', 'human', 'accomplish', 'effortlessli', 'easi', 'forget', 'third', 'wish', 'almost', 'alway', 'pleas', 'undo', 'first'], ['may', 'agi', 'enthusiast', 'decad', 'ponder', 'futur', 'yet', 'netflix', 'show', 'alter', 'carbon', 'ai', 'be', 'scorn', 'human', 'therefor', 'program', 'friendli', 'ai', 'need', 'captur', 'mean', 'life'], ['jun', 'analyz', 'way', 'ai', 'human', 'psycholog', 'like', 'differ', 'way', 'friendli', 'ai', 'must', 'seen', 'background', 'alway', 'possibl', 'make', 'engin', 'assumpt', 'conserv'], ['friendli', 'littl', 'argument', 'fate', 'human', 'googl', 'search', 'engin', 'begin', 'depend', 'ai', 'eventu', 'creat', 'flexibl', 'self', 'teach', 'ai', 'mirror', 'human', 'learn', 'hard', 'calibr', 'much', 'move', 'alway', 'look'], ['jul', 'sentenc', 'ai', 'friend', 'friendli', 'sentenc', 'ai', 'human', 'alway', 'friendli', 'order', 'calcul', 'similar', 'use']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SnippetList_1_Word Stemming by PorterStemmer   - comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>may, sentenc, ai, friend, friendli, sentenc, ai, human, alway, friendli, order, calcul, similar, use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feb, behavior, alway, ascrib, agent, biolog, drive, center, human, compat, ai, big, challeng, get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unit, state, develop, deploy, militari, robot, familiar, way, human, psycholog, reason, action, quit, unlik, thu, eliez, yudkowski, among, promin, friendli, ai, moral, reason, alway, essenti, unfinish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mar, call, approach, human, center, ai, consist, three, goal, help, world, better, wors, alway, respons, new, york, edit, headlin, make, ai, human, friendli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dec, need, friendli, ai, luke, muehlhaus, nick, bostrom, human, alway, intellig, agent, earth, one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aug, superintellig, ai, extrem, good, accomplish, goal, pioneer, eliez, yudkowski, term, friendli, ai, ai, whose, goal, align, human, accomplish, effortlessli, easi, forget, third, wish, almost, alway, pleas, undo, first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>may, agi, enthusiast, decad, ponder, futur, yet, netflix, show, alter, carbon, ai, be, scorn, human, therefor, program, friendli, ai, need, captur, mean, life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jun, analyz, way, ai, human, psycholog, like, differ, way, friendli, ai, must, seen, background, alway, possibl, make, engin, assumpt, conserv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>friendli, littl, argument, fate, human, googl, search, engin, begin, depend, ai, eventu, creat, flexibl, self, teach, ai, mirror, human, learn, hard, calibr, much, move, alway, look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jul, sentenc, ai, friend, friendli, sentenc, ai, human, alway, friendli, order, calcul, similar, use</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                   SnippetList_1_Word Stemming by PorterStemmer   - comma separated for display\n",
       "0  may, sentenc, ai, friend, friendli, sentenc, ai, human, alway, friendli, order, calcul, similar, use                                                                                                                        \n",
       "1  feb, behavior, alway, ascrib, agent, biolog, drive, center, human, compat, ai, big, challeng, get                                                                                                                           \n",
       "2  unit, state, develop, deploy, militari, robot, familiar, way, human, psycholog, reason, action, quit, unlik, thu, eliez, yudkowski, among, promin, friendli, ai, moral, reason, alway, essenti, unfinish                    \n",
       "3  mar, call, approach, human, center, ai, consist, three, goal, help, world, better, wors, alway, respons, new, york, edit, headlin, make, ai, human, friendli                                                                \n",
       "4  dec, need, friendli, ai, luke, muehlhaus, nick, bostrom, human, alway, intellig, agent, earth, one                                                                                                                          \n",
       "5  aug, superintellig, ai, extrem, good, accomplish, goal, pioneer, eliez, yudkowski, term, friendli, ai, ai, whose, goal, align, human, accomplish, effortlessli, easi, forget, third, wish, almost, alway, pleas, undo, first\n",
       "6  may, agi, enthusiast, decad, ponder, futur, yet, netflix, show, alter, carbon, ai, be, scorn, human, therefor, program, friendli, ai, need, captur, mean, life                                                              \n",
       "7  jun, analyz, way, ai, human, psycholog, like, differ, way, friendli, ai, must, seen, background, alway, possibl, make, engin, assumpt, conserv                                                                              \n",
       "8  friendli, littl, argument, fate, human, googl, search, engin, begin, depend, ai, eventu, creat, flexibl, self, teach, ai, mirror, human, learn, hard, calibr, much, move, alway, look                                       \n",
       "9  jul, sentenc, ai, friend, friendli, sentenc, ai, human, alway, friendli, order, calcul, similar, use                                                                                                                        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text processing-SnippetList_1\n",
    "import nltk\n",
    "import string\n",
    "import math \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "pd.set_option('display.max_columns', None)      # or 1000\n",
    "pd.set_option('display.max_rows', None)         # or 1000\n",
    "pd.set_option('display.max_colwidth', -1)       # or 199\n",
    "\n",
    "\n",
    "\n",
    "# snippets / sentences examples ##########\n",
    "\n",
    "data_all = pd.DataFrame(googleSearchSnippetlist_1)\n",
    "data_all.columns = ['Academic sentence - short example']\n",
    "display(data_all)  \n",
    "print(data_all)\n",
    "\n",
    "low_documents = []\n",
    "for document in googleSearchSnippetlist_1:\n",
    "    low_documents.append(document.lower())\n",
    "    \n",
    "data_low = pd.DataFrame(low_documents)\n",
    "data_low.columns = ['Lower case sentence']\n",
    "display(data_low)  \n",
    "    \n",
    "# tokenization by split # Sentences Tokenized into Words - split by whitespace\n",
    "\n",
    "sentences_documents = []\n",
    "#document_counter = 0\n",
    "for document in low_documents:\n",
    "    sentences_documents.append(document.split())\n",
    "\n",
    "printableList1 = []\n",
    "for sentence1 in sentences_documents:\n",
    "    sentence1AsString = ''\n",
    "    for idx1, aWord1 in enumerate(sentence1):        \n",
    "        if idx1 == len(sentence1) - 1:\n",
    "            sentence1AsString = sentence1AsString + aWord1\n",
    "        else:\n",
    "            str1 = aWord1 + ', '\n",
    "            sentence1AsString = sentence1AsString + str1\n",
    "    printableList1.append(sentence1AsString)\n",
    "\n",
    "data_sentences1 = pd.DataFrame(printableList1)\n",
    "data_sentences1.columns = ['Sentence tokenized into words   - string form and comma separated for display']\n",
    "display(data_sentences1)\n",
    "\n",
    "# change compound words to separate words ie. 'conditional-statements' -> 'conditional', 'statements' \n",
    "print(\"\\n\" 'Single words' \"\\n\")\n",
    "single_word_documents = []\n",
    "for sentence_words in sentences_documents:\n",
    "    single_word_list = []\n",
    "    for word in sentence_words:\n",
    "        regex = re.compile(\"[-_]\")\n",
    "        trimmed = regex.sub(' ', word)\n",
    "        separate = trimmed.split( )\n",
    "        for item in separate:\n",
    "            single_word_list.append(item)        \n",
    "    single_word_documents.append(single_word_list)\n",
    "print(single_word_documents)\n",
    "\n",
    "printableList2 = []\n",
    "for sentence2 in single_word_documents:\n",
    "    sentence2AsString = ''\n",
    "    for idx2, aWord2 in enumerate(sentence2):        \n",
    "        if idx2 == len(sentence2) - 1:\n",
    "            sentence2AsString = sentence2AsString + aWord2\n",
    "        else:\n",
    "            str2 = aWord2 + ', '\n",
    "            sentence2AsString = sentence2AsString + str2\n",
    "    printableList2.append(sentence2AsString)\n",
    "\n",
    "data_sentences2 = pd.DataFrame(printableList2)\n",
    "data_sentences2.columns = ['Single words   - string form and comma separated for display']\n",
    "display(data_sentences2)     \n",
    "    \n",
    "    \n",
    "    \n",
    "# remove all tokens that are not alphabetic #############\n",
    "print(\"\\n\" 'Tokenized with alphabetic chars only' \"\\n\")\n",
    "alpha_documents = []\n",
    "for single_word_sentence in single_word_documents:\n",
    "    cleaned_list = []\n",
    "    for single_word in single_word_sentence:\n",
    "        regex = re.compile('[^a-zA-Z]')\n",
    "        #First parameter is the replacement, second parameter is your input string\n",
    "        nonAlphaRemoved = regex.sub('', single_word)\n",
    "        # add string to list only if it has content\n",
    "        if nonAlphaRemoved:\n",
    "            cleaned_list.append(nonAlphaRemoved)\n",
    "    alpha_documents.append(cleaned_list)\n",
    "print(alpha_documents)\n",
    "\n",
    "printableList3 = []\n",
    "for sentence3 in alpha_documents:\n",
    "    sentence3AsString = ''\n",
    "    for idx3, aWord3 in enumerate(sentence3):        \n",
    "        if idx3 == len(sentence3) - 1:\n",
    "            sentence3AsString = sentence3AsString + aWord3\n",
    "        else:\n",
    "            str3 = aWord3 + ', '\n",
    "            sentence3AsString = sentence3AsString + str3\n",
    "    printableList3.append(sentence3AsString)\n",
    "\n",
    "data_sentences3 = pd.DataFrame(printableList3)\n",
    "data_sentences3.columns = ['Tokenized with alphabetic chars only   - string form and comma separated for display']\n",
    "display(data_sentences3)     \n",
    "\n",
    "\n",
    "# filter out stopwords ########\n",
    "print(\"\\n\" 'English stopwords filtered tokens' \"\\n\")\n",
    "stop_filtered_tokens_1 = []\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for fword in alpha_documents:\n",
    "    fword_list = []\n",
    "    for sword in fword:\n",
    "        #fword_list = [sword for sword in alpha_documents if not sword in english_stop_words]\n",
    "        if not sword in english_stop_words:\n",
    "            fword_list.append(sword)\n",
    "    stop_filtered_tokens_1.append(fword_list)\n",
    "print(stop_filtered_tokens_1)  \n",
    "\n",
    "\n",
    "printableList4 = []\n",
    "for sentence4 in stop_filtered_tokens_1:\n",
    "    sentence4AsString = ''\n",
    "    for idx4, aWord4 in enumerate(sentence4):        \n",
    "        if idx4 == len(sentence4) - 1:\n",
    "            sentence4AsString = sentence4AsString + aWord4\n",
    "        else:\n",
    "            str4 = aWord4 + ', '\n",
    "            sentence4AsString = sentence4AsString + str4\n",
    "    printableList4.append(sentence4AsString)\n",
    "\n",
    "data_sentences4 = pd.DataFrame(printableList4)\n",
    "data_sentences4.columns = ['English stopwords filtered tokens   - comma separated for display']\n",
    "display(data_sentences4)    \n",
    "\n",
    "\n",
    "# tokenization by PorterStemmer ############\n",
    "print(\"\\n\" 'Word Stemming by PorterStemmer' \"\\n\")\n",
    "porter_documents_1 = []\n",
    "for ps_word_list in stop_filtered_tokens_1:\n",
    "    PS = PorterStemmer()\n",
    "    porter_list = []\n",
    "    for ps_word in ps_word_list:\n",
    "        porter_list.append(PS.stem(ps_word))\n",
    "    porter_documents_1.append(porter_list)\n",
    "print(porter_documents_1)\n",
    "\n",
    "printableList5 = []\n",
    "for sentence5 in porter_documents_1:\n",
    "    sentence5AsString = ''\n",
    "    for idx5, aWord5 in enumerate(sentence5):        \n",
    "        if idx5 == len(sentence5) - 1:\n",
    "            sentence5AsString = sentence5AsString + aWord5\n",
    "        else:\n",
    "            str5 = aWord5 + ', '\n",
    "            sentence5AsString = sentence5AsString + str5\n",
    "    printableList5.append(sentence5AsString)\n",
    "\n",
    "data_sentences5 = pd.DataFrame(printableList5)\n",
    "data_sentences5.columns = ['SnippetList_1_Word Stemming by PorterStemmer   - comma separated for display']\n",
    "display(data_sentences5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Academic sentence - short example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It was just a friendly little argument about the fate of humanity. ... Ashlee Vance, \\nthe author of the biography Elon Musk, that he was afraid that his friend Larry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>May 15, 2018 ... I have already talked about custom word embeddings in a previous post, where \\nword ... Sentence 1: AI is our friend and it has been friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Feb 16, 2018 ... \"AI can be our friend,\" says Gates, speaking with \"Hamilton\" composer ... over the \\nlast several hundred years, that has been great for society,\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>May 14, 2018 ... Tad Friend writes that thinking about artificial intelligence can help clarify ... In \\n1988, the roboticist Hans Moravec observed, in what has become ..... Therefore, “\\nto program a friendly AI, we need to capture the meaning of life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jan 9, 2018 ... For example, in 2014 it was claimed that a piece of AI software called ... When we \\nthink of friendship we think of people hanging out together, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jul 4, 2018 ... Sentence 1: AI is our friend and it has been friendly. Sentence 2: AI and humans \\nhave always been friendly. In order to calculate similarity using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Nov 18, 2018 ... Microsoft has built a new type of A.I. assistant that wants to be your friend. ... \\nMicrosoft's friendly Xiaoice A.I can figure out what you want — before you ask ... \\nXiaoice has already been a weather reader on Dragon TV, one of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dec 7, 2017 ... MIT's Sherry Turkle has concerns about Jibo and other sociable AI. ... She has \\nbeen studying children and computers since 1978 and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jun 30, 2018 ... whether an environment is a friend, a foe, or anything in between, remains a \\npoorly ... Keywords: AI safety; friendly and adversarial; game theory; bounded \\nrationality. .... Although each arm was pulled approximately 500 times ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Jun 7, 2001 ... The Machine Intelligence Research Institute was previously known as the .... With \\nCreating Friendly AI, the Singularity Institute has begun to fill in one .... internal \\nsolution, such as duplicating the human friendship instincts,.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                               Academic sentence - short example\n",
       "0  It was just a friendly little argument about the fate of humanity. ... Ashlee Vance, \\nthe author of the biography Elon Musk, that he was afraid that his friend Larry ...                                                                                   \n",
       "1  May 15, 2018 ... I have already talked about custom word embeddings in a previous post, where \\nword ... Sentence 1: AI is our friend and it has been friendly                                                                                               \n",
       "2  Feb 16, 2018 ... \"AI can be our friend,\" says Gates, speaking with \"Hamilton\" composer ... over the \\nlast several hundred years, that has been great for society,\" ...                                                                                      \n",
       "3  May 14, 2018 ... Tad Friend writes that thinking about artificial intelligence can help clarify ... In \\n1988, the roboticist Hans Moravec observed, in what has become ..... Therefore, “\\nto program a friendly AI, we need to capture the meaning of life.\n",
       "4  Jan 9, 2018 ... For example, in 2014 it was claimed that a piece of AI software called ... When we \\nthink of friendship we think of people hanging out together, ...                                                                                        \n",
       "5  Jul 4, 2018 ... Sentence 1: AI is our friend and it has been friendly. Sentence 2: AI and humans \\nhave always been friendly. In order to calculate similarity using ...                                                                                     \n",
       "6  Nov 18, 2018 ... Microsoft has built a new type of A.I. assistant that wants to be your friend. ... \\nMicrosoft's friendly Xiaoice A.I can figure out what you want — before you ask ... \\nXiaoice has already been a weather reader on Dragon TV, one of ...\n",
       "7  Dec 7, 2017 ... MIT's Sherry Turkle has concerns about Jibo and other sociable AI. ... She has \\nbeen studying children and computers since 1978 and the ...                                                                                                 \n",
       "8  Jun 30, 2018 ... whether an environment is a friend, a foe, or anything in between, remains a \\npoorly ... Keywords: AI safety; friendly and adversarial; game theory; bounded \\nrationality. .... Although each arm was pulled approximately 500 times ...  \n",
       "9  Jun 7, 2001 ... The Machine Intelligence Research Institute was previously known as the .... With \\nCreating Friendly AI, the Singularity Institute has begun to fill in one .... internal \\nsolution, such as duplicating the human friendship instincts,.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lower case sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it was just a friendly little argument about the fate of humanity. ... ashlee vance, \\nthe author of the biography elon musk, that he was afraid that his friend larry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>may 15, 2018 ... i have already talked about custom word embeddings in a previous post, where \\nword ... sentence 1: ai is our friend and it has been friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feb 16, 2018 ... \"ai can be our friend,\" says gates, speaking with \"hamilton\" composer ... over the \\nlast several hundred years, that has been great for society,\" ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>may 14, 2018 ... tad friend writes that thinking about artificial intelligence can help clarify ... in \\n1988, the roboticist hans moravec observed, in what has become ..... therefore, “\\nto program a friendly ai, we need to capture the meaning of life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jan 9, 2018 ... for example, in 2014 it was claimed that a piece of ai software called ... when we \\nthink of friendship we think of people hanging out together, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jul 4, 2018 ... sentence 1: ai is our friend and it has been friendly. sentence 2: ai and humans \\nhave always been friendly. in order to calculate similarity using ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nov 18, 2018 ... microsoft has built a new type of a.i. assistant that wants to be your friend. ... \\nmicrosoft's friendly xiaoice a.i can figure out what you want — before you ask ... \\nxiaoice has already been a weather reader on dragon tv, one of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dec 7, 2017 ... mit's sherry turkle has concerns about jibo and other sociable ai. ... she has \\nbeen studying children and computers since 1978 and the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jun 30, 2018 ... whether an environment is a friend, a foe, or anything in between, remains a \\npoorly ... keywords: ai safety; friendly and adversarial; game theory; bounded \\nrationality. .... although each arm was pulled approximately 500 times ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jun 7, 2001 ... the machine intelligence research institute was previously known as the .... with \\ncreating friendly ai, the singularity institute has begun to fill in one .... internal \\nsolution, such as duplicating the human friendship instincts,.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                             Lower case sentence\n",
       "0  it was just a friendly little argument about the fate of humanity. ... ashlee vance, \\nthe author of the biography elon musk, that he was afraid that his friend larry ...                                                                                   \n",
       "1  may 15, 2018 ... i have already talked about custom word embeddings in a previous post, where \\nword ... sentence 1: ai is our friend and it has been friendly                                                                                               \n",
       "2  feb 16, 2018 ... \"ai can be our friend,\" says gates, speaking with \"hamilton\" composer ... over the \\nlast several hundred years, that has been great for society,\" ...                                                                                      \n",
       "3  may 14, 2018 ... tad friend writes that thinking about artificial intelligence can help clarify ... in \\n1988, the roboticist hans moravec observed, in what has become ..... therefore, “\\nto program a friendly ai, we need to capture the meaning of life.\n",
       "4  jan 9, 2018 ... for example, in 2014 it was claimed that a piece of ai software called ... when we \\nthink of friendship we think of people hanging out together, ...                                                                                        \n",
       "5  jul 4, 2018 ... sentence 1: ai is our friend and it has been friendly. sentence 2: ai and humans \\nhave always been friendly. in order to calculate similarity using ...                                                                                     \n",
       "6  nov 18, 2018 ... microsoft has built a new type of a.i. assistant that wants to be your friend. ... \\nmicrosoft's friendly xiaoice a.i can figure out what you want — before you ask ... \\nxiaoice has already been a weather reader on dragon tv, one of ...\n",
       "7  dec 7, 2017 ... mit's sherry turkle has concerns about jibo and other sociable ai. ... she has \\nbeen studying children and computers since 1978 and the ...                                                                                                 \n",
       "8  jun 30, 2018 ... whether an environment is a friend, a foe, or anything in between, remains a \\npoorly ... keywords: ai safety; friendly and adversarial; game theory; bounded \\nrationality. .... although each arm was pulled approximately 500 times ...  \n",
       "9  jun 7, 2001 ... the machine intelligence research institute was previously known as the .... with \\ncreating friendly ai, the singularity institute has begun to fill in one .... internal \\nsolution, such as duplicating the human friendship instincts,.  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence tokenized into words   - string form and comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it, was, just, a, friendly, little, argument, about, the, fate, of, humanity., ..., ashlee, vance,, the, author, of, the, biography, elon, musk,, that, he, was, afraid, that, his, friend, larry, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>may, 15,, 2018, ..., i, have, already, talked, about, custom, word, embeddings, in, a, previous, post,, where, word, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feb, 16,, 2018, ..., \"ai, can, be, our, friend,\", says, gates,, speaking, with, \"hamilton\", composer, ..., over, the, last, several, hundred, years,, that, has, been, great, for, society,\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>may, 14,, 2018, ..., tad, friend, writes, that, thinking, about, artificial, intelligence, can, help, clarify, ..., in, 1988,, the, roboticist, hans, moravec, observed,, in, what, has, become, ....., therefore,, “, to, program, a, friendly, ai,, we, need, to, capture, the, meaning, of, life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jan, 9,, 2018, ..., for, example,, in, 2014, it, was, claimed, that, a, piece, of, ai, software, called, ..., when, we, think, of, friendship, we, think, of, people, hanging, out, together,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jul, 4,, 2018, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly., sentence, 2:, ai, and, humans, have, always, been, friendly., in, order, to, calculate, similarity, using, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nov, 18,, 2018, ..., microsoft, has, built, a, new, type, of, a.i., assistant, that, wants, to, be, your, friend., ..., microsoft's, friendly, xiaoice, a.i, can, figure, out, what, you, want, —, before, you, ask, ..., xiaoice, has, already, been, a, weather, reader, on, dragon, tv,, one, of, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dec, 7,, 2017, ..., mit's, sherry, turkle, has, concerns, about, jibo, and, other, sociable, ai., ..., she, has, been, studying, children, and, computers, since, 1978, and, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jun, 30,, 2018, ..., whether, an, environment, is, a, friend,, a, foe,, or, anything, in, between,, remains, a, poorly, ..., keywords:, ai, safety;, friendly, and, adversarial;, game, theory;, bounded, rationality., ...., although, each, arm, was, pulled, approximately, 500, times, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jun, 7,, 2001, ..., the, machine, intelligence, research, institute, was, previously, known, as, the, ...., with, creating, friendly, ai,, the, singularity, institute, has, begun, to, fill, in, one, ...., internal, solution,, such, as, duplicating, the, human, friendship, instincts,.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                              Sentence tokenized into words   - string form and comma separated for display\n",
       "0  it, was, just, a, friendly, little, argument, about, the, fate, of, humanity., ..., ashlee, vance,, the, author, of, the, biography, elon, musk,, that, he, was, afraid, that, his, friend, larry, ...                                                                                                  \n",
       "1  may, 15,, 2018, ..., i, have, already, talked, about, custom, word, embeddings, in, a, previous, post,, where, word, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly                                                                                                               \n",
       "2  feb, 16,, 2018, ..., \"ai, can, be, our, friend,\", says, gates,, speaking, with, \"hamilton\", composer, ..., over, the, last, several, hundred, years,, that, has, been, great, for, society,\", ...                                                                                                       \n",
       "3  may, 14,, 2018, ..., tad, friend, writes, that, thinking, about, artificial, intelligence, can, help, clarify, ..., in, 1988,, the, roboticist, hans, moravec, observed,, in, what, has, become, ....., therefore,, “, to, program, a, friendly, ai,, we, need, to, capture, the, meaning, of, life.    \n",
       "4  jan, 9,, 2018, ..., for, example,, in, 2014, it, was, claimed, that, a, piece, of, ai, software, called, ..., when, we, think, of, friendship, we, think, of, people, hanging, out, together,, ...                                                                                                      \n",
       "5  jul, 4,, 2018, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly., sentence, 2:, ai, and, humans, have, always, been, friendly., in, order, to, calculate, similarity, using, ...                                                                                                    \n",
       "6  nov, 18,, 2018, ..., microsoft, has, built, a, new, type, of, a.i., assistant, that, wants, to, be, your, friend., ..., microsoft's, friendly, xiaoice, a.i, can, figure, out, what, you, want, —, before, you, ask, ..., xiaoice, has, already, been, a, weather, reader, on, dragon, tv,, one, of, ...\n",
       "7  dec, 7,, 2017, ..., mit's, sherry, turkle, has, concerns, about, jibo, and, other, sociable, ai., ..., she, has, been, studying, children, and, computers, since, 1978, and, the, ...                                                                                                                   \n",
       "8  jun, 30,, 2018, ..., whether, an, environment, is, a, friend,, a, foe,, or, anything, in, between,, remains, a, poorly, ..., keywords:, ai, safety;, friendly, and, adversarial;, game, theory;, bounded, rationality., ...., although, each, arm, was, pulled, approximately, 500, times, ...          \n",
       "9  jun, 7,, 2001, ..., the, machine, intelligence, research, institute, was, previously, known, as, the, ...., with, creating, friendly, ai,, the, singularity, institute, has, begun, to, fill, in, one, ...., internal, solution,, such, as, duplicating, the, human, friendship, instincts,.            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single words\n",
      "\n",
      "[['it', 'was', 'just', 'a', 'friendly', 'little', 'argument', 'about', 'the', 'fate', 'of', 'humanity.', '...', 'ashlee', 'vance,', 'the', 'author', 'of', 'the', 'biography', 'elon', 'musk,', 'that', 'he', 'was', 'afraid', 'that', 'his', 'friend', 'larry', '...'], ['may', '15,', '2018', '...', 'i', 'have', 'already', 'talked', 'about', 'custom', 'word', 'embeddings', 'in', 'a', 'previous', 'post,', 'where', 'word', '...', 'sentence', '1:', 'ai', 'is', 'our', 'friend', 'and', 'it', 'has', 'been', 'friendly'], ['feb', '16,', '2018', '...', '\"ai', 'can', 'be', 'our', 'friend,\"', 'says', 'gates,', 'speaking', 'with', '\"hamilton\"', 'composer', '...', 'over', 'the', 'last', 'several', 'hundred', 'years,', 'that', 'has', 'been', 'great', 'for', 'society,\"', '...'], ['may', '14,', '2018', '...', 'tad', 'friend', 'writes', 'that', 'thinking', 'about', 'artificial', 'intelligence', 'can', 'help', 'clarify', '...', 'in', '1988,', 'the', 'roboticist', 'hans', 'moravec', 'observed,', 'in', 'what', 'has', 'become', '.....', 'therefore,', '“', 'to', 'program', 'a', 'friendly', 'ai,', 'we', 'need', 'to', 'capture', 'the', 'meaning', 'of', 'life.'], ['jan', '9,', '2018', '...', 'for', 'example,', 'in', '2014', 'it', 'was', 'claimed', 'that', 'a', 'piece', 'of', 'ai', 'software', 'called', '...', 'when', 'we', 'think', 'of', 'friendship', 'we', 'think', 'of', 'people', 'hanging', 'out', 'together,', '...'], ['jul', '4,', '2018', '...', 'sentence', '1:', 'ai', 'is', 'our', 'friend', 'and', 'it', 'has', 'been', 'friendly.', 'sentence', '2:', 'ai', 'and', 'humans', 'have', 'always', 'been', 'friendly.', 'in', 'order', 'to', 'calculate', 'similarity', 'using', '...'], ['nov', '18,', '2018', '...', 'microsoft', 'has', 'built', 'a', 'new', 'type', 'of', 'a.i.', 'assistant', 'that', 'wants', 'to', 'be', 'your', 'friend.', '...', \"microsoft's\", 'friendly', 'xiaoice', 'a.i', 'can', 'figure', 'out', 'what', 'you', 'want', '—', 'before', 'you', 'ask', '...', 'xiaoice', 'has', 'already', 'been', 'a', 'weather', 'reader', 'on', 'dragon', 'tv,', 'one', 'of', '...'], ['dec', '7,', '2017', '...', \"mit's\", 'sherry', 'turkle', 'has', 'concerns', 'about', 'jibo', 'and', 'other', 'sociable', 'ai.', '...', 'she', 'has', 'been', 'studying', 'children', 'and', 'computers', 'since', '1978', 'and', 'the', '...'], ['jun', '30,', '2018', '...', 'whether', 'an', 'environment', 'is', 'a', 'friend,', 'a', 'foe,', 'or', 'anything', 'in', 'between,', 'remains', 'a', 'poorly', '...', 'keywords:', 'ai', 'safety;', 'friendly', 'and', 'adversarial;', 'game', 'theory;', 'bounded', 'rationality.', '....', 'although', 'each', 'arm', 'was', 'pulled', 'approximately', '500', 'times', '...'], ['jun', '7,', '2001', '...', 'the', 'machine', 'intelligence', 'research', 'institute', 'was', 'previously', 'known', 'as', 'the', '....', 'with', 'creating', 'friendly', 'ai,', 'the', 'singularity', 'institute', 'has', 'begun', 'to', 'fill', 'in', 'one', '....', 'internal', 'solution,', 'such', 'as', 'duplicating', 'the', 'human', 'friendship', 'instincts,.']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Single words   - string form and comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it, was, just, a, friendly, little, argument, about, the, fate, of, humanity., ..., ashlee, vance,, the, author, of, the, biography, elon, musk,, that, he, was, afraid, that, his, friend, larry, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>may, 15,, 2018, ..., i, have, already, talked, about, custom, word, embeddings, in, a, previous, post,, where, word, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feb, 16,, 2018, ..., \"ai, can, be, our, friend,\", says, gates,, speaking, with, \"hamilton\", composer, ..., over, the, last, several, hundred, years,, that, has, been, great, for, society,\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>may, 14,, 2018, ..., tad, friend, writes, that, thinking, about, artificial, intelligence, can, help, clarify, ..., in, 1988,, the, roboticist, hans, moravec, observed,, in, what, has, become, ....., therefore,, “, to, program, a, friendly, ai,, we, need, to, capture, the, meaning, of, life.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jan, 9,, 2018, ..., for, example,, in, 2014, it, was, claimed, that, a, piece, of, ai, software, called, ..., when, we, think, of, friendship, we, think, of, people, hanging, out, together,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jul, 4,, 2018, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly., sentence, 2:, ai, and, humans, have, always, been, friendly., in, order, to, calculate, similarity, using, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nov, 18,, 2018, ..., microsoft, has, built, a, new, type, of, a.i., assistant, that, wants, to, be, your, friend., ..., microsoft's, friendly, xiaoice, a.i, can, figure, out, what, you, want, —, before, you, ask, ..., xiaoice, has, already, been, a, weather, reader, on, dragon, tv,, one, of, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dec, 7,, 2017, ..., mit's, sherry, turkle, has, concerns, about, jibo, and, other, sociable, ai., ..., she, has, been, studying, children, and, computers, since, 1978, and, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jun, 30,, 2018, ..., whether, an, environment, is, a, friend,, a, foe,, or, anything, in, between,, remains, a, poorly, ..., keywords:, ai, safety;, friendly, and, adversarial;, game, theory;, bounded, rationality., ...., although, each, arm, was, pulled, approximately, 500, times, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jun, 7,, 2001, ..., the, machine, intelligence, research, institute, was, previously, known, as, the, ...., with, creating, friendly, ai,, the, singularity, institute, has, begun, to, fill, in, one, ...., internal, solution,, such, as, duplicating, the, human, friendship, instincts,.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                               Single words   - string form and comma separated for display\n",
       "0  it, was, just, a, friendly, little, argument, about, the, fate, of, humanity., ..., ashlee, vance,, the, author, of, the, biography, elon, musk,, that, he, was, afraid, that, his, friend, larry, ...                                                                                                  \n",
       "1  may, 15,, 2018, ..., i, have, already, talked, about, custom, word, embeddings, in, a, previous, post,, where, word, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly                                                                                                               \n",
       "2  feb, 16,, 2018, ..., \"ai, can, be, our, friend,\", says, gates,, speaking, with, \"hamilton\", composer, ..., over, the, last, several, hundred, years,, that, has, been, great, for, society,\", ...                                                                                                       \n",
       "3  may, 14,, 2018, ..., tad, friend, writes, that, thinking, about, artificial, intelligence, can, help, clarify, ..., in, 1988,, the, roboticist, hans, moravec, observed,, in, what, has, become, ....., therefore,, “, to, program, a, friendly, ai,, we, need, to, capture, the, meaning, of, life.    \n",
       "4  jan, 9,, 2018, ..., for, example,, in, 2014, it, was, claimed, that, a, piece, of, ai, software, called, ..., when, we, think, of, friendship, we, think, of, people, hanging, out, together,, ...                                                                                                      \n",
       "5  jul, 4,, 2018, ..., sentence, 1:, ai, is, our, friend, and, it, has, been, friendly., sentence, 2:, ai, and, humans, have, always, been, friendly., in, order, to, calculate, similarity, using, ...                                                                                                    \n",
       "6  nov, 18,, 2018, ..., microsoft, has, built, a, new, type, of, a.i., assistant, that, wants, to, be, your, friend., ..., microsoft's, friendly, xiaoice, a.i, can, figure, out, what, you, want, —, before, you, ask, ..., xiaoice, has, already, been, a, weather, reader, on, dragon, tv,, one, of, ...\n",
       "7  dec, 7,, 2017, ..., mit's, sherry, turkle, has, concerns, about, jibo, and, other, sociable, ai., ..., she, has, been, studying, children, and, computers, since, 1978, and, the, ...                                                                                                                   \n",
       "8  jun, 30,, 2018, ..., whether, an, environment, is, a, friend,, a, foe,, or, anything, in, between,, remains, a, poorly, ..., keywords:, ai, safety;, friendly, and, adversarial;, game, theory;, bounded, rationality., ...., although, each, arm, was, pulled, approximately, 500, times, ...          \n",
       "9  jun, 7,, 2001, ..., the, machine, intelligence, research, institute, was, previously, known, as, the, ...., with, creating, friendly, ai,, the, singularity, institute, has, begun, to, fill, in, one, ...., internal, solution,, such, as, duplicating, the, human, friendship, instincts,.            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized with alphabetic chars only\n",
      "\n",
      "[['it', 'was', 'just', 'a', 'friendly', 'little', 'argument', 'about', 'the', 'fate', 'of', 'humanity', 'ashlee', 'vance', 'the', 'author', 'of', 'the', 'biography', 'elon', 'musk', 'that', 'he', 'was', 'afraid', 'that', 'his', 'friend', 'larry'], ['may', 'i', 'have', 'already', 'talked', 'about', 'custom', 'word', 'embeddings', 'in', 'a', 'previous', 'post', 'where', 'word', 'sentence', 'ai', 'is', 'our', 'friend', 'and', 'it', 'has', 'been', 'friendly'], ['feb', 'ai', 'can', 'be', 'our', 'friend', 'says', 'gates', 'speaking', 'with', 'hamilton', 'composer', 'over', 'the', 'last', 'several', 'hundred', 'years', 'that', 'has', 'been', 'great', 'for', 'society'], ['may', 'tad', 'friend', 'writes', 'that', 'thinking', 'about', 'artificial', 'intelligence', 'can', 'help', 'clarify', 'in', 'the', 'roboticist', 'hans', 'moravec', 'observed', 'in', 'what', 'has', 'become', 'therefore', 'to', 'program', 'a', 'friendly', 'ai', 'we', 'need', 'to', 'capture', 'the', 'meaning', 'of', 'life'], ['jan', 'for', 'example', 'in', 'it', 'was', 'claimed', 'that', 'a', 'piece', 'of', 'ai', 'software', 'called', 'when', 'we', 'think', 'of', 'friendship', 'we', 'think', 'of', 'people', 'hanging', 'out', 'together'], ['jul', 'sentence', 'ai', 'is', 'our', 'friend', 'and', 'it', 'has', 'been', 'friendly', 'sentence', 'ai', 'and', 'humans', 'have', 'always', 'been', 'friendly', 'in', 'order', 'to', 'calculate', 'similarity', 'using'], ['nov', 'microsoft', 'has', 'built', 'a', 'new', 'type', 'of', 'ai', 'assistant', 'that', 'wants', 'to', 'be', 'your', 'friend', 'microsofts', 'friendly', 'xiaoice', 'ai', 'can', 'figure', 'out', 'what', 'you', 'want', 'before', 'you', 'ask', 'xiaoice', 'has', 'already', 'been', 'a', 'weather', 'reader', 'on', 'dragon', 'tv', 'one', 'of'], ['dec', 'mits', 'sherry', 'turkle', 'has', 'concerns', 'about', 'jibo', 'and', 'other', 'sociable', 'ai', 'she', 'has', 'been', 'studying', 'children', 'and', 'computers', 'since', 'and', 'the'], ['jun', 'whether', 'an', 'environment', 'is', 'a', 'friend', 'a', 'foe', 'or', 'anything', 'in', 'between', 'remains', 'a', 'poorly', 'keywords', 'ai', 'safety', 'friendly', 'and', 'adversarial', 'game', 'theory', 'bounded', 'rationality', 'although', 'each', 'arm', 'was', 'pulled', 'approximately', 'times'], ['jun', 'the', 'machine', 'intelligence', 'research', 'institute', 'was', 'previously', 'known', 'as', 'the', 'with', 'creating', 'friendly', 'ai', 'the', 'singularity', 'institute', 'has', 'begun', 'to', 'fill', 'in', 'one', 'internal', 'solution', 'such', 'as', 'duplicating', 'the', 'human', 'friendship', 'instincts']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized with alphabetic chars only   - string form and comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it, was, just, a, friendly, little, argument, about, the, fate, of, humanity, ashlee, vance, the, author, of, the, biography, elon, musk, that, he, was, afraid, that, his, friend, larry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>may, i, have, already, talked, about, custom, word, embeddings, in, a, previous, post, where, word, sentence, ai, is, our, friend, and, it, has, been, friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feb, ai, can, be, our, friend, says, gates, speaking, with, hamilton, composer, over, the, last, several, hundred, years, that, has, been, great, for, society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>may, tad, friend, writes, that, thinking, about, artificial, intelligence, can, help, clarify, in, the, roboticist, hans, moravec, observed, in, what, has, become, therefore, to, program, a, friendly, ai, we, need, to, capture, the, meaning, of, life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jan, for, example, in, it, was, claimed, that, a, piece, of, ai, software, called, when, we, think, of, friendship, we, think, of, people, hanging, out, together</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jul, sentence, ai, is, our, friend, and, it, has, been, friendly, sentence, ai, and, humans, have, always, been, friendly, in, order, to, calculate, similarity, using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nov, microsoft, has, built, a, new, type, of, ai, assistant, that, wants, to, be, your, friend, microsofts, friendly, xiaoice, ai, can, figure, out, what, you, want, before, you, ask, xiaoice, has, already, been, a, weather, reader, on, dragon, tv, one, of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dec, mits, sherry, turkle, has, concerns, about, jibo, and, other, sociable, ai, she, has, been, studying, children, and, computers, since, and, the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jun, whether, an, environment, is, a, friend, a, foe, or, anything, in, between, remains, a, poorly, keywords, ai, safety, friendly, and, adversarial, game, theory, bounded, rationality, although, each, arm, was, pulled, approximately, times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jun, the, machine, intelligence, research, institute, was, previously, known, as, the, with, creating, friendly, ai, the, singularity, institute, has, begun, to, fill, in, one, internal, solution, such, as, duplicating, the, human, friendship, instincts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                               Tokenized with alphabetic chars only   - string form and comma separated for display\n",
       "0  it, was, just, a, friendly, little, argument, about, the, fate, of, humanity, ashlee, vance, the, author, of, the, biography, elon, musk, that, he, was, afraid, that, his, friend, larry                                                                       \n",
       "1  may, i, have, already, talked, about, custom, word, embeddings, in, a, previous, post, where, word, sentence, ai, is, our, friend, and, it, has, been, friendly                                                                                                 \n",
       "2  feb, ai, can, be, our, friend, says, gates, speaking, with, hamilton, composer, over, the, last, several, hundred, years, that, has, been, great, for, society                                                                                                  \n",
       "3  may, tad, friend, writes, that, thinking, about, artificial, intelligence, can, help, clarify, in, the, roboticist, hans, moravec, observed, in, what, has, become, therefore, to, program, a, friendly, ai, we, need, to, capture, the, meaning, of, life      \n",
       "4  jan, for, example, in, it, was, claimed, that, a, piece, of, ai, software, called, when, we, think, of, friendship, we, think, of, people, hanging, out, together                                                                                               \n",
       "5  jul, sentence, ai, is, our, friend, and, it, has, been, friendly, sentence, ai, and, humans, have, always, been, friendly, in, order, to, calculate, similarity, using                                                                                          \n",
       "6  nov, microsoft, has, built, a, new, type, of, ai, assistant, that, wants, to, be, your, friend, microsofts, friendly, xiaoice, ai, can, figure, out, what, you, want, before, you, ask, xiaoice, has, already, been, a, weather, reader, on, dragon, tv, one, of\n",
       "7  dec, mits, sherry, turkle, has, concerns, about, jibo, and, other, sociable, ai, she, has, been, studying, children, and, computers, since, and, the                                                                                                            \n",
       "8  jun, whether, an, environment, is, a, friend, a, foe, or, anything, in, between, remains, a, poorly, keywords, ai, safety, friendly, and, adversarial, game, theory, bounded, rationality, although, each, arm, was, pulled, approximately, times               \n",
       "9  jun, the, machine, intelligence, research, institute, was, previously, known, as, the, with, creating, friendly, ai, the, singularity, institute, has, begun, to, fill, in, one, internal, solution, such, as, duplicating, the, human, friendship, instincts   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "English stopwords filtered tokens\n",
      "\n",
      "[['friendly', 'little', 'argument', 'fate', 'humanity', 'ashlee', 'vance', 'author', 'biography', 'elon', 'musk', 'afraid', 'friend', 'larry'], ['may', 'already', 'talked', 'custom', 'word', 'embeddings', 'previous', 'post', 'word', 'sentence', 'ai', 'friend', 'friendly'], ['feb', 'ai', 'friend', 'says', 'gates', 'speaking', 'hamilton', 'composer', 'last', 'several', 'hundred', 'years', 'great', 'society'], ['may', 'tad', 'friend', 'writes', 'thinking', 'artificial', 'intelligence', 'help', 'clarify', 'roboticist', 'hans', 'moravec', 'observed', 'become', 'therefore', 'program', 'friendly', 'ai', 'need', 'capture', 'meaning', 'life'], ['jan', 'example', 'claimed', 'piece', 'ai', 'software', 'called', 'think', 'friendship', 'think', 'people', 'hanging', 'together'], ['jul', 'sentence', 'ai', 'friend', 'friendly', 'sentence', 'ai', 'humans', 'always', 'friendly', 'order', 'calculate', 'similarity', 'using'], ['nov', 'microsoft', 'built', 'new', 'type', 'ai', 'assistant', 'wants', 'friend', 'microsofts', 'friendly', 'xiaoice', 'ai', 'figure', 'want', 'ask', 'xiaoice', 'already', 'weather', 'reader', 'dragon', 'tv', 'one'], ['dec', 'mits', 'sherry', 'turkle', 'concerns', 'jibo', 'sociable', 'ai', 'studying', 'children', 'computers', 'since'], ['jun', 'whether', 'environment', 'friend', 'foe', 'anything', 'remains', 'poorly', 'keywords', 'ai', 'safety', 'friendly', 'adversarial', 'game', 'theory', 'bounded', 'rationality', 'although', 'arm', 'pulled', 'approximately', 'times'], ['jun', 'machine', 'intelligence', 'research', 'institute', 'previously', 'known', 'creating', 'friendly', 'ai', 'singularity', 'institute', 'begun', 'fill', 'one', 'internal', 'solution', 'duplicating', 'human', 'friendship', 'instincts']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English stopwords filtered tokens   - comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>friendly, little, argument, fate, humanity, ashlee, vance, author, biography, elon, musk, afraid, friend, larry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>may, already, talked, custom, word, embeddings, previous, post, word, sentence, ai, friend, friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feb, ai, friend, says, gates, speaking, hamilton, composer, last, several, hundred, years, great, society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>may, tad, friend, writes, thinking, artificial, intelligence, help, clarify, roboticist, hans, moravec, observed, become, therefore, program, friendly, ai, need, capture, meaning, life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jan, example, claimed, piece, ai, software, called, think, friendship, think, people, hanging, together</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jul, sentence, ai, friend, friendly, sentence, ai, humans, always, friendly, order, calculate, similarity, using</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nov, microsoft, built, new, type, ai, assistant, wants, friend, microsofts, friendly, xiaoice, ai, figure, want, ask, xiaoice, already, weather, reader, dragon, tv, one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dec, mits, sherry, turkle, concerns, jibo, sociable, ai, studying, children, computers, since</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jun, whether, environment, friend, foe, anything, remains, poorly, keywords, ai, safety, friendly, adversarial, game, theory, bounded, rationality, although, arm, pulled, approximately, times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jun, machine, intelligence, research, institute, previously, known, creating, friendly, ai, singularity, institute, begun, fill, one, internal, solution, duplicating, human, friendship, instincts</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     English stopwords filtered tokens   - comma separated for display\n",
       "0  friendly, little, argument, fate, humanity, ashlee, vance, author, biography, elon, musk, afraid, friend, larry                                                                                    \n",
       "1  may, already, talked, custom, word, embeddings, previous, post, word, sentence, ai, friend, friendly                                                                                               \n",
       "2  feb, ai, friend, says, gates, speaking, hamilton, composer, last, several, hundred, years, great, society                                                                                          \n",
       "3  may, tad, friend, writes, thinking, artificial, intelligence, help, clarify, roboticist, hans, moravec, observed, become, therefore, program, friendly, ai, need, capture, meaning, life           \n",
       "4  jan, example, claimed, piece, ai, software, called, think, friendship, think, people, hanging, together                                                                                            \n",
       "5  jul, sentence, ai, friend, friendly, sentence, ai, humans, always, friendly, order, calculate, similarity, using                                                                                   \n",
       "6  nov, microsoft, built, new, type, ai, assistant, wants, friend, microsofts, friendly, xiaoice, ai, figure, want, ask, xiaoice, already, weather, reader, dragon, tv, one                           \n",
       "7  dec, mits, sherry, turkle, concerns, jibo, sociable, ai, studying, children, computers, since                                                                                                      \n",
       "8  jun, whether, environment, friend, foe, anything, remains, poorly, keywords, ai, safety, friendly, adversarial, game, theory, bounded, rationality, although, arm, pulled, approximately, times    \n",
       "9  jun, machine, intelligence, research, institute, previously, known, creating, friendly, ai, singularity, institute, begun, fill, one, internal, solution, duplicating, human, friendship, instincts"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Stemming by PorterStemmer\n",
      "\n",
      "[['friendli', 'littl', 'argument', 'fate', 'human', 'ashle', 'vanc', 'author', 'biographi', 'elon', 'musk', 'afraid', 'friend', 'larri'], ['may', 'alreadi', 'talk', 'custom', 'word', 'embed', 'previou', 'post', 'word', 'sentenc', 'ai', 'friend', 'friendli'], ['feb', 'ai', 'friend', 'say', 'gate', 'speak', 'hamilton', 'compos', 'last', 'sever', 'hundr', 'year', 'great', 'societi'], ['may', 'tad', 'friend', 'write', 'think', 'artifici', 'intellig', 'help', 'clarifi', 'roboticist', 'han', 'moravec', 'observ', 'becom', 'therefor', 'program', 'friendli', 'ai', 'need', 'captur', 'mean', 'life'], ['jan', 'exampl', 'claim', 'piec', 'ai', 'softwar', 'call', 'think', 'friendship', 'think', 'peopl', 'hang', 'togeth'], ['jul', 'sentenc', 'ai', 'friend', 'friendli', 'sentenc', 'ai', 'human', 'alway', 'friendli', 'order', 'calcul', 'similar', 'use'], ['nov', 'microsoft', 'built', 'new', 'type', 'ai', 'assist', 'want', 'friend', 'microsoft', 'friendli', 'xiaoic', 'ai', 'figur', 'want', 'ask', 'xiaoic', 'alreadi', 'weather', 'reader', 'dragon', 'tv', 'one'], ['dec', 'mit', 'sherri', 'turkl', 'concern', 'jibo', 'sociabl', 'ai', 'studi', 'children', 'comput', 'sinc'], ['jun', 'whether', 'environ', 'friend', 'foe', 'anyth', 'remain', 'poorli', 'keyword', 'ai', 'safeti', 'friendli', 'adversari', 'game', 'theori', 'bound', 'ration', 'although', 'arm', 'pull', 'approxim', 'time'], ['jun', 'machin', 'intellig', 'research', 'institut', 'previous', 'known', 'creat', 'friendli', 'ai', 'singular', 'institut', 'begun', 'fill', 'one', 'intern', 'solut', 'duplic', 'human', 'friendship', 'instinct']]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SnippetList_2_Word Stemming by PorterStemmer   - comma separated for display</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>friendli, littl, argument, fate, human, ashle, vanc, author, biographi, elon, musk, afraid, friend, larri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>may, alreadi, talk, custom, word, embed, previou, post, word, sentenc, ai, friend, friendli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feb, ai, friend, say, gate, speak, hamilton, compos, last, sever, hundr, year, great, societi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>may, tad, friend, write, think, artifici, intellig, help, clarifi, roboticist, han, moravec, observ, becom, therefor, program, friendli, ai, need, captur, mean, life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jan, exampl, claim, piec, ai, softwar, call, think, friendship, think, peopl, hang, togeth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jul, sentenc, ai, friend, friendli, sentenc, ai, human, alway, friendli, order, calcul, similar, use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nov, microsoft, built, new, type, ai, assist, want, friend, microsoft, friendli, xiaoic, ai, figur, want, ask, xiaoic, alreadi, weather, reader, dragon, tv, one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dec, mit, sherri, turkl, concern, jibo, sociabl, ai, studi, children, comput, sinc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jun, whether, environ, friend, foe, anyth, remain, poorli, keyword, ai, safeti, friendli, adversari, game, theori, bound, ration, although, arm, pull, approxim, time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jun, machin, intellig, research, institut, previous, known, creat, friendli, ai, singular, institut, begun, fill, one, intern, solut, duplic, human, friendship, instinct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                SnippetList_2_Word Stemming by PorterStemmer   - comma separated for display\n",
       "0  friendli, littl, argument, fate, human, ashle, vanc, author, biographi, elon, musk, afraid, friend, larri                                                                \n",
       "1  may, alreadi, talk, custom, word, embed, previou, post, word, sentenc, ai, friend, friendli                                                                              \n",
       "2  feb, ai, friend, say, gate, speak, hamilton, compos, last, sever, hundr, year, great, societi                                                                            \n",
       "3  may, tad, friend, write, think, artifici, intellig, help, clarifi, roboticist, han, moravec, observ, becom, therefor, program, friendli, ai, need, captur, mean, life    \n",
       "4  jan, exampl, claim, piec, ai, softwar, call, think, friendship, think, peopl, hang, togeth                                                                               \n",
       "5  jul, sentenc, ai, friend, friendli, sentenc, ai, human, alway, friendli, order, calcul, similar, use                                                                     \n",
       "6  nov, microsoft, built, new, type, ai, assist, want, friend, microsoft, friendli, xiaoic, ai, figur, want, ask, xiaoic, alreadi, weather, reader, dragon, tv, one         \n",
       "7  dec, mit, sherri, turkl, concern, jibo, sociabl, ai, studi, children, comput, sinc                                                                                       \n",
       "8  jun, whether, environ, friend, foe, anyth, remain, poorli, keyword, ai, safeti, friendli, adversari, game, theori, bound, ration, although, arm, pull, approxim, time    \n",
       "9  jun, machin, intellig, research, institut, previous, known, creat, friendli, ai, singular, institut, begun, fill, one, intern, solut, duplic, human, friendship, instinct"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Text processing-SnippetList_2\n",
    "import nltk\n",
    "import string\n",
    "import math \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "pd.set_option('display.max_columns', None)      # or 1000\n",
    "pd.set_option('display.max_rows', None)         # or 1000\n",
    "pd.set_option('display.max_colwidth', -1)       # or 199\n",
    "\n",
    "\n",
    "\n",
    "# snippets / sentences examples ##########\n",
    "\n",
    "data_all = pd.DataFrame(googleSearchSnippetlist_2)\n",
    "data_all.columns = ['Academic sentence - short example']\n",
    "display(data_all)  \n",
    "#print(data_all)\n",
    "\n",
    "low_documents = []\n",
    "for document in googleSearchSnippetlist_2:\n",
    "    low_documents.append(document.lower())\n",
    "    \n",
    "data_low = pd.DataFrame(low_documents)\n",
    "data_low.columns = ['Lower case sentence']\n",
    "display(data_low)  \n",
    "    \n",
    "# tokenization by split # Sentences Tokenized into Words - split by whitespace\n",
    "\n",
    "sentences_documents = []\n",
    "#document_counter = 0\n",
    "for document in low_documents:\n",
    "    sentences_documents.append(document.split())\n",
    "\n",
    "printableList1 = []\n",
    "for sentence1 in sentences_documents:\n",
    "    sentence1AsString = ''\n",
    "    for idx1, aWord1 in enumerate(sentence1):        \n",
    "        if idx1 == len(sentence1) - 1:\n",
    "            sentence1AsString = sentence1AsString + aWord1\n",
    "        else:\n",
    "            str1 = aWord1 + ', '\n",
    "            sentence1AsString = sentence1AsString + str1\n",
    "    printableList1.append(sentence1AsString)\n",
    "\n",
    "data_sentences1 = pd.DataFrame(printableList1)\n",
    "data_sentences1.columns = ['Sentence tokenized into words   - string form and comma separated for display']\n",
    "display(data_sentences1)\n",
    "\n",
    "# change compound words to separate words ie. 'conditional-statements' -> 'conditional', 'statements' \n",
    "print(\"\\n\" 'Single words' \"\\n\")\n",
    "single_word_documents = []\n",
    "for sentence_words in sentences_documents:\n",
    "    single_word_list = []\n",
    "    for word in sentence_words:\n",
    "        regex = re.compile(\"[-_]\")\n",
    "        trimmed = regex.sub(' ', word)\n",
    "        separate = trimmed.split( )\n",
    "        for item in separate:\n",
    "            single_word_list.append(item)        \n",
    "    single_word_documents.append(single_word_list)\n",
    "print(single_word_documents)\n",
    "\n",
    "printableList2 = []\n",
    "for sentence2 in single_word_documents:\n",
    "    sentence2AsString = ''\n",
    "    for idx2, aWord2 in enumerate(sentence2):        \n",
    "        if idx2 == len(sentence2) - 1:\n",
    "            sentence2AsString = sentence2AsString + aWord2\n",
    "        else:\n",
    "            str2 = aWord2 + ', '\n",
    "            sentence2AsString = sentence2AsString + str2\n",
    "    printableList2.append(sentence2AsString)\n",
    "\n",
    "data_sentences2 = pd.DataFrame(printableList2)\n",
    "data_sentences2.columns = ['Single words   - string form and comma separated for display']\n",
    "display(data_sentences2)     \n",
    "    \n",
    "    \n",
    "    \n",
    "# remove all tokens that are not alphabetic #############\n",
    "print(\"\\n\" 'Tokenized with alphabetic chars only' \"\\n\")\n",
    "alpha_documents = []\n",
    "for single_word_sentence in single_word_documents:\n",
    "    cleaned_list = []\n",
    "    for single_word in single_word_sentence:\n",
    "        regex = re.compile('[^a-zA-Z]')\n",
    "        #First parameter is the replacement, second parameter is your input string\n",
    "        nonAlphaRemoved = regex.sub('', single_word)\n",
    "        # add string to list only if it has content\n",
    "        if nonAlphaRemoved:\n",
    "            cleaned_list.append(nonAlphaRemoved)\n",
    "    alpha_documents.append(cleaned_list)\n",
    "print(alpha_documents)\n",
    "\n",
    "printableList3 = []\n",
    "for sentence3 in alpha_documents:\n",
    "    sentence3AsString = ''\n",
    "    for idx3, aWord3 in enumerate(sentence3):        \n",
    "        if idx3 == len(sentence3) - 1:\n",
    "            sentence3AsString = sentence3AsString + aWord3\n",
    "        else:\n",
    "            str3 = aWord3 + ', '\n",
    "            sentence3AsString = sentence3AsString + str3\n",
    "    printableList3.append(sentence3AsString)\n",
    "\n",
    "data_sentences3 = pd.DataFrame(printableList3)\n",
    "data_sentences3.columns = ['Tokenized with alphabetic chars only   - string form and comma separated for display']\n",
    "display(data_sentences3)     \n",
    "\n",
    "\n",
    "# filter out stopwords ########\n",
    "print(\"\\n\" 'English stopwords filtered tokens' \"\\n\")\n",
    "stop_filtered_tokens_2 = []\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for fword in alpha_documents:\n",
    "    fword_list = []\n",
    "    for sword in fword:\n",
    "        #fword_list = [sword for sword in alpha_documents if not sword in english_stop_words]\n",
    "        if not sword in english_stop_words:\n",
    "            fword_list.append(sword)\n",
    "    stop_filtered_tokens_2.append(fword_list)\n",
    "print(stop_filtered_tokens_2)  \n",
    "\n",
    "\n",
    "printableList4 = []\n",
    "for sentence4 in stop_filtered_tokens_2:\n",
    "    sentence4AsString = ''\n",
    "    for idx4, aWord4 in enumerate(sentence4):        \n",
    "        if idx4 == len(sentence4) - 1:\n",
    "            sentence4AsString = sentence4AsString + aWord4\n",
    "        else:\n",
    "            str4 = aWord4 + ', '\n",
    "            sentence4AsString = sentence4AsString + str4\n",
    "    printableList4.append(sentence4AsString)\n",
    "\n",
    "data_sentences4 = pd.DataFrame(printableList4)\n",
    "data_sentences4.columns = ['English stopwords filtered tokens   - comma separated for display']\n",
    "display(data_sentences4)    \n",
    "\n",
    "\n",
    "# tokenization by PorterStemmer ############\n",
    "print(\"\\n\" 'Word Stemming by PorterStemmer' \"\\n\")\n",
    "porter_documents_2 = []\n",
    "for ps_word_list in stop_filtered_tokens_2:\n",
    "    PS = PorterStemmer()\n",
    "    porter_list = []\n",
    "    for ps_word in ps_word_list:\n",
    "        porter_list.append(PS.stem(ps_word))\n",
    "    porter_documents_2.append(porter_list)\n",
    "print(porter_documents_2)\n",
    "\n",
    "printableList5 = []\n",
    "for sentence5 in porter_documents_2:\n",
    "    sentence5AsString = ''\n",
    "    for idx5, aWord5 in enumerate(sentence5):        \n",
    "        if idx5 == len(sentence5) - 1:\n",
    "            sentence5AsString = sentence5AsString + aWord5\n",
    "        else:\n",
    "            str5 = aWord5 + ', '\n",
    "            sentence5AsString = sentence5AsString + str5\n",
    "    printableList5.append(sentence5AsString)\n",
    "\n",
    "data_sentences5 = pd.DataFrame(printableList5)\n",
    "data_sentences5.columns = ['SnippetList_2_Word Stemming by PorterStemmer   - comma separated for display']\n",
    "display(data_sentences5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSim= 0.9285714285714286\n"
     ]
    }
   ],
   "source": [
    "# define jaccard similarity for python #\n",
    "def jaccard_similarity(query, jdoc):\n",
    "    intersection = set(query).intersection(set(jdoc))\n",
    "    union = set(query).union(set(jdoc))\n",
    "    return len(intersection)/len(union)\n",
    "\n",
    "def listToString (sourceList):\n",
    "    subListAsString = ''    \n",
    "    for listIndex, listWord in enumerate(sourceList):        \n",
    "        if listIndex == len(sourceList) - 1:\n",
    "            subListAsString = subListAsString + listWord\n",
    "        else:\n",
    "            strWithComma = listWord + ', '\n",
    "            subListAsString = subListAsString + strWithComma        \n",
    "    return subListAsString\n",
    "\n",
    "# convert snippetlist_1 to string\n",
    "porterDocStr_1 =''\n",
    "for i in range(9):\n",
    "    porterDocStr_1 += listToString(porter_documents_1[i])\n",
    "#print(porterDocStr_1)\n",
    "\n",
    "# convert snippetlist_2 to string\n",
    "porterDocStr_2 =' '\n",
    "for j in range(9):\n",
    "    porterDocStr_2 += listToString(porter_documents_2[j])\n",
    "#print(porterDocStr_2)\n",
    "                              \n",
    "result = jaccard_similarity(porterDocStr_1, porterDocStr_2)\n",
    "print('JSim=',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tStart with a simple academic sentences of your choice for S1 and S2 to see how the process works. It is important that you start with sentences with very close meaning and move up to sentences with very disparate meaning (Use a fixed number of pair of sentences of your choice to elaborate your reasoning). Compare the result with sentence semantic similarity that you have seen in Lab2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "wnSim_sentence= 0.6666666666666666\n",
      "JSim_sentence= 0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "#!!!!Note: this one is to calculate the similarity between S1 and S2 after simple word_tokenization\n",
    "# WordNet semantic similarity-only word_tokenization-sentence\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "\n",
    "#example\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    " \n",
    "def Wn_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    best_score = [0.0]\n",
    "    for ss1 in synsets1:\n",
    "        for ss2 in synsets2:\n",
    "            best1_score=ss1.path_similarity(ss2)\n",
    "        if best1_score is not None:\n",
    "            best_score.append(best1_score)\n",
    "        max1=max(best_score)\n",
    "        if best_score is not None:\n",
    "            score += max1\n",
    "        if max1 is not 0.0:\n",
    "            count += 1\n",
    "        best_score=[0.0]\n",
    "    print(score/count)      \n",
    "   \n",
    "    # Average the values\n",
    "    score /= count\n",
    "    return score\n",
    "    \n",
    "wnSim=Wn_similarity(sentence_1,sentence_2)\n",
    "print('wnSim_sentence=',wnSim)\n",
    "JSim=jaccard_similarity(word_tokenize(sentence_1),word_tokenize(sentence_2))\n",
    "print('JSim_sentence=',JSim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20159195159195123\n",
      "wnSim_porterDocStr= 0.20159195159195123\n",
      "0.18893974081474132\n",
      "wnSim_tokenStr= 0.18893974081474132\n"
     ]
    }
   ],
   "source": [
    "#!!!!Note: this one is to calculate the similarity of snippetlist_1 and snippetlist_1\n",
    "# WordNet semantic similarity-snippetslist\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "\n",
    "#example\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    " \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    " \n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    " \n",
    "def Wn_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(sentence1)\n",
    "    sentence2 = pos_tag(sentence2)\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    best_score = [0.0]\n",
    "    for ss1 in synsets1:\n",
    "        for ss2 in synsets2:\n",
    "            best1_score=ss1.path_similarity(ss2)\n",
    "        if best1_score is not None:\n",
    "            best_score.append(best1_score)\n",
    "        max1=max(best_score)\n",
    "        if best_score is not None:\n",
    "            score += max1\n",
    "        if max1 is not 0.0:\n",
    "            count += 1\n",
    "        best_score=[0.0]\n",
    "    print(score/count)      \n",
    "   \n",
    "    # Average the values\n",
    "    score /= count\n",
    "    return score\n",
    "    \n",
    "# Snippetlist similarity\n",
    "\n",
    "# convert stop_filtered_tokens_1 to string\n",
    "tokenStr_1 =''\n",
    "for i in range(9):\n",
    "    tokenStr_1 += listToString(stop_filtered_tokens_1[i])\n",
    "#print(porterDocStr_1)\n",
    "\n",
    "# convert stop_filtered_tokens_2 to string\n",
    "tokenStr_2 =''\n",
    "for j in range(9):\n",
    "    tokenStr_2 += listToString(stop_filtered_tokens_2[j])\n",
    "    \n",
    "wnSim = Wn_similarity(porterDocStr_1, porterDocStr_2)\n",
    "print('wnSim_porterDocStr=',wnSim)\n",
    "wnSim = Wn_similarity(tokenStr_1, tokenStr_2)\n",
    "print('wnSim_tokenStr=',wnSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Academic sentence - short example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AI and humans have always been friendly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AI is our friend and it has been friendly.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Academic sentence - short example\n",
       "0  AI and humans have always been friendly.  \n",
       "1  AI is our friend and it has been friendly."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized with alphabetic chars only\n",
      "\n",
      "  Sentences_Word Stemming by PorterStemmer   - comma separated for display\n",
      "0  ai, human, alway, friendli                                             \n",
      "1  ai, friend, friendli                                                   \n",
      "0.10096153846153846\n",
      "wnSim_sentence_moretextprocessing= 0.10096153846153846\n",
      "JSim_sentence_moretextprocessing= 0.4\n"
     ]
    }
   ],
   "source": [
    "#!!!!Note:only for reference to check how big effect of text processing on S1&S2 similarity.\n",
    "# Text processing-sentence.\n",
    "import nltk\n",
    "import string\n",
    "import math \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "pd.set_option('display.max_columns', None)      # or 1000\n",
    "pd.set_option('display.max_rows', None)         # or 1000\n",
    "pd.set_option('display.max_colwidth', -1)       # or 199\n",
    "\n",
    "sentence=[\"AI and humans have always been friendly.\",\"AI is our friend and it has been friendly.\"]\n",
    "\n",
    "# sentences examples ##########\n",
    "\n",
    "data_all = pd.DataFrame(sentence)\n",
    "data_all.columns = ['Academic sentence - short example']\n",
    "display(data_all)  \n",
    "#print(data_all)\n",
    "\n",
    "low_documents = []\n",
    "for document in sentence:\n",
    "    low_documents.append(document.lower())\n",
    "    \n",
    "data_low = pd.DataFrame(low_documents)\n",
    "data_low.columns = ['Lower case sentence']\n",
    "   \n",
    "# tokenization by split # Sentences Tokenized into Words - split by whitespace\n",
    "\n",
    "sentences_documents = []\n",
    "#document_counter = 0\n",
    "for document in low_documents:\n",
    "    sentences_documents.append(document.split())\n",
    "\n",
    "printableList1 = []\n",
    "for sentence1 in sentences_documents:\n",
    "    sentence1AsString = ''\n",
    "    for idx1, aWord1 in enumerate(sentence1):        \n",
    "        if idx1 == len(sentence1) - 1:\n",
    "            sentence1AsString = sentence1AsString + aWord1\n",
    "        else:\n",
    "            str1 = aWord1 + ', '\n",
    "            sentence1AsString = sentence1AsString + str1\n",
    "    printableList1.append(sentence1AsString)\n",
    "\n",
    "data_sentences1 = pd.DataFrame(printableList1)\n",
    "data_sentences1.columns = ['Sentence tokenized into words   - string form and comma separated for display']\n",
    "\n",
    "# change compound words to separate words ie. 'conditional-statements' -> 'conditional', 'statements' \n",
    "#print(\"\\n\" 'Single words' \"\\n\")\n",
    "single_word_documents = []\n",
    "for sentence_words in sentences_documents:\n",
    "    single_word_list = []\n",
    "    for word in sentence_words:\n",
    "        regex = re.compile(\"[-_]\")\n",
    "        trimmed = regex.sub(' ', word)\n",
    "        separate = trimmed.split( )\n",
    "        for item in separate:\n",
    "            single_word_list.append(item)        \n",
    "    single_word_documents.append(single_word_list)\n",
    "\n",
    "printableList2 = []\n",
    "for sentence2 in single_word_documents:\n",
    "    sentence2AsString = ''\n",
    "    for idx2, aWord2 in enumerate(sentence2):        \n",
    "        if idx2 == len(sentence2) - 1:\n",
    "            sentence2AsString = sentence2AsString + aWord2\n",
    "        else:\n",
    "            str2 = aWord2 + ', '\n",
    "            sentence2AsString = sentence2AsString + str2\n",
    "    printableList2.append(sentence2AsString)\n",
    "\n",
    "data_sentences2 = pd.DataFrame(printableList2)\n",
    "data_sentences2.columns = ['Single words   - string form and comma separated for display']\n",
    "        \n",
    "# remove all tokens that are not alphabetic #############\n",
    "print(\"\\n\" 'Tokenized with alphabetic chars only' \"\\n\")\n",
    "alpha_documents = []\n",
    "for single_word_sentence in single_word_documents:\n",
    "    cleaned_list = []\n",
    "    for single_word in single_word_sentence:\n",
    "        regex = re.compile('[^a-zA-Z]')\n",
    "        #First parameter is the replacement, second parameter is your input string\n",
    "        nonAlphaRemoved = regex.sub('', single_word)\n",
    "        # add string to list only if it has content\n",
    "        if nonAlphaRemoved:\n",
    "            cleaned_list.append(nonAlphaRemoved)\n",
    "    alpha_documents.append(cleaned_list)\n",
    "\n",
    "printableList3 = []\n",
    "for sentence3 in alpha_documents:\n",
    "    sentence3AsString = ''\n",
    "    for idx3, aWord3 in enumerate(sentence3):        \n",
    "        if idx3 == len(sentence3) - 1:\n",
    "            sentence3AsString = sentence3AsString + aWord3\n",
    "        else:\n",
    "            str3 = aWord3 + ', '\n",
    "            sentence3AsString = sentence3AsString + str3\n",
    "    printableList3.append(sentence3AsString)\n",
    "\n",
    "data_sentences3 = pd.DataFrame(printableList3)\n",
    "data_sentences3.columns = ['Tokenized with alphabetic chars only   - string form and comma separated for display']    \n",
    "\n",
    "# filter out stopwords ########\n",
    "#print(\"\\n\" 'English stopwords filtered tokens' \"\\n\")\n",
    "stop_filtered_tokens_3 = []\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for fword in alpha_documents:\n",
    "    fword_list = []\n",
    "    for sword in fword:\n",
    "        #fword_list = [sword for sword in alpha_documents if not sword in english_stop_words]\n",
    "        if not sword in english_stop_words:\n",
    "            fword_list.append(sword)\n",
    "    stop_filtered_tokens_3.append(fword_list) \n",
    "\n",
    "printableList4 = []\n",
    "for sentence4 in stop_filtered_tokens_3:\n",
    "    sentence4AsString = ''\n",
    "    for idx4, aWord4 in enumerate(sentence4):        \n",
    "        if idx4 == len(sentence4) - 1:\n",
    "            sentence4AsString = sentence4AsString + aWord4\n",
    "        else:\n",
    "            str4 = aWord4 + ', '\n",
    "            sentence4AsString = sentence4AsString + str4\n",
    "    printableList4.append(sentence4AsString)\n",
    "\n",
    "data_sentences4 = pd.DataFrame(printableList4)\n",
    "data_sentences4.columns = ['English stopwords filtered tokens   - comma separated for display']\n",
    "   \n",
    "# tokenization by PorterStemmer ############\n",
    "#print(\"\\n\" 'Word Stemming by PorterStemmer' \"\\n\")\n",
    "porter_documents_3 = []\n",
    "for ps_word_list in stop_filtered_tokens_3:\n",
    "    PS = PorterStemmer()\n",
    "    porter_list = []\n",
    "    for ps_word in ps_word_list:\n",
    "        porter_list.append(PS.stem(ps_word))\n",
    "    porter_documents_3.append(porter_list)\n",
    "\n",
    "printableList5 = []\n",
    "for sentence5 in porter_documents_3:\n",
    "    sentence5AsString = ''\n",
    "    for idx5, aWord5 in enumerate(sentence5):        \n",
    "        if idx5 == len(sentence5) - 1:\n",
    "            sentence5AsString = sentence5AsString + aWord5\n",
    "        else:\n",
    "            str5 = aWord5 + ', '\n",
    "            sentence5AsString = sentence5AsString + str5\n",
    "    printableList5.append(sentence5AsString)\n",
    "\n",
    "data_sentences5 = pd.DataFrame(printableList5)\n",
    "data_sentences5.columns = ['Sentences_Word Stemming by PorterStemmer   - comma separated for display']\n",
    "print(data_sentences5)\n",
    "\n",
    "wnSim=Wn_similarity(stop_filtered_tokens_3[0],stop_filtered_tokens_3[1])\n",
    "print('wnSim_sentence_moretextprocessing=',wnSim)\n",
    "JSim=jaccard_similarity(stop_filtered_tokens_3[0],stop_filtered_tokens_3[1])\n",
    "print('JSim_sentence_moretextprocessing=',JSim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tRefine your code in order to expand the terms of each snippets to include all the hyponyms and hypernyms of the associated words by quering the WordNet database, and repeat the overlapping process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: two methods of searching hyponyms and hypernyms were listed as below. You can review which one is better? Actually they are bit different although the result is bit close. And both results are much lower than the similarity without terms expanding...I am not sure is it Jaccard similarity enough to meet\"overlapping process\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[Synset('blackthorn.n.02'), Synset('cockspur_thorn.n.01'), Synset('english_hawthorn.n.01'), Synset('evergreen_thorn.n.01'), Synset('mayhaw.n.01'), Synset('parsley_haw.n.01'), Synset('red_haw.n.01'), Synset('red_haw.n.02'), Synset('scarlet_haw.n.01'), Synset('whitethorn.n.01')]]]\n",
      "(('blackthorn', 'pear_haw', 'pear_hawthorn', 'Crataegus_calpodendron', 'Crataegus_tomentosa'), ('cockspur_thorn', 'cockspur_hawthorn', 'Crataegus_crus-galli'), ('English_hawthorn', 'Crataegus_monogyna'), ('evergreen_thorn', 'Crataegus_oxyacantha'), ('mayhaw', 'summer_haw', 'Crataegus_aestivalis'), ('parsley_haw', 'parsley-leaved_thorn', 'Crataegus_apiifolia', 'Crataegus_marshallii'), ('red_haw', 'Crataegus_pedicellata', 'Crataegus_coccinea'), ('red_haw', 'downy_haw', 'Crataegus_mollis', 'Crataegus_coccinea_mollis'), ('scarlet_haw', 'Crataegus_biltmoreana'), ('whitethorn', 'English_hawthorn', 'may', 'Crataegus_laevigata', 'Crataegus_oxycantha'))\n",
      "0.18045112781954886\n"
     ]
    }
   ],
   "source": [
    "#Snippetlist_1_ExpandJaccardSimilairy_Merja\n",
    "from nltk.corpus import wordnet as wn\n",
    "import itertools\n",
    "from itertools import chain\n",
    "# we use stop_filtered_tokes -list of word lists that is defined above\n",
    "# as seed for wordnet\n",
    "#print('---Source word lists---')\n",
    "#print(stop_filtered_tokens)\n",
    "\n",
    "listOfSentences_1 = []\n",
    "Snippetlist_1 = tokenStr_1.split(',')\n",
    "#print(porterDocStr_1)\n",
    "#print(len(Snippetlist_1))\n",
    "#print(wn.synsets(Snippetlist_1[0]))\n",
    "\n",
    "#listOfTokenHyponymDictionaries = []\n",
    "\n",
    "for ss in Snippetlist_1:\n",
    "    listOfTokenHyponymDictionaries = []\n",
    "    mySynSets = wn.synsets(ss.strip())\n",
    "    #print(SynList)\n",
    "    if mySynSets:\n",
    "        hyponymsForTokensDict = {}\n",
    "        for mySynset in mySynSets:\n",
    "                #print (mySynset)\n",
    "            if(mySynset.hypernyms()):\n",
    "                    #for aHyponym in mySynset.hypernyms()[0].hyponyms():\n",
    "                    #    print(aHyponym.name())\n",
    "                hyponymsForTokensDict = [mySynset.hypernyms()[0].hyponyms()]                    \n",
    "        if any(hyponymsForTokensDict):        \n",
    "            listOfTokenHyponymDictionaries.append(hyponymsForTokensDict)        \n",
    "    listOfSentences_1.append(listOfTokenHyponymDictionaries)\n",
    "print(listOfSentences_1[0])\n",
    "#print(len(listOfSentences_1))\n",
    "\n",
    "# get the words list\n",
    "NewwordsList_1=[]\n",
    "\n",
    "for sentence in listOfSentences_1: \n",
    "    ExpandwordsList = []\n",
    "    for item in sentence:\n",
    "        for syns in item:\n",
    "            for syn in syns:\n",
    "                ExpandwordsList.append(list(chain(*[syn.lemma_names()])))\n",
    "    NewwordsList_1.append(ExpandwordsList)\n",
    "#print(NewwordsList_1)\n",
    "#print(len(NewwordsList_1))\n",
    "\n",
    "ExpandSentence_1=[]\n",
    "for sentence in NewwordsList_1:\n",
    "    newitem_1=[]\n",
    "    for eachitem in sentence:\n",
    "        newitem_1.append(tuple(eachitem))\n",
    "    ExpandSentence_1.append(tuple(newitem_1))\n",
    "#print(len(ExpandSentence_1))\n",
    "ExpandSentence_1=[item for item in ExpandSentence_1 if item]\n",
    "print(ExpandSentence_1[0])\n",
    "#print(','.join(ExpandSentence_1))\n",
    "\n",
    "listOfSentences_2 = []\n",
    "Snippetlist_2 = tokenStr_2.split(',')\n",
    "#print(porterDocStr_2)\n",
    "#print(Snippetlist_2[1])\n",
    "#print(wn.synsets(Snippetlist_2[0]))\n",
    "\n",
    "#listOfTokenHyponymDictionaries = []\n",
    "for ss in Snippetlist_2:\n",
    "        #print('--- word to Process --- ')\n",
    "        #print(t)\n",
    "    listOfTokenHyponymDictionaries = []\n",
    "    mySynSets = wn.synsets(ss.strip())\n",
    "    if mySynSets:\n",
    "        hyponymsForTokensDict = {}\n",
    "        for mySynset in mySynSets:\n",
    "                #print (mySynset)\n",
    "            if(mySynset.hypernyms()):\n",
    "                    #for aHyponym in mySynset.hypernyms()[0].hyponyms():\n",
    "                    #    print(aHyponym.name())\n",
    "                hyponymsForTokensDict = [mySynset.hypernyms()[0].hyponyms()]                    \n",
    "        if any(hyponymsForTokensDict):        \n",
    "            listOfTokenHyponymDictionaries.append(hyponymsForTokensDict)        \n",
    "    listOfSentences_2.append(listOfTokenHyponymDictionaries)\n",
    "#print(listOfSentences_2)\n",
    "#print(len(listOfSentences_2))\n",
    "\n",
    "# get the words list\n",
    "NewwordsList_2=[]\n",
    "\n",
    "for sentence in listOfSentences_2: \n",
    "    ExpandwordsList = []\n",
    "    for item in sentence:\n",
    "        for syns in item:\n",
    "            for syn in syns:\n",
    "                ExpandwordsList.append(list(chain(*[syn.lemma_names()])))\n",
    "    NewwordsList_2.append(ExpandwordsList)\n",
    "#print(NewwordsList_2)\n",
    "#print(len(NewwordsList_2))\n",
    "\n",
    "ExpandSentence_2=[]\n",
    "for sentence in NewwordsList_2:\n",
    "    newitem_2=[]\n",
    "    for eachitem in sentence:\n",
    "        newitem_2.append(tuple(eachitem))\n",
    "    ExpandSentence_2.append(tuple(newitem_2))\n",
    "ExpandSentence_2=[item for item in ExpandSentence_2 if item]\n",
    "#print(len(ExpandSentence_2))\n",
    "#print(ExpandSentence_2)\n",
    "\n",
    "result = jaccard_similarity(ExpandSentence_1, ExpandSentence_2)\n",
    "#result = jaccard_similarity(listOfSentences_1, listOfSentences_2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Gregorian_calendar_month',), ('hawthorn', 'haw'))\n",
      "ExpandJaccardSim= 0.11612903225806452\n"
     ]
    }
   ],
   "source": [
    "#Snippetlist_ExpandJaccardSimilarity_Sicily\n",
    "from nltk.corpus import wordnet as wn\n",
    "import itertools\n",
    "from itertools import chain\n",
    "# we use stop_filtered_tokes -list of word lists that is defined above\n",
    "# as seed for wordnet\n",
    "#print('---Source word lists---')\n",
    "#print(stop_filtered_tokens)\n",
    "\n",
    "Snippetlist_1 = tokenStr_1.split(',')\n",
    "#print(tokenStr_1)\n",
    "#print(Snippetlist_1)\n",
    "#print(Snippetlist_1[1])\n",
    "#print(wn.synsets(Snippetlist_1[0]))\n",
    "\n",
    "SynsetList_1=[]     \n",
    "for ss in Snippetlist_1:\n",
    "    mySynSets = wn.synsets(ss.strip())\n",
    "    hyper_list = []\n",
    "    hypo_list = []\n",
    "    new_list = []\n",
    "    for i,j in enumerate(mySynSets):\n",
    "        #print(i,j.name())\n",
    "        #hyper_list = ','.join(list(chain(*[i.lemma_names() for i in j.hypernyms()])))\n",
    "        hyper_list.append(list(chain(*[i.lemma_names() for i in j.hypernyms()])))\n",
    "        #print(hyper_list)\n",
    "        hypo_list.append(list(chain(*[i.lemma_names() for i in j.hyponyms()])))\n",
    "        #print(hypo_list)\n",
    "        new_list = hypo_list+hyper_list\n",
    "        new_list=[item for item in new_list if item]\n",
    "        #print(new_list)\n",
    "    SynsetList_1.append(new_list)\n",
    "#print(SynsetList_1)\n",
    "\n",
    "ExpandSentence_1=[]\n",
    "for sentence in SynsetList_1:\n",
    "    newitem_1=[]\n",
    "    for eachitem in sentence:\n",
    "        newitem_1.append(tuple(eachitem))\n",
    "    ExpandSentence_1.append(tuple(newitem_1))\n",
    "print(ExpandSentence_1[0])\n",
    "    \n",
    "Snippetlist_2 = tokenStr_2.split(',')\n",
    "#print(tokenStr_1)\n",
    "#print(Snippetlist_1)\n",
    "#print(Snippetlist_1[1])\n",
    "#print(wn.synsets(Snippetlist_1[0]))\n",
    "\n",
    "SynsetList_2=[]     \n",
    "for ss in Snippetlist_2:\n",
    "    mySynSets = wn.synsets(ss.strip())\n",
    "    hyper_list = []\n",
    "    hypo_list = []\n",
    "    new_list = []\n",
    "    for i,j in enumerate(mySynSets):\n",
    "        #print(i,j.name())\n",
    "        #hyper_list = ','.join(list(chain(*[i.lemma_names() for i in j.hypernyms()])))\n",
    "        hyper_list.append(list(chain(*[i.lemma_names() for i in j.hypernyms()])))\n",
    "        #print(hyper_list)\n",
    "        hypo_list.append(list(chain(*[i.lemma_names() for i in j.hyponyms()])))\n",
    "        #print(hypo_list)\n",
    "        new_list = hypo_list+hyper_list\n",
    "        new_list=[item for item in new_list if item]\n",
    "        #print(new_list)\n",
    "    SynsetList_2.append(new_list)\n",
    "#print(SynsetList_2)\n",
    "\n",
    "ExpandSentence_2=[]\n",
    "for sentence in SynsetList_2:\n",
    "    newitem_2=[]\n",
    "    for eachitem in sentence:\n",
    "        newitem_2.append(tuple(eachitem))\n",
    "    ExpandSentence_2.append(tuple(newitem_2))\n",
    "#print(ExpandSentence_2)\n",
    "\n",
    "ExpandJaccardSim = jaccard_similarity(ExpandSentence_1, ExpandSentence_2)\n",
    "print('ExpandJaccardSim=',ExpandJaccardSim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[Synset('blackthorn.n.02'), Synset('cockspur_thorn.n.01'), Synset('english_hawthorn.n.01'), Synset('evergreen_thorn.n.01'), Synset('mayhaw.n.01'), Synset('parsley_haw.n.01'), Synset('red_haw.n.01'), Synset('red_haw.n.02'), Synset('scarlet_haw.n.01'), Synset('whitethorn.n.01')]]], [[[Synset('account.v.04'), Synset('affirm.v.02'), Synset('condemn.v.02'), Synset('consecrate.v.04'), Synset('count_out.v.01'), Synset('decree.v.01'), Synset('opine.v.01'), Synset('plead.v.03'), Synset('proclaim.v.02'), Synset('profess.v.02'), Synset('profess.v.04'), Synset('promise.v.01'), Synset('promise.v.02'), Synset('propose.v.01'), Synset('sentence.v.01'), Synset('swear.v.04'), Synset('take_the_fifth.v.01'), Synset('testify.v.01')]]], [[[Synset('artificial_insemination.n.01')]]], [[[Synset('adventist.n.01'), Synset('apostle.n.02'), Synset('apostle.n.03'), Synset('arianist.n.01'), Synset('born-again_christian.n.01'), Synset('catholic.n.01'), Synset('communicant.n.01'), Synset('copt.n.02'), Synset('friend.n.05'), Synset('gentile.n.03'), Synset('gentile.n.04'), Synset('melkite.n.01'), Synset('melkite.n.02'), Synset('nazarene.n.02'), Synset('old_catholic.n.01'), Synset('protestant.n.01'), Synset('shaker.n.02'), Synset('tractarian.n.01')]]], [[[Synset('cavalry.n.01'), Synset('friendly.n.01'), Synset('garrison.n.03'), Synset('hostile.n.01')]]], [[[Synset('account.v.04'), Synset('affirm.v.02'), Synset('condemn.v.02'), Synset('consecrate.v.04'), Synset('count_out.v.01'), Synset('decree.v.01'), Synset('opine.v.01'), Synset('plead.v.03'), Synset('proclaim.v.02'), Synset('profess.v.02'), Synset('profess.v.04'), Synset('promise.v.01'), Synset('promise.v.02'), Synset('propose.v.01'), Synset('sentence.v.01'), Synset('swear.v.04'), Synset('take_the_fifth.v.01'), Synset('testify.v.01')]]], [[[Synset('artificial_insemination.n.01')]]], [[[Synset('australopithecine.n.01'), Synset('dryopithecine.n.01'), Synset('homo.n.02'), Synset('javanthropus.n.01'), Synset('pithecanthropus.n.01'), Synset('sinanthropus.n.01'), Synset('sivapithecus.n.01')]]], [], [[[Synset('cavalry.n.01'), Synset('friendly.n.01'), Synset('garrison.n.03'), Synset('hostile.n.01')]]], [[[Synset('accept.v.01'), Synset('approve.v.02'), Synset('assign.v.08'), Synset('calculate.v.02'), Synset('choose.v.03'), Synset('declare.v.04'), Synset('disapprove.v.01'), Synset('disapprove.v.02'), Synset('expect.v.01'), Synset('fail.v.06'), Synset('impute.v.01'), Synset('measure.v.04'), Synset('pass.v.16'), Synset('prejudge.v.01'), Synset('rate.v.01'), Synset('reappraise.v.01'), Synset('reject.v.01'), Synset('review.v.02'), Synset('stand.v.06'), Synset('test.v.01'), Synset('think.v.01')]]], [[[Synset('count.v.08'), Synset('credit.v.04'), Synset('lean.v.04')]]], [[[Synset('closure.n.03'), Synset('common_fate.n.01'), Synset('good_continuation.n.01'), Synset('proximity.n.03'), Synset('similarity.n.02')]]], [], [[[Synset('acting.n.01'), Synset('aid.n.02'), Synset('attempt.n.01'), Synset('behavior.n.01'), Synset('behavior.n.04'), Synset('burst.n.03'), Synset('buzz.n.02'), Synset('calibration.n.01'), Synset('ceremony.n.02'), Synset('ceremony.n.03'), Synset('concealment.n.03'), Synset('continuance.n.01'), Synset('control.n.05'), Synset('creation.n.01'), Synset('cup_of_tea.n.01'), Synset('demand.n.04'), Synset('dismantling.n.01'), Synset('diversion.n.01'), Synset('domesticity.n.02'), Synset('education.n.01'), Synset('energizing.n.01'), Synset('enjoyment.n.02'), Synset('follow-up.n.02'), Synset('fun.n.03'), Synset('game.n.01'), Synset('grouping.n.02'), Synset('lamentation.n.02'), Synset('last.n.03'), Synset('laughter.n.02'), Synset('leadership.n.01'), Synset('liveliness.n.01'), Synset('market.n.01'), Synset('measurement.n.01'), Synset('music.n.03'), Synset('mystification.n.03'), Synset('negotiation.n.02'), Synset('occupation.n.01'), Synset('occupation.n.03'), Synset('operation.n.03'), Synset('operation.n.05'), Synset('operation.n.11'), Synset('organization.n.06'), Synset('perturbation.n.03'), Synset('placement.n.03'), Synset('pleasure.n.04'), Synset('politics.n.05'), Synset('practice.n.01'), Synset('precession.n.02'), Synset('preparation.n.01'), Synset('procedure.n.01'), Synset('protection.n.01'), Synset('provision.n.02'), Synset('puncture.n.03'), Synset('release.n.09'), Synset('representation.n.10'), Synset('role.n.04'), Synset('search.n.01'), Synset('sensory_activity.n.01'), Synset('service.n.11'), Synset('sin.n.06'), Synset('solo.n.01'), Synset('space_walk.n.01'), Synset('support.n.01'), Synset('support.n.08'), Synset('timekeeping.n.01'), Synset('training.n.01'), Synset('turn.n.03'), Synset('use.n.01'), Synset('variation.n.02'), Synset('verbalization.n.02'), Synset('waste.n.02'), Synset('work.n.01'), Synset('works.n.03'), Synset('worship.n.01'), Synset('writing.n.05'), Synset('wrongdoing.n.02')]]], [], [[[Synset('accept.v.01'), Synset('approve.v.02'), Synset('assign.v.08'), Synset('calculate.v.02'), Synset('choose.v.03'), Synset('declare.v.04'), Synset('disapprove.v.01'), Synset('disapprove.v.02'), Synset('expect.v.01'), Synset('fail.v.06'), Synset('impute.v.01'), Synset('measure.v.04'), Synset('pass.v.16'), Synset('prejudge.v.01'), Synset('rate.v.01'), Synset('reappraise.v.01'), Synset('reject.v.01'), Synset('review.v.02'), Synset('stand.v.06'), Synset('test.v.01'), Synset('think.v.01')]]], [[[Synset('affected_role.n.01'), Synset('agentive_role.n.01'), Synset('benefactive_role.n.01'), Synset('instrumental_role.n.01'), Synset('locative_role.n.01'), Synset('recipient_role.n.01'), Synset('resultant_role.n.01'), Synset('temporal_role.n.01')]]], [], [[[Synset('course.v.03'), Synset('drive.v.21'), Synset('drive.v.22'), Synset('falcon.v.01'), Synset('ferret.v.02'), Synset('fowl.v.01'), Synset('fowl.v.02'), Synset('foxhunt.v.01'), Synset('hawk.v.02'), Synset('jacklight.v.01'), Synset('poach.v.01'), Synset('rabbit.v.01'), Synset('scrounge.v.01'), Synset('seal.v.06'), Synset('snipe.v.01'), Synset('still-hunt.v.01'), Synset('turtle.v.02'), Synset('whale.v.01')]]], [[[Synset('advance.v.05'), Synset('agitate.v.06'), Synset('beat.v.08'), Synset('blow.v.14'), Synset('brandish.v.01'), Synset('center.v.03'), Synset('change_hands.v.01'), Synset('chase_away.v.01'), Synset('circulate.v.03'), Synset('circulate.v.06'), Synset('dandle.v.01'), Synset('disarrange.v.01'), Synset('dislocate.v.01'), Synset('displace.v.01'), Synset('drag.v.07'), Synset('drive.v.03'), Synset('drop.v.01'), Synset('engage.v.10'), Synset('expel.v.01'), Synset('exteriorize.v.01'), Synset('flick.v.06'), Synset('fluctuate.v.01'), Synset('funnel.v.01'), Synset('glide.v.03'), Synset('herd.v.01'), Synset('hit.v.12'), Synset('hustle.v.01'), Synset('jar.v.03'), Synset('lateralize.v.01'), Synset('launch.v.05'), Synset('lift.v.02'), Synset('lift.v.03'), Synset('lower.v.01'), Synset('mobilize.v.04'), Synset('overturn.v.02'), Synset('play.v.24'), Synset('pour.v.01'), Synset('press_down.v.01'), Synset('propel.v.01'), Synset('pull.v.01'), Synset('pull.v.09'), Synset('pulse.v.03'), Synset('pump.v.06'), Synset('push.v.01'), Synset('put.v.01'), Synset('race.v.04'), Synset('raise.v.02'), Synset('rake.v.01'), Synset('relocate.v.02'), Synset('rock.v.02'), Synset('roll.v.14'), Synset('rout_out.v.02'), Synset('run.v.26'), Synset('saltate.v.01'), Synset('scan.v.05'), Synset('send.v.01'), Synset('separate.v.02'), Synset('shift.v.02'), Synset('singsong.v.02'), Synset('sink.v.02'), Synset('slide.v.03'), Synset('sling.v.03'), Synset('slip.v.09'), Synset('spill.v.01'), Synset('spill.v.03'), Synset('station.v.01'), Synset('stir.v.01'), Synset('swing.v.01'), Synset('take_back.v.04'), Synset('transfer.v.02'), Synset('transfer.v.04'), Synset('translate.v.05'), Synset('translate.v.08'), Synset('transmit.v.04'), Synset('transport.v.01'), Synset('transport.v.02'), Synset('transport.v.04'), Synset('tug.v.05'), Synset('turn.v.04'), Synset('turn.v.10'), Synset('turn.v.19'), Synset('unseat.v.02'), Synset('unwind.v.01'), Synset('uproot.v.03'), Synset('upstage.v.02'), Synset('wash.v.04'), Synset('wedge.v.02'), Synset('whistle.v.04'), Synset('wind.v.03'), Synset('woosh.v.01'), Synset('work.v.10'), Synset('work.v.21')]]], [[[Synset('australopithecine.n.01'), Synset('dryopithecine.n.01'), Synset('homo.n.02'), Synset('javanthropus.n.01'), Synset('pithecanthropus.n.01'), Synset('sinanthropus.n.01'), Synset('sivapithecus.n.01')]]], [], [[[Synset('artificial_insemination.n.01')]]], [], [[[Synset('cavil.v.01'), Synset('challenge.v.04'), Synset('demur.v.01'), Synset('mind.v.01'), Synset('raise_hell.v.01'), Synset('remonstrate.v.01')]]], [], [[[Synset('apply.v.09'), Synset('euphemize.v.01'), Synset('express.v.04'), Synset('hark_back.v.01'), Synset('name.v.02'), Synset('slur.v.02'), Synset('twist.v.09')]]], [[[Synset('develop.v.21'), Synset('inflate.v.02')]]], [[[Synset('deploy.v.02'), Synset('diffuse.v.01'), Synset('discharge.v.02'), Synset('export.v.03'), Synset('generalize.v.04'), Synset('metastasize.v.01'), Synset('propagate.v.05'), Synset('redistribute.v.01'), Synset('sprawl.v.02'), Synset('strew.v.01')]]], [[[Synset('guerrilla_force.n.01'), Synset('line_personnel.n.01'), Synset('management_personnel.n.01'), Synset('military.n.01'), Synset('military_personnel.n.01'), Synset('military_police.n.01'), Synset('military_service.n.01'), Synset('paramilitary.n.01'), Synset('patrol.n.03'), Synset('police.n.01'), Synset('rank_and_file.n.01'), Synset('security_force.n.01'), Synset('staff.n.01'), Synset('work_force.n.01')]]], [[[Synset('action.n.07'), Synset('actuator.n.01'), Synset('automaton.n.02'), Synset('axis.n.06'), Synset('carriage.n.04'), Synset('cartridge_ejector.n.01'), Synset('cartridge_extractor.n.01'), Synset('clockwork.n.01'), Synset('control.n.09'), Synset('cooling_system.n.02'), Synset('delayed_action.n.01'), Synset('drive.n.02'), Synset('drive_line.n.01'), Synset('fail-safe.n.01'), Synset('film_advance.n.01'), Synset('gear.n.03'), Synset('homing_device.n.01'), Synset('hydraulic_system.n.01'), Synset('ignition.n.02'), Synset('lock.n.03'), Synset('mechanical_device.n.01'), Synset('radiator.n.03'), Synset('rotating_mechanism.n.01'), Synset('steering_linkage.n.01'), Synset('steering_system.n.01'), Synset('tape_drive.n.01'), Synset('whirler.n.02'), Synset('works.n.04')]]], [[[Synset('apparition.n.01'), Synset('banshee.n.01'), Synset('control.n.10'), Synset('evil_spirit.n.01'), Synset('familiar.n.03'), Synset('genie.n.01'), Synset('kachina.n.02'), Synset('numen.n.01'), Synset('peri.n.02'), Synset('presence.n.03'), Synset('python.n.02'), Synset('sylvan.n.01'), Synset('thunderbird.n.01'), Synset('zombi.n.01')]]], [[[Synset('allotment.n.01'), Synset('allowance.n.01'), Synset('cut.n.01'), Synset('dispensation.n.02'), Synset('dole.n.01'), Synset('interest.n.05'), Synset('profit_sharing.n.01'), Synset('ration.n.02'), Synset('slice.n.01'), Synset('split.n.03'), Synset('tranche.n.01'), Synset('way.n.12')]]], [[[Synset('australopithecine.n.01'), Synset('dryopithecine.n.01'), Synset('homo.n.02'), Synset('javanthropus.n.01'), Synset('pithecanthropus.n.01'), Synset('sinanthropus.n.01'), Synset('sivapithecus.n.01')]]], [[[Synset('agrobiology.n.01'), Synset('agrology.n.01'), Synset('agronomy.n.01'), Synset('architectonics.n.01'), Synset('cognitive_science.n.01'), Synset('cryptanalysis.n.01'), Synset('information_science.n.01'), Synset('linguistics.n.01'), Synset('mathematics.n.01'), Synset('metallurgy.n.01'), Synset('metrology.n.01'), Synset('natural_history.n.01'), Synset('natural_science.n.01'), Synset('nutrition.n.03'), Synset('psychology.n.01'), Synset('social_science.n.01'), Synset('strategics.n.01'), Synset('systematics.n.01'), Synset('thanatology.n.01')]]], [[[Synset('associate.v.01'), Synset('brainstorm.v.01'), Synset('chew_over.v.01'), Synset('concentrate.v.02'), Synset('evaluate.v.02'), Synset('give.v.10'), Synset('philosophize.v.01'), Synset('plan.v.02'), Synset('puzzle_over.v.01'), Synset('rationalize.v.04'), Synset('reason.v.01'), Synset('reason.v.03'), Synset('study.v.06'), Synset('think.v.08'), Synset('think.v.09'), Synset('think.v.11'), Synset('think_about.v.01')]]], [[[Synset('carry_through.v.01'), Synset('close.v.17'), Synset('follow_through.v.02'), Synset('get_through.v.01'), Synset('round_out.v.01'), Synset('see_through.v.03'), Synset('top.v.10')]]], [], [], [[[Synset('agar.n.02'), Synset('algin.n.01'), Synset('ammoniac.n.01'), Synset('balata.n.01'), Synset('carrageenin.n.01'), Synset('cherry-tree_gum.n.01'), Synset('chicle.n.01'), Synset('conima.n.01'), Synset('dragon's_blood.n.01'), Synset('eucalyptus_gum.n.01'), Synset('euphorbium.n.01'), Synset('frankincense.n.01'), Synset('galbanum.n.01'), Synset('ghatti.n.01'), Synset('guar_gum.n.01'), Synset('gum_arabic.n.01'), Synset('gum_butea.n.01'), Synset('gutta-percha.n.01'), Synset('kino.n.01'), Synset('lacquer.n.01'), Synset('mesquite_gum.n.01'), Synset('mucilage.n.01'), Synset('opopanax.n.01'), Synset('sangapenum.n.01'), Synset('sterculia_gum.n.01'), Synset('sweet_gum.n.02'), Synset('tragacanth.n.01')]]], [], [], [], [], [[[Synset('cavalry.n.01'), Synset('friendly.n.01'), Synset('garrison.n.03'), Synset('hostile.n.01')]]], [[[Synset('artificial_insemination.n.01')]]], [[[Synset('effect.n.04'), Synset('grammatical_meaning.n.01'), Synset('intension.n.01'), Synset('intent.n.02'), Synset('lexical_meaning.n.01'), Synset('moral.n.01'), Synset('nuance.n.01'), Synset('overtone.n.01'), Synset('point.n.03'), Synset('referent.n.01'), Synset('sense.n.02'), Synset('symbolization.n.01')]]], [[[Synset('associate.v.01'), Synset('brainstorm.v.01'), Synset('chew_over.v.01'), Synset('concentrate.v.02'), Synset('evaluate.v.02'), Synset('give.v.10'), Synset('philosophize.v.01'), Synset('plan.v.02'), Synset('puzzle_over.v.01'), Synset('rationalize.v.04'), Synset('reason.v.01'), Synset('reason.v.03'), Synset('study.v.06'), Synset('think.v.08'), Synset('think.v.09'), Synset('think.v.11'), Synset('think_about.v.01')]]], [], [[[Synset('body_of_water.n.01'), Synset('inessential.n.01'), Synset('necessity.n.02'), Synset('part.n.03'), Synset('reservoir.n.04'), Synset('subject.n.02'), Synset('unit.n.05'), Synset('variable.n.01')]]], [], [[[Synset('bring_to.v.01'), Synset('call.v.28'), Synset('reawaken.v.01')]]], [[[Synset('approach.v.05'), Synset('greet.v.01')]]], [[[Synset('australopithecine.n.01'), Synset('dryopithecine.n.01'), Synset('homo.n.02'), Synset('javanthropus.n.01'), Synset('pithecanthropus.n.01'), Synset('sinanthropus.n.01'), Synset('sivapithecus.n.01')]]], [[[Synset('advance.v.05'), Synset('agitate.v.06'), Synset('beat.v.08'), Synset('blow.v.14'), Synset('brandish.v.01'), Synset('center.v.03'), Synset('change_hands.v.01'), Synset('chase_away.v.01'), Synset('circulate.v.03'), Synset('circulate.v.06'), Synset('dandle.v.01'), Synset('disarrange.v.01'), Synset('dislocate.v.01'), Synset('displace.v.01'), Synset('drag.v.07'), Synset('drive.v.03'), Synset('drop.v.01'), Synset('engage.v.10'), Synset('expel.v.01'), Synset('exteriorize.v.01'), Synset('flick.v.06'), Synset('fluctuate.v.01'), Synset('funnel.v.01'), Synset('glide.v.03'), Synset('herd.v.01'), Synset('hit.v.12'), Synset('hustle.v.01'), Synset('jar.v.03'), Synset('lateralize.v.01'), Synset('launch.v.05'), Synset('lift.v.02'), Synset('lift.v.03'), Synset('lower.v.01'), Synset('mobilize.v.04'), Synset('overturn.v.02'), Synset('play.v.24'), Synset('pour.v.01'), Synset('press_down.v.01'), Synset('propel.v.01'), Synset('pull.v.01'), Synset('pull.v.09'), Synset('pulse.v.03'), Synset('pump.v.06'), Synset('push.v.01'), Synset('put.v.01'), Synset('race.v.04'), Synset('raise.v.02'), Synset('rake.v.01'), Synset('relocate.v.02'), Synset('rock.v.02'), Synset('roll.v.14'), Synset('rout_out.v.02'), Synset('run.v.26'), Synset('saltate.v.01'), Synset('scan.v.05'), Synset('send.v.01'), Synset('separate.v.02'), Synset('shift.v.02'), Synset('singsong.v.02'), Synset('sink.v.02'), Synset('slide.v.03'), Synset('sling.v.03'), Synset('slip.v.09'), Synset('spill.v.01'), Synset('spill.v.03'), Synset('station.v.01'), Synset('stir.v.01'), Synset('swing.v.01'), Synset('take_back.v.04'), Synset('transfer.v.02'), Synset('transfer.v.04'), Synset('translate.v.05'), Synset('translate.v.08'), Synset('transmit.v.04'), Synset('transport.v.01'), Synset('transport.v.02'), Synset('transport.v.04'), Synset('tug.v.05'), Synset('turn.v.04'), Synset('turn.v.10'), Synset('turn.v.19'), Synset('unseat.v.02'), Synset('unwind.v.01'), Synset('uproot.v.03'), Synset('upstage.v.02'), Synset('wash.v.04'), Synset('wedge.v.02'), Synset('whistle.v.04'), Synset('wind.v.03'), Synset('woosh.v.01'), Synset('work.v.10'), Synset('work.v.21')]]], [[[Synset('artificial_insemination.n.01')]]], [[[Synset('abound.v.01'), Synset('accept.v.08'), Synset('account.v.01'), Synset('account_for.v.01'), Synset('act.v.06'), Synset('answer.v.06'), Synset('appear.v.04'), Synset('bake.v.04'), Synset('balance.v.04'), Synset('be_well.v.01'), Synset('beat.v.12'), Synset('begin.v.06'), Synset('begin.v.07'), Synset('belong.v.01'), Synset('belong.v.02'), Synset('belong.v.04'), Synset('belong.v.05'), Synset('breathe.v.04'), Synset('buy.v.03'), Synset('clean.v.05'), Synset('cohere.v.03'), Synset('come_in_for.v.01'), Synset('come_in_handy.v.01'), Synset('compact.v.01'), Synset('compare.v.02'), Synset('confuse.v.02'), Synset('connect.v.07'), Synset('consist.v.02'), Synset('consist.v.04'), Synset('contain.v.04'), Synset('contain.v.05'), Synset('continue.v.10'), Synset('cost.v.01'), Synset('count.v.02'), Synset('count.v.07'), Synset('cover.v.18'), Synset('cut.v.25'), Synset('cut_across.v.02'), Synset('deck.v.01'), Synset('depend.v.01'), Synset('deserve.v.01'), Synset('disagree.v.02'), Synset('distribute.v.09'), Synset('diverge.v.02'), Synset('draw.v.21'), Synset('end.v.03'), Synset('fall.v.04'), Synset('fall.v.16'), Synset('feel.v.04'), Synset('figure.v.02'), Synset('fit.v.07'), Synset('gape.v.02'), Synset('go.v.10'), Synset('gravitate.v.02'), Synset('hail.v.02'), Synset('hang.v.06'), Synset('head.v.04'), Synset('hold.v.17'), Synset('hoodoo.v.01'), Synset('hum.v.02'), Synset('impend.v.01'), Synset('incarnate.v.02'), Synset('iridesce.v.01'), Synset('jumble.v.01'), Synset('kill.v.04'), Synset('lend.v.03'), Synset('let_go.v.02'), Synset('lie.v.04'), Synset('litter.v.01'), Synset('loiter.v.01'), Synset('look.v.02'), Synset('look.v.03'), Synset('lubricate.v.01'), Synset('make.v.31'), Synset('make_sense.v.01'), Synset('measure.v.03'), Synset('mope.v.02'), Synset('object.v.02'), Synset('osculate.v.01'), Synset('owe.v.03'), Synset('pay.v.07'), Synset('point.v.10'), Synset('press.v.08'), Synset('promise.v.04'), Synset('prove.v.01'), Synset('put_out.v.06'), Synset('rage.v.02'), Synset('range.v.01'), Synset('rank.v.01'), Synset('rate.v.02'), Synset('recognize.v.08'), Synset('relate.v.04'), Synset('remain.v.03'), Synset('represent.v.03'), Synset('rest.v.01'), Synset('retard.v.02'), Synset('run.v.05'), Synset('run_into.v.01'), Synset('rut.v.01'), Synset('seem.v.03'), Synset('seethe.v.02'), Synset('sell.v.02'), Synset('sell.v.06'), Synset('sell.v.07'), Synset('shine.v.04'), Synset('shine.v.05'), Synset('sparkle.v.02'), Synset('specify.v.03'), Synset('squat.v.02'), Synset('stagnate.v.01'), Synset('stagnate.v.03'), Synset('stand.v.02'), Synset('stand.v.03'), Synset('stand_by.v.03'), Synset('stay.v.01'), Synset('stay.v.04'), Synset('stick.v.04'), Synset('stink.v.01'), Synset('subtend.v.01'), Synset('suck.v.04'), Synset('suffer.v.06'), Synset('suffer.v.08'), Synset('suit.v.02'), Synset('swim.v.03'), Synset('swim.v.04'), Synset('swing.v.10'), Synset('tend.v.01'), Synset('test.v.04'), Synset('total.v.01'), Synset('translate.v.07'), Synset('transplant.v.02'), Synset('trim.v.05'), Synset('underlie.v.01'), Synset('want.v.02'), Synset('wash.v.05'), Synset('wind.v.02'), Synset('work.v.14')]]], [[[Synset('ace.n.02'), Synset('club.n.06'), Synset('crib.n.04'), Synset('deuce.n.04'), Synset('diamond.n.04'), Synset('draw.n.05'), Synset('face_card.n.01'), Synset('gaming_card.n.01'), Synset('heart.n.10'), Synset('hole_card.n.02'), Synset('joker.n.04'), Synset('singleton.n.03'), Synset('spade.n.01'), Synset('spot.n.13'), Synset('stopper.n.03'), Synset('suit.n.06'), Synset('trey.n.02'), Synset('trump.n.01'), Synset('wild_card.n.02')]]], [[[Synset('basket.n.04'), Synset('break.n.12'), Synset('bull's_eye.n.01'), Synset('conversion.n.03'), Synset('equalizer.n.03'), Synset('field_goal.n.01'), Synset('goal.n.04'), Synset('hat_trick.n.01'), Synset('open_frame.n.01'), Synset('run.n.01'), Synset('safety.n.06'), Synset('spare.n.03'), Synset('strike.n.04'), Synset('touchdown.n.01')]]], [[[Synset('advance.v.08'), Synset('build.v.04'), Synset('build_up.v.05'), Synset('condition.v.04'), Synset('educate.v.01'), Synset('emend.v.01'), Synset('enhance.v.02'), Synset('enrich.v.01'), Synset('fancify.v.01'), Synset('fructify.v.02'), Synset('help.v.02'), Synset('help.v.08'), Synset('iron_out.v.01'), Synset('perfect.v.01'), Synset('polish.v.02'), Synset('purify.v.01'), Synset('raise.v.25'), Synset('reform.v.01'), Synset('reform.v.05'), Synset('regenerate.v.09'), Synset('relieve.v.01'), Synset('repair.v.01'), Synset('turn_around.v.02'), Synset('upgrade.v.02'), Synset('upgrade.v.05')]]], [[[Synset('actinoid.n.01'), Synset('arrangement.n.02'), Synset('association.n.08'), Synset('biological_group.n.01'), Synset('circuit.n.05'), Synset('citizenry.n.01'), Synset('collection.n.01'), Synset('community.n.06'), Synset('edition.n.02'), Synset('electron_shell.n.01'), Synset('ethnic_group.n.01'), Synset('halogen.n.01'), Synset('kingdom.n.06'), Synset('multitude.n.03'), Synset('people.n.01'), Synset('population.n.02'), Synset('race.n.03'), Synset('rare_earth.n.01'), Synset('sainthood.n.01'), Synset('series.n.06'), Synset('social_group.n.01'), Synset('straggle.n.01'), Synset('subgroup.n.01'), Synset('swarm.n.02'), Synset('system.n.02'), Synset('varna.n.02'), Synset('world.n.08')]]], [[[Synset('acidify.v.02'), Synset('alkalize.v.01'), Synset('be_born.v.01'), Synset('become.v.01'), Synset('better.v.03'), Synset('boil.v.01'), Synset('break.v.55'), Synset('burn.v.03'), Synset('calcify.v.03'), Synset('calm.v.03'), Synset('carbonize.v.01'), Synset('carbonize.v.02'), Synset('carnify.v.01'), Synset('catalyze.v.01'), Synset('chondrify.v.01'), Synset('citrate.v.01'), Synset('close.v.02'), Synset('clot.v.01'), Synset('cloud_over.v.02'), Synset('coke.v.01'), Synset('come_to.v.04'), Synset('conceive.v.03'), Synset('concentrate.v.01'), Synset('cool.v.02'), Synset('cross-fertilize.v.01'), Synset('curdle.v.01'), Synset('denitrify.v.01'), Synset('die.v.01'), Synset('disengage.v.03'), Synset('dissolve.v.01'), Synset('dress.v.02'), Synset('emaciate.v.02'), Synset('emancipate.v.01'), Synset('empty.v.02'), Synset('emulsify.v.02'), Synset('equilibrate.v.01'), Synset('erupt.v.03'), Synset('esterify.v.01'), Synset('etherify.v.01'), Synset('explode.v.04'), Synset('fall.v.03'), Synset('fill.v.02'), Synset('fluctuate.v.03'), Synset('freeze.v.02'), Synset('frenchify.v.02'), Synset('gain.v.09'), Synset('get_into.v.01'), Synset('get_worse.v.01'), Synset('heat.v.04'), Synset('homogenize.v.02'), Synset('homogenize.v.03'), Synset('integrate.v.03'), Synset('ionize.v.02'), Synset('liquefy.v.01'), Synset('open.v.03'), Synset('ossify.v.01'), Synset('overgrow.v.02'), Synset('precipitate.v.02'), Synset('react.v.03'), Synset('reduce.v.20'), Synset('relax.v.01'), Synset('secularize.v.01'), Synset('solvate.v.02'), Synset('sorb.v.01'), Synset('sour.v.01'), Synset('tense.v.03'), Synset('thicken.v.02'), Synset('thin.v.01'), Synset('thrive.v.02'), Synset('thrombose.v.01'), Synset('wake_up.v.02'), Synset('worsen.v.01'), Synset('zonk_out.v.01')]]], [[[Synset('evil.n.02'), Synset('inadvisability.n.01'), Synset('liability.n.03'), Synset('undesirability.n.01'), Synset('unsoundness.n.03'), Synset('unworthiness.n.01'), Synset('worse.n.01')]]], [], [[[Synset('creditworthiness.n.01'), Synset('responsibility.n.03')]]], [], [[[Synset('bourbon.n.04'), Synset('capetian_dynasty.n.01'), Synset('carolingian_dynasty.n.01'), Synset('flavian_dynasty.n.01'), Synset('habsburg.n.01'), Synset('han.n.01'), Synset('hanover.n.02'), Synset('hohenzollern.n.01'), Synset('lancaster.n.02'), Synset('liao.n.01'), Synset('merovingian.n.02'), Synset('ming.n.01'), Synset('ottoman.n.02'), Synset('plantagenet.n.01'), Synset('ptolemy.n.02'), Synset('qin.n.01'), Synset('qing.n.01'), Synset('romanov.n.02'), Synset('saxe-coburg-gotha.n.01'), Synset('seljuk.n.01'), Synset('shang.n.01'), Synset('stuart.n.03'), Synset('sung.n.01'), Synset('tang.n.02'), Synset('tudor.n.01'), Synset('umayyad.n.01'), Synset('valois.n.01'), Synset('wei.n.01'), Synset('windsor.n.02'), Synset('york.n.01'), Synset('yuan.n.02'), Synset('zhou.n.01')]]], [[[Synset('breed.n.02'), Synset('nature.n.05'), Synset('version.n.02')]]], [[[Synset('air-cool.v.01'), Synset('alphabetize.v.02'), Synset('arm.v.02'), Synset('bed.v.01'), Synset('berth.v.01'), Synset('bewhisker.v.01'), Synset('border.v.04'), Synset('bottom.v.01'), Synset('brattice.v.01'), Synset('bush.v.01'), Synset('calk.v.01'), Synset('canal.v.01'), Synset('capitalize.v.02'), Synset('caption.v.01'), Synset('causeway.v.01'), Synset('charge.v.24'), Synset('cleat.v.01'), Synset('coal.v.02'), Synset('computerize.v.01'), Synset('constitutionalize.v.01'), Synset('copper-bottom.v.01'), Synset('corbel.v.01'), Synset('cornice.v.01'), Synset('costume.v.02'), Synset('crenel.v.01'), Synset('curtain.v.01'), Synset('dado.v.01'), Synset('date.v.05'), Synset('equip.v.01'), Synset('extend.v.04'), Synset('feed.v.03'), Synset('flood.v.03'), Synset('fret.v.03'), Synset('fuel.v.01'), Synset('fuel.v.02'), Synset('fund.v.03'), Synset('furnish.v.02'), Synset('gate.v.01'), Synset('glass.v.01'), Synset('grate.v.01'), Synset('hat.v.02'), Synset('headline.v.02'), Synset('headquarter.v.01'), Synset('heat.v.02'), Synset('hobnail.v.01'), Synset('hydrate.v.01'), Synset('index.v.02'), Synset('innervate.v.01'), Synset('interleave.v.01'), Synset('joint.v.02'), Synset('kern.v.01'), Synset('key.v.02'), Synset('leverage.v.02'), Synset('machicolate.v.01'), Synset('match.v.02'), Synset('offer.v.01'), Synset('partner.v.01'), Synset('patch.v.02'), Synset('pour.v.05'), Synset('provision.v.01'), Synset('pump.v.04'), Synset('rafter.v.01'), Synset('rail.v.03'), Synset('railroad.v.02'), Synset('ramp.v.02'), Synset('reflectorize.v.01'), Synset('retrofit.v.01'), Synset('rim.v.02'), Synset('sanitate.v.01'), Synset('seat.v.04'), Synset('seat.v.05'), Synset('shelter.v.01'), Synset('signalize.v.01'), Synset('slat.v.01'), Synset('step.v.05'), Synset('stint.v.02'), Synset('stock.v.03'), Synset('stock.v.04'), Synset('stock.v.05'), Synset('stock.v.06'), Synset('subtitle.v.01'), Synset('surfeit.v.01'), Synset('tap.v.06'), Synset('terrace.v.01'), Synset('theme.v.01'), Synset('ticket.v.02'), Synset('toggle.v.01'), Synset('tool.v.03'), Synset('top.v.05'), Synset('transistorize.v.01'), Synset('tube.v.01'), Synset('uniform.v.01'), Synset('upholster.v.01'), Synset('victual.v.01'), Synset('water.v.02'), Synset('wharf.v.01'), Synset('wive.v.03'), Synset('yield.v.01')]]], [[[Synset('evacuate.v.05'), Synset('exude.v.01'), Synset('make.v.49'), Synset('stool.v.04'), Synset('sweat.v.01'), Synset('urinate.v.01'), Synset('vomit.v.01')]]], [[[Synset('artificial_insemination.n.01')]]], [[[Synset('australopithecine.n.01'), Synset('dryopithecine.n.01'), Synset('homo.n.02'), Synset('javanthropus.n.01'), Synset('pithecanthropus.n.01'), Synset('sinanthropus.n.01'), Synset('sivapithecus.n.01')]]], [], [[[Synset('abound.v.01'), Synset('accept.v.08'), Synset('account.v.01'), Synset('account_for.v.01'), Synset('act.v.06'), Synset('answer.v.06'), Synset('appear.v.04'), Synset('bake.v.04'), Synset('balance.v.04'), Synset('be_well.v.01'), Synset('beat.v.12'), Synset('begin.v.06'), Synset('begin.v.07'), Synset('belong.v.01'), Synset('belong.v.02'), Synset('belong.v.04'), Synset('belong.v.05'), Synset('breathe.v.04'), Synset('buy.v.03'), Synset('clean.v.05'), Synset('cohere.v.03'), Synset('come_in_for.v.01'), Synset('come_in_handy.v.01'), Synset('compact.v.01'), Synset('compare.v.02'), Synset('confuse.v.02'), Synset('connect.v.07'), Synset('consist.v.02'), Synset('consist.v.04'), Synset('contain.v.04'), Synset('contain.v.05'), Synset('continue.v.10'), Synset('cost.v.01'), Synset('count.v.02'), Synset('count.v.07'), Synset('cover.v.18'), Synset('cut.v.25'), Synset('cut_across.v.02'), Synset('deck.v.01'), Synset('depend.v.01'), Synset('deserve.v.01'), Synset('disagree.v.02'), Synset('distribute.v.09'), Synset('diverge.v.02'), Synset('draw.v.21'), Synset('end.v.03'), Synset('fall.v.04'), Synset('fall.v.16'), Synset('feel.v.04'), Synset('figure.v.02'), Synset('fit.v.07'), Synset('gape.v.02'), Synset('go.v.10'), Synset('gravitate.v.02'), Synset('hail.v.02'), Synset('hang.v.06'), Synset('head.v.04'), Synset('hold.v.17'), Synset('hoodoo.v.01'), Synset('hum.v.02'), Synset('impend.v.01'), Synset('incarnate.v.02'), Synset('iridesce.v.01'), Synset('jumble.v.01'), Synset('kill.v.04'), Synset('lend.v.03'), Synset('let_go.v.02'), Synset('lie.v.04'), Synset('litter.v.01'), Synset('loiter.v.01'), Synset('look.v.02'), Synset('look.v.03'), Synset('lubricate.v.01'), Synset('make.v.31'), Synset('make_sense.v.01'), Synset('measure.v.03'), Synset('mope.v.02'), Synset('object.v.02'), Synset('osculate.v.01'), Synset('owe.v.03'), Synset('pay.v.07'), Synset('point.v.10'), Synset('press.v.08'), Synset('promise.v.04'), Synset('prove.v.01'), Synset('put_out.v.06'), Synset('rage.v.02'), Synset('range.v.01'), Synset('rank.v.01'), Synset('rate.v.02'), Synset('recognize.v.08'), Synset('relate.v.04'), Synset('remain.v.03'), Synset('represent.v.03'), Synset('rest.v.01'), Synset('retard.v.02'), Synset('run.v.05'), Synset('run_into.v.01'), Synset('rut.v.01'), Synset('seem.v.03'), Synset('seethe.v.02'), Synset('sell.v.02'), Synset('sell.v.06'), Synset('sell.v.07'), Synset('shine.v.04'), Synset('shine.v.05'), Synset('sparkle.v.02'), Synset('specify.v.03'), Synset('squat.v.02'), Synset('stagnate.v.01'), Synset('stagnate.v.03'), Synset('stand.v.02'), Synset('stand.v.03'), Synset('stand_by.v.03'), Synset('stay.v.01'), Synset('stay.v.04'), Synset('stick.v.04'), Synset('stink.v.01'), Synset('subtend.v.01'), Synset('suck.v.04'), Synset('suffer.v.06'), Synset('suffer.v.08'), Synset('suit.v.02'), Synset('swim.v.03'), Synset('swim.v.04'), Synset('swing.v.10'), Synset('tend.v.01'), Synset('test.v.04'), Synset('total.v.01'), Synset('translate.v.07'), Synset('transplant.v.02'), Synset('trim.v.05'), Synset('underlie.v.01'), Synset('want.v.02'), Synset('wash.v.05'), Synset('wind.v.02'), Synset('work.v.14')]]], [[[Synset('cavalry.n.01'), Synset('friendly.n.01'), Synset('garrison.n.03'), Synset('hostile.n.01')]]], [[[Synset('artificial_insemination.n.01')]]], [], [], [[[Synset('breed.v.02'), Synset('deflower.v.01'), Synset('nick.v.04'), Synset('ride.v.14'), Synset('serve.v.14'), Synset('sleep_together.v.01'), Synset('sodomize.v.01'), Synset('sodomize.v.02'), Synset('tread.v.06')]]], [], [[[Synset('australopithecine.n.01'), Synset('dryopithecine.n.01'), Synset('homo.n.02'), Synset('javanthropus.n.01'), Synset('pithecanthropus.n.01'), Synset('sinanthropus.n.01'), Synset('sivapithecus.n.01')]]], [], [], [[[Synset('affected_role.n.01'), Synset('agentive_role.n.01'), Synset('benefactive_role.n.01'), Synset('instrumental_role.n.01'), Synset('locative_role.n.01'), Synset('recipient_role.n.01'), Synset('resultant_role.n.01'), Synset('temporal_role.n.01')]]], [[[Synset('earth.v.02')]]], [], [], [[[Synset('artificial_insemination.n.01')]]], [], [[[Synset('americana.n.01'), Synset('anachronism.n.02'), Synset('antiquity.n.03'), Synset('article.n.02'), Synset('block.n.01'), Synset('building_material.n.01'), Synset('button.n.07'), Synset('commodity.n.01'), Synset('cone.n.01'), Synset('covering.n.02'), Synset('creation.n.02'), Synset('decker.n.02'), Synset('decoration.n.01'), Synset('electroplate.n.01'), Synset('excavation.n.03'), Synset('extra.n.03'), Synset('fabric.n.01'), Synset('facility.n.01'), Synset('facility.n.04'), Synset('fixture.n.01'), Synset('float.n.06'), Synset('insert.n.02'), Synset('instrumentality.n.03'), Synset('layer.n.01'), Synset('lemon.n.05'), Synset('line.n.18'), Synset('marker.n.01'), Synset('mystification.n.02'), Synset('opening.n.10'), Synset('padding.n.01'), Synset('paving.n.01'), Synset('plaything.n.01'), Synset('ready-made.n.01'), Synset('restoration.n.05'), Synset('sheet.n.06'), Synset('sphere.n.02'), Synset('square.n.07'), Synset('squeaker.n.01'), Synset('strip.n.02'), Synset('structure.n.01'), Synset('surface.n.01'), Synset('thing.n.04'), Synset('track.n.03'), Synset('way.n.06'), Synset('weight.n.04')]]], [[[Synset('achieve.v.01'), Synset('arrive.v.02'), Synset('hit.v.16'), Synset('luck_out.v.01'), Synset('nail_down.v.01'), Synset('pan_out.v.01'), Synset('pass.v.09'), Synset('pass.v.14'), Synset('pull_off.v.03'), Synset('run.v.24'), Synset('work.v.03')]]], [[[Synset('basket.n.04'), Synset('break.n.12'), Synset('bull's_eye.n.01'), Synset('conversion.n.03'), Synset('equalizer.n.03'), Synset('field_goal.n.01'), Synset('goal.n.04'), Synset('hat_trick.n.01'), Synset('open_frame.n.01'), Synset('run.n.01'), Synset('safety.n.06'), Synset('spare.n.03'), Synset('strike.n.04'), Synset('touchdown.n.01')]]], [[[Synset('cave.v.02'), Synset('map.v.02'), Synset('pioneer.v.03')]]], [], [], [[[Synset('baptize.v.01'), Synset('dub.v.01'), Synset('entitle.v.02'), Synset('refer.v.07'), Synset('rename.v.01'), Synset('style.v.01'), Synset('tag.v.03'), Synset('term.v.01')]]], [[[Synset('cavalry.n.01'), Synset('friendly.n.01'), Synset('garrison.n.03'), Synset('hostile.n.01')]]], [[[Synset('artificial_insemination.n.01')]]], [[[Synset('artificial_insemination.n.01')]]], [], [[[Synset('basket.n.04'), Synset('break.n.12'), Synset('bull's_eye.n.01'), Synset('conversion.n.03'), Synset('equalizer.n.03'), Synset('field_goal.n.01'), Synset('goal.n.04'), Synset('hat_trick.n.01'), Synset('open_frame.n.01'), Synset('run.n.01'), Synset('safety.n.06'), Synset('spare.n.03'), Synset('strike.n.04'), Synset('touchdown.n.01')]]], [[[Synset('align.v.04'), Synset('attune.v.01'), Synset('calibrate.v.01'), Synset('citify.v.01'), Synset('depressurize.v.01'), Synset('focus.v.05'), Synset('harmonize.v.05'), Synset('justify.v.05'), Synset('linearize.v.01'), Synset('match.v.05'), Synset('modulate.v.04'), Synset('plumb.v.04'), Synset('pressurize.v.03'), Synset('proportion.v.02'), Synset('readjust.v.02'), Synset('regulate.v.01'), Synset('set.v.08'), Synset('synchronize.v.01'), Synset('temper.v.03'), Synset('time.v.04'), Synset('time.v.05'), Synset('trim.v.09'), Synset('tune.v.01'), Synset('tune.v.02'), Synset('zero.v.01'), Synset('zero.v.02')]]], [[[Synset('australopithecine.n.01'), Synset('dryopithecine.n.01'), Synset('homo.n.02'), Synset('javanthropus.n.01'), Synset('pithecanthropus.n.01'), Synset('sinanthropus.n.01'), Synset('sivapithecus.n.01')]]], [[[Synset('achieve.v.01'), Synset('arrive.v.02'), Synset('hit.v.16'), Synset('luck_out.v.01'), Synset('nail_down.v.01'), Synset('pan_out.v.01'), Synset('pass.v.09'), Synset('pass.v.14'), Synset('pull_off.v.03'), Synset('run.v.24'), Synset('work.v.03')]]], [], [], [[[Synset('forget.v.04')]]], [[[Synset('first_base.n.01'), Synset('home_plate.n.01'), Synset('second_base.n.01'), Synset('third_base.n.01')]]], [[[Synset('curtsy.v.02'), Synset('hail.v.04'), Synset('salute.v.02'), Synset('salute.v.05'), Synset('salute.v.06'), Synset('say_farewell.v.01'), Synset('shake_hands.v.01'), Synset('welcome.v.02'), Synset('wish.v.06')]]], [], [], [[[Synset('content.v.02'), Synset('please.v.01'), Synset('please.v.03')]]], [[[Synset('bare.v.01'), Synset('unclothe.v.02'), Synset('undrape.v.01'), Synset('unmask.v.02'), Synset('unveil.v.01'), Synset('unwrap.v.01')]]], [], [], [[[Synset('fancier.n.01'), Synset('suitor.n.01'), Synset('worshiper.n.01')]]], [[[Synset('aleph-null.n.01'), Synset('billion.n.01'), Synset('billion.n.03'), Synset('crore.n.01'), Synset('eighteen.n.01'), Synset('eighty.n.01'), Synset('eleven.n.01'), Synset('fifteen.n.01'), Synset('fifty.n.01'), Synset('five_hundred.n.01'), Synset('forty.n.01'), Synset('fourteen.n.01'), Synset('great_gross.n.01'), Synset('gross.n.01'), Synset('hundred.n.01'), Synset('hundred_thousand.n.01'), Synset('long_hundred.n.01'), Synset('million.n.01'), Synset('nineteen.n.01'), Synset('ninety.n.01'), Synset('octillion.n.01'), Synset('quadrillion.n.01'), Synset('quadrillion.n.02'), Synset('quintillion.n.01'), Synset('septillion.n.01'), Synset('seventeen.n.01'), Synset('seventy-eight.n.01'), Synset('seventy.n.01'), Synset('sextillion.n.01'), Synset('sixteen.n.01'), Synset('sixty.n.01'), Synset('teens.n.02'), Synset('ten.n.01'), Synset('ten_thousand.n.01'), Synset('thirteen.n.01'), Synset('thirty.n.01'), Synset('thousand.n.01'), Synset('trillion.n.02'), Synset('trillion.n.03'), Synset('twelve.n.01'), Synset('twenty-eight.n.01'), Synset('twenty-five.n.01'), Synset('twenty-four.n.01'), Synset('twenty-nine.n.01'), Synset('twenty-one.n.01'), Synset('twenty-seven.n.01'), Synset('twenty-six.n.01'), Synset('twenty-three.n.01'), Synset('twenty-two.n.01'), Synset('twenty.n.01')]]], [[[Synset('associate.v.01'), Synset('brainstorm.v.01'), Synset('chew_over.v.01'), Synset('concentrate.v.02'), Synset('evaluate.v.02'), Synset('give.v.10'), Synset('philosophize.v.01'), Synset('plan.v.02'), Synset('puzzle_over.v.01'), Synset('rationalize.v.04'), Synset('reason.v.01'), Synset('reason.v.03'), Synset('study.v.06'), Synset('think.v.08'), Synset('think.v.09'), Synset('think.v.11'), Synset('think_about.v.01')]]], [[[Synset('basic.n.02'), Synset('consumer_goods.n.01'), Synset('drygoods.n.01'), Synset('entrant.n.01'), Synset('export.n.01'), Synset('fancy_goods.n.01'), Synset('fungible.n.01'), Synset('future.n.03'), Synset('import.n.01'), Synset('merchandise.n.01'), Synset('middling.n.01'), Synset('salvage.n.01'), Synset('shopping.n.02'), Synset('sporting_goods.n.01'), Synset('worldly_possession.n.01')]]], [], [], [[[Synset('boat-race.v.01'), Synset('campaign.v.01'), Synset('horse-race.v.01'), Synset('place.v.15'), Synset('show.v.12'), Synset('speed_skate.v.01')]]], [[[Synset('alter.v.05'), Synset('emasculate.v.02'), Synset('vasectomize.v.01')]]], [[[Synset('anamorphosis.n.02'), Synset('carbon.n.03'), Synset('cast.n.06'), Synset('duplicate.n.02'), Synset('facsimile.n.01'), Synset('imitation.n.02'), Synset('knockoff.n.01'), Synset('miniature.n.02'), Synset('modification.n.02'), Synset('photocopy.n.01'), Synset('print.n.05'), Synset('quadruplicate.n.01'), Synset('replica.n.01'), Synset('triplicate.n.01'), Synset('xerox.n.01')]]], [[[Synset('artificial_insemination.n.01')]]], [[[Synset('abound.v.01'), Synset('accept.v.08'), Synset('account.v.01'), Synset('account_for.v.01'), Synset('act.v.06'), Synset('answer.v.06'), Synset('appear.v.04'), Synset('bake.v.04'), Synset('balance.v.04'), Synset('be_well.v.01'), Synset('beat.v.12'), Synset('begin.v.06'), Synset('begin.v.07'), Synset('belong.v.01'), Synset('belong.v.02'), Synset('belong.v.04'), Synset('belong.v.05'), Synset('breathe.v.04'), Synset('buy.v.03'), Synset('clean.v.05'), Synset('cohere.v.03'), Synset('come_in_for.v.01'), Synset('come_in_handy.v.01'), Synset('compact.v.01'), Synset('compare.v.02'), Synset('confuse.v.02'), Synset('connect.v.07'), Synset('consist.v.02'), Synset('consist.v.04'), Synset('contain.v.04'), Synset('contain.v.05'), Synset('continue.v.10'), Synset('cost.v.01'), Synset('count.v.02'), Synset('count.v.07'), Synset('cover.v.18'), Synset('cut.v.25'), Synset('cut_across.v.02'), Synset('deck.v.01'), Synset('depend.v.01'), Synset('deserve.v.01'), Synset('disagree.v.02'), Synset('distribute.v.09'), Synset('diverge.v.02'), Synset('draw.v.21'), Synset('end.v.03'), Synset('fall.v.04'), Synset('fall.v.16'), Synset('feel.v.04'), Synset('figure.v.02'), Synset('fit.v.07'), Synset('gape.v.02'), Synset('go.v.10'), Synset('gravitate.v.02'), Synset('hail.v.02'), Synset('hang.v.06'), Synset('head.v.04'), Synset('hold.v.17'), Synset('hoodoo.v.01'), Synset('hum.v.02'), Synset('impend.v.01'), Synset('incarnate.v.02'), Synset('iridesce.v.01'), Synset('jumble.v.01'), Synset('kill.v.04'), Synset('lend.v.03'), Synset('let_go.v.02'), Synset('lie.v.04'), Synset('litter.v.01'), Synset('loiter.v.01'), Synset('look.v.02'), Synset('look.v.03'), Synset('lubricate.v.01'), Synset('make.v.31'), Synset('make_sense.v.01'), Synset('measure.v.03'), Synset('mope.v.02'), Synset('object.v.02'), Synset('osculate.v.01'), Synset('owe.v.03'), Synset('pay.v.07'), Synset('point.v.10'), Synset('press.v.08'), Synset('promise.v.04'), Synset('prove.v.01'), Synset('put_out.v.06'), Synset('rage.v.02'), Synset('range.v.01'), Synset('rank.v.01'), Synset('rate.v.02'), Synset('recognize.v.08'), Synset('relate.v.04'), Synset('remain.v.03'), Synset('represent.v.03'), Synset('rest.v.01'), Synset('retard.v.02'), Synset('run.v.05'), Synset('run_into.v.01'), Synset('rut.v.01'), Synset('seem.v.03'), Synset('seethe.v.02'), Synset('sell.v.02'), Synset('sell.v.06'), Synset('sell.v.07'), Synset('shine.v.04'), Synset('shine.v.05'), Synset('sparkle.v.02'), Synset('specify.v.03'), Synset('squat.v.02'), Synset('stagnate.v.01'), Synset('stagnate.v.03'), Synset('stand.v.02'), Synset('stand.v.03'), Synset('stand_by.v.03'), Synset('stay.v.01'), Synset('stay.v.04'), Synset('stick.v.04'), Synset('stink.v.01'), Synset('subtend.v.01'), Synset('suck.v.04'), Synset('suffer.v.06'), Synset('suffer.v.08'), Synset('suit.v.02'), Synset('swim.v.03'), Synset('swim.v.04'), Synset('swing.v.10'), Synset('tend.v.01'), Synset('test.v.04'), Synset('total.v.01'), Synset('translate.v.07'), Synset('transplant.v.02'), Synset('trim.v.05'), Synset('underlie.v.01'), Synset('want.v.02'), Synset('wash.v.05'), Synset('wind.v.02'), Synset('work.v.14')]]], [[[Synset('contract_out.v.02'), Synset('disobey.v.01'), Synset('regret.v.03'), Synset('reject.v.04'), Synset('repudiate.v.03')]]], [[[Synset('australopithecine.n.01'), Synset('dryopithecine.n.01'), Synset('homo.n.02'), Synset('javanthropus.n.01'), Synset('pithecanthropus.n.01'), Synset('sinanthropus.n.01'), Synset('sivapithecus.n.01')]]], [], [[[Synset('construct.v.04'), Synset('construct.v.05'), Synset('design.v.04'), Synset('draw.v.04'), Synset('evolve.v.01'), Synset('gestate.v.01'), Synset('give_birth.v.02'), Synset('imagine.v.01'), Synset('invent.v.01'), Synset('plan.v.03'), Synset('program.v.02'), Synset('re-create.v.03'), Synset('schematize.v.01'), Synset('think_up.v.01'), Synset('write.v.10')]]], [[[Synset('cavalry.n.01'), Synset('friendly.n.01'), Synset('garrison.n.03'), Synset('hostile.n.01')]]], [[[Synset('artificial_insemination.n.01')]]], [[[Synset('abound.v.01'), Synset('accept.v.08'), Synset('account.v.01'), Synset('account_for.v.01'), Synset('act.v.06'), Synset('answer.v.06'), Synset('appear.v.04'), Synset('bake.v.04'), Synset('balance.v.04'), Synset('be_well.v.01'), Synset('beat.v.12'), Synset('begin.v.06'), Synset('begin.v.07'), Synset('belong.v.01'), Synset('belong.v.02'), Synset('belong.v.04'), Synset('belong.v.05'), Synset('breathe.v.04'), Synset('buy.v.03'), Synset('clean.v.05'), Synset('cohere.v.03'), Synset('come_in_for.v.01'), Synset('come_in_handy.v.01'), Synset('compact.v.01'), Synset('compare.v.02'), Synset('confuse.v.02'), Synset('connect.v.07'), Synset('consist.v.02'), Synset('consist.v.04'), Synset('contain.v.04'), Synset('contain.v.05'), Synset('continue.v.10'), Synset('cost.v.01'), Synset('count.v.02'), Synset('count.v.07'), Synset('cover.v.18'), Synset('cut.v.25'), Synset('cut_across.v.02'), Synset('deck.v.01'), Synset('depend.v.01'), Synset('deserve.v.01'), Synset('disagree.v.02'), Synset('distribute.v.09'), Synset('diverge.v.02'), Synset('draw.v.21'), Synset('end.v.03'), Synset('fall.v.04'), Synset('fall.v.16'), Synset('feel.v.04'), Synset('figure.v.02'), Synset('fit.v.07'), Synset('gape.v.02'), Synset('go.v.10'), Synset('gravitate.v.02'), Synset('hail.v.02'), Synset('hang.v.06'), Synset('head.v.04'), Synset('hold.v.17'), Synset('hoodoo.v.01'), Synset('hum.v.02'), Synset('impend.v.01'), Synset('incarnate.v.02'), Synset('iridesce.v.01'), Synset('jumble.v.01'), Synset('kill.v.04'), Synset('lend.v.03'), Synset('let_go.v.02'), Synset('lie.v.04'), Synset('litter.v.01'), Synset('loiter.v.01'), Synset('look.v.02'), Synset('look.v.03'), Synset('lubricate.v.01'), Synset('make.v.31'), Synset('make_sense.v.01'), Synset('measure.v.03'), Synset('mope.v.02'), Synset('object.v.02'), Synset('osculate.v.01'), Synset('owe.v.03'), Synset('pay.v.07'), Synset('point.v.10'), Synset('press.v.08'), Synset('promise.v.04'), Synset('prove.v.01'), Synset('put_out.v.06'), Synset('rage.v.02'), Synset('range.v.01'), Synset('rank.v.01'), Synset('rate.v.02'), Synset('recognize.v.08'), Synset('relate.v.04'), Synset('remain.v.03'), Synset('represent.v.03'), Synset('rest.v.01'), Synset('retard.v.02'), Synset('run.v.05'), Synset('run_into.v.01'), Synset('rut.v.01'), Synset('seem.v.03'), Synset('seethe.v.02'), Synset('sell.v.02'), Synset('sell.v.06'), Synset('sell.v.07'), Synset('shine.v.04'), Synset('shine.v.05'), Synset('sparkle.v.02'), Synset('specify.v.03'), Synset('squat.v.02'), Synset('stagnate.v.01'), Synset('stagnate.v.03'), Synset('stand.v.02'), Synset('stand.v.03'), Synset('stand_by.v.03'), Synset('stay.v.01'), Synset('stay.v.04'), Synset('stick.v.04'), Synset('stink.v.01'), Synset('subtend.v.01'), Synset('suck.v.04'), Synset('suffer.v.06'), Synset('suffer.v.08'), Synset('suit.v.02'), Synset('swim.v.03'), Synset('swim.v.04'), Synset('swing.v.10'), Synset('tend.v.01'), Synset('test.v.04'), Synset('total.v.01'), Synset('translate.v.07'), Synset('transplant.v.02'), Synset('trim.v.05'), Synset('underlie.v.01'), Synset('want.v.02'), Synset('wash.v.05'), Synset('wind.v.02'), Synset('work.v.14')]]], [[[Synset('accept.v.02'), Synset('acquire.v.05'), Synset('borrow.v.01'), Synset('buy.v.01'), Synset('buy.v.04'), Synset('capture.v.06'), Synset('catch.v.10'), Synset('collect.v.05'), Synset('come_by.v.02'), Synset('earn.v.02'), Synset('enter_upon.v.01'), Synset('find.v.03'), Synset('find.v.10'), Synset('gain.v.08'), Synset('get.v.21'), Synset('glom.v.02'), Synset('inherit.v.01'), Synset('isolate.v.02'), Synset('lease.v.04'), Synset('line_up.v.02'), Synset('obtain.v.01'), Synset('partake.v.02'), Synset('pick_up.v.06'), Synset('poll.v.03'), Synset('preempt.v.01'), Synset('preempt.v.03'), Synset('press_out.v.03'), Synset('profit.v.01'), Synset('receive.v.01'), Synset('reclaim.v.01'), Synset('recover.v.01'), Synset('recover.v.04'), Synset('turn.v.18'), Synset('win_back.v.01')]]], [[[Synset('calculate.v.05'), Synset('design.v.02'), Synset('mean.v.07'), Synset('slate.v.01')]]], [], [[[Synset('administer.v.04'), Synset('analyze.v.04'), Synset('bleed.v.02'), Synset('cauterize.v.01'), Synset('correct.v.08'), Synset('cup.v.03'), Synset('detox.v.01'), Synset('doctor.v.02'), Synset('dress.v.14'), Synset('hyperventilate.v.01'), Synset('insufflate.v.02'), Synset('iodize.v.02'), Synset('irrigate.v.02'), Synset('manipulate.v.06'), Synset('massage.v.02'), Synset('medicate.v.02'), Synset('nurse.v.01'), Synset('operate_on.v.01'), Synset('pack.v.13'), Synset('purge.v.07'), Synset('remedy.v.02'), Synset('shock.v.06'), Synset('splint.v.01'), Synset('vet.v.03')]]], [[[Synset('allotment.n.01'), Synset('allowance.n.01'), Synset('cut.n.01'), Synset('dispensation.n.02'), Synset('dole.n.01'), Synset('interest.n.05'), Synset('profit_sharing.n.01'), Synset('ration.n.02'), Synset('slice.n.01'), Synset('split.n.03'), Synset('tranche.n.01'), Synset('way.n.12')]]], [[[Synset('artificial_insemination.n.01')]]], [[[Synset('australopithecine.n.01'), Synset('dryopithecine.n.01'), Synset('homo.n.02'), Synset('javanthropus.n.01'), Synset('pithecanthropus.n.01'), Synset('sinanthropus.n.01'), Synset('sivapithecus.n.01')]]], [[[Synset('agrobiology.n.01'), Synset('agrology.n.01'), Synset('agronomy.n.01'), Synset('architectonics.n.01'), Synset('cognitive_science.n.01'), Synset('cryptanalysis.n.01'), Synset('information_science.n.01'), Synset('linguistics.n.01'), Synset('mathematics.n.01'), Synset('metallurgy.n.01'), Synset('metrology.n.01'), Synset('natural_history.n.01'), Synset('natural_science.n.01'), Synset('nutrition.n.03'), Synset('psychology.n.01'), Synset('social_science.n.01'), Synset('strategics.n.01'), Synset('systematics.n.01'), Synset('thanatology.n.01')]]], [], [], [[[Synset('allotment.n.01'), Synset('allowance.n.01'), Synset('cut.n.01'), Synset('dispensation.n.02'), Synset('dole.n.01'), Synset('interest.n.05'), Synset('profit_sharing.n.01'), Synset('ration.n.02'), Synset('slice.n.01'), Synset('split.n.03'), Synset('tranche.n.01'), Synset('way.n.12')]]], [[[Synset('cavalry.n.01'), Synset('friendly.n.01'), Synset('garrison.n.03'), Synset('hostile.n.01')]]], [[[Synset('artificial_insemination.n.01')]]], [[[Synset('mustiness.n.01')]]], [[[Synset('catch.v.18'), Synset('follow.v.23'), Synset('grok.v.01'), Synset('interpret.v.01'), Synset('make_out.v.03'), Synset('penetrate.v.02'), Synset('read.v.10'), Synset('sense.v.04'), Synset('solve.v.01'), Synset('touch.v.13'), Synset('understand.v.03')]]], [[[Synset('background.v.01'), Synset('bear_down.v.05'), Synset('bring_out.v.04'), Synset('drive_home.v.02'), Synset('point_up.v.01'), Synset('re-emphasise.v.01'), Synset('topicalize.v.01'), Synset('underscore.v.01')]]], [], [[[Synset('aspirant.n.01'), Synset('bidder.n.01'), Synset('claimant.n.01'), Synset('job_candidate.n.01'), Synset('material.n.05'), Synset('petitioner.n.01'), Synset('possible.n.02'), Synset('probable.n.01'), Synset('submitter.n.02')]]], [[[Synset('evacuate.v.05'), Synset('exude.v.01'), Synset('make.v.49'), Synset('stool.v.04'), Synset('sweat.v.01'), Synset('urinate.v.01'), Synset('vomit.v.01')]]], [[[Synset('chart.v.02'), Synset('frame.v.05'), Synset('intend.v.02'), Synset('lay_out.v.05'), Synset('mastermind.v.01'), Synset('plot.v.01'), Synset('project.v.08'), Synset('schedule.v.01'), Synset('schedule.v.02'), Synset('scheme.v.02')]]], [[[Synset('acquiring.n.01'), Synset('action.n.01'), Synset('activity.n.01'), Synset('assumption.n.07'), Synset('causing.n.01'), Synset('communication.n.01'), Synset('delivery.n.07'), Synset('departure.n.01'), Synset('derivation.n.08'), Synset('discovery.n.01'), Synset('disposal.n.03'), Synset('distribution.n.03'), Synset('egress.n.03'), Synset('equalization.n.01'), Synset('exhumation.n.01'), Synset('forfeit.n.03'), Synset('group_action.n.01'), Synset('hindrance.n.03'), Synset('hire.n.02'), Synset('implementation.n.02'), Synset('inactivity.n.03'), Synset('judgment.n.02'), Synset('leaning.n.04'), Synset('legitimation.n.01'), Synset('mitzvah.n.02'), Synset('motivation.n.03'), Synset('nonaccomplishment.n.01'), Synset('proclamation.n.02'), Synset('production.n.01'), Synset('propulsion.n.02'), Synset('recovery.n.03'), Synset('rejection.n.01'), Synset('residency.n.01'), Synset('running_away.n.01'), Synset('speech_act.n.01'), Synset('stay.n.01'), Synset('stop.n.02'), Synset('touch.n.05'), Synset('waste.n.05'), Synset('wear.n.03')]]], [], [[[Synset('couple.n.03'), Synset('crumb.n.01'), Synset('dab.n.02'), Synset('dollop.n.01'), Synset('dreg.n.01'), Synset('drop.n.02'), Synset('hair's-breadth.n.01'), Synset('handful.n.01'), Synset('helping.n.01'), Synset('jack.n.01'), Synset('little.n.01'), Synset('minimum.n.01'), Synset('modicum.n.01'), Synset('morsel.n.01'), Synset('nip.n.01'), Synset('nose.n.04'), Synset('scattering.n.01'), Synset('shoestring.n.02'), Synset('shred.n.01'), Synset('shtik.n.01'), Synset('spot.n.10'), Synset('spray.n.02'), Synset('step.n.06'), Synset('tad.n.01'), Synset('taste.n.05'), Synset('tot.n.01'), Synset('touch.n.06'), Synset('trace.n.01')]]], [[[Synset('analysis.n.02'), Synset('argumentation.n.02'), Synset('conjecture.n.03'), Synset('deduction.n.04'), Synset('inference.n.01'), Synset('prediction.n.01'), Synset('ratiocination.n.02'), Synset('regress.n.01'), Synset('synthesis.n.02')]]], [[[Synset('destine.v.01'), Synset('predestine.v.01'), Synset('will.v.01')]]], [[[Synset('actinoid.n.01'), Synset('arrangement.n.02'), Synset('association.n.08'), Synset('biological_group.n.01'), Synset('circuit.n.05'), Synset('citizenry.n.01'), Synset('collection.n.01'), Synset('community.n.06'), Synset('edition.n.02'), Synset('electron_shell.n.01'), Synset('ethnic_group.n.01'), Synset('halogen.n.01'), Synset('kingdom.n.06'), Synset('multitude.n.03'), Synset('people.n.01'), Synset('population.n.02'), Synset('race.n.03'), Synset('rare_earth.n.01'), Synset('sainthood.n.01'), Synset('series.n.06'), Synset('social_group.n.01'), Synset('straggle.n.01'), Synset('subgroup.n.01'), Synset('swarm.n.02'), Synset('system.n.02'), Synset('varna.n.02'), Synset('world.n.08')]]], [[[Synset('cast_about.v.01'), Synset('google.v.01'), Synset('mapquest.v.01'), Synset('prospect.v.02'), Synset('re-explore.v.01')]]], [[[Synset('auscultate.v.01'), Synset('autopsy.v.01'), Synset('candle.v.01'), Synset('check.v.02'), Synset('inspect.v.01'), Synset('peruse.v.01'), Synset('scan.v.01'), Synset('scan.v.02'), Synset('search.v.02'), Synset('search.v.04'), Synset('size_up.v.01'), Synset('survey.v.02'), Synset('x-ray.v.01')]]], [[[Synset('analyzer.n.01'), Synset('cautery.n.01'), Synset('drafting_instrument.n.01'), Synset('engine.n.04'), Synset('extractor.n.01'), Synset('instrument_of_execution.n.01'), Synset('instrument_of_punishment.n.01'), Synset('measuring_instrument.n.01'), Synset('medical_instrument.n.01'), Synset('navigational_instrument.n.01'), Synset('optical_instrument.n.01'), Synset('plotter.n.04'), Synset('scientific_instrument.n.01'), Synset('sonograph.n.01'), Synset('surveying_instrument.n.01'), Synset('tracer.n.02'), Synset('weapon.n.01'), Synset('whip.n.01')]]], [[[Synset('average.v.02'), Synset('begin.v.09'), Synset('come_to.v.03'), Synset('compass.v.01'), Synset('culminate.v.03'), Synset('reach.v.07'), Synset('score.v.06'), Synset('wangle.v.01')]]], [[[Synset('addressee.n.01'), Synset('alienee.n.01'), Synset('annuitant.n.01'), Synset('assignee.n.01'), Synset('beneficiary.n.01'), Synset('borrower.n.01'), Synset('conferee.n.01'), Synset('consignee.n.01'), Synset('dependant.n.01'), Synset('grantee.n.01'), Synset('heir.n.01'), Synset('honoree.n.01'), Synset('host.n.07'), Synset('mandatary.n.01'), Synset('payee.n.01'), Synset('protege.n.01'), Synset('sendee.n.01'), Synset('transferee.n.01'), Synset('warrantee.n.01')]]], [[[Synset('artificial_insemination.n.01')]]], [], [[[Synset('arouse.v.01'), Synset('assemble.v.01'), Synset('bear.v.05'), Synset('beat.v.18'), Synset('beget.v.01'), Synset('blast.v.05'), Synset('bring.v.03'), Synset('build.v.03'), Synset('cause.v.01'), Synset('chop.v.03'), Synset('choreograph.v.01'), Synset('clear.v.02'), Synset('cleave.v.02'), Synset('compose.v.02'), Synset('construct.v.01'), Synset('copy.v.04'), Synset('create.v.05'), Synset('create_by_mental_act.v.01'), Synset('create_from_raw_material.v.01'), Synset('create_verbally.v.01'), Synset('cut.v.06'), Synset('cut.v.22'), Synset('derive.v.04'), Synset('direct.v.03'), Synset('distill.v.03'), Synset('establish.v.05'), Synset('film-make.v.01'), Synset('film.v.02'), Synset('form.v.01'), Synset('froth.v.02'), Synset('generate.v.01'), Synset('give.v.09'), Synset('grind.v.06'), Synset('incorporate.v.03'), Synset('institute.v.02'), Synset('lay_down.v.01'), Synset('manufacture.v.04'), Synset('offset.v.04'), Synset('originate.v.02'), Synset('prepare.v.03'), Synset('press.v.07'), Synset('produce.v.01'), Synset('produce.v.03'), Synset('puncture.v.02'), Synset('put_on.v.04'), Synset('raise.v.07'), Synset('raise.v.11'), Synset('re-create.v.01'), Synset('realize.v.03'), Synset('recreate.v.04'), Synset('regenerate.v.07'), Synset('reproduce.v.02'), Synset('scrape.v.02'), Synset('short-circuit.v.02'), Synset('strike.v.13'), Synset('style.v.02'), Synset('track.v.05'), Synset('twine.v.03')]]], [], [[[Synset('abator.n.01'), Synset('abjurer.n.01'), Synset('abomination.n.01'), Synset('abstainer.n.02'), Synset('achiever.n.01'), Synset('acquaintance.n.03'), Synset('acquirer.n.01'), Synset('active.n.03'), Synset('actor.n.02'), Synset('adjudicator.n.01'), Synset('admirer.n.02'), Synset('adoptee.n.01'), Synset('adult.n.01'), Synset('adventurer.n.01'), Synset('adversary.n.01'), Synset('advisee.n.01'), Synset('advocate.n.01'), Synset('affiant.n.01'), Synset('african.n.01'), Synset('agnostic.n.01'), Synset('amateur.n.01'), Synset('amerindian.n.01'), Synset('ancient.n.02'), Synset('anomaly.n.02'), Synset('anti-american.n.01'), Synset('anti.n.01'), Synset('applicant.n.01'), Synset('appointee.n.02'), Synset('appreciator.n.01'), Synset('apprehender.n.02'), Synset('aquarius.n.01'), Synset('archaist.n.01'), Synset('aries.n.01'), Synset('arrogator.n.01'), Synset('assessee.n.01'), Synset('asthmatic.n.01'), Synset('authority.n.02'), Synset('autodidact.n.01'), Synset('baby_boomer.n.01'), Synset('baby_buster.n.01'), Synset('bad_guy.n.01'), Synset('bad_person.n.01'), Synset('baldhead.n.01'), Synset('balker.n.01'), Synset('bather.n.02'), Synset('beard.n.03'), Synset('bedfellow.n.02'), Synset('bereaved.n.01'), Synset('best.n.02'), Synset('birth.n.05'), Synset('biter.n.01'), Synset('black.n.05'), Synset('blogger.n.01'), Synset('blond.n.01'), Synset('bluecoat.n.01'), Synset('bodybuilder.n.01'), Synset('bomber.n.02'), Synset('brunet.n.01'), Synset('bullfighter.n.01'), Synset('buster.n.04'), Synset('cancer.n.02'), Synset('candidate.n.02'), Synset('capitalist.n.02'), Synset('capricorn.n.01'), Synset('captor.n.01'), Synset('case.n.06'), Synset('cashier.n.02'), Synset('celebrant.n.01'), Synset('censor.n.01'), Synset('chameleon.n.01'), Synset('changer.n.01'), Synset('charmer.n.02'), Synset('child.n.03'), Synset('chutzpanik.n.01'), Synset('closer.n.01'), Synset('clumsy_person.n.01'), Synset('collector.n.01'), Synset('color-blind_person.n.01'), Synset('combatant.n.01'), Synset('commoner.n.01'), Synset('communicator.n.01'), Synset('complexifier.n.01'), Synset('compulsive.n.01'), Synset('computer_user.n.01'), Synset('contemplative.n.01'), Synset('contestant.n.01'), Synset('convert.n.01'), Synset('copycat.n.01'), Synset('counter.n.05'), Synset('counterterrorist.n.01'), Synset('coward.n.01'), Synset('crawler.n.02'), Synset('creator.n.02'), Synset('creature.n.02'), Synset('creditor.n.01'), Synset('cripple.n.01'), Synset('dancer.n.02'), Synset('dead_person.n.01'), Synset('deaf_person.n.01'), Synset('debaser.n.01'), Synset('debtor.n.01'), Synset('defecator.n.01'), Synset('delayer.n.01'), Synset('deliverer.n.04'), Synset('demander.n.01'), Synset('dieter.n.01'), Synset('differentiator.n.01'), Synset('disentangler.n.01'), Synset('disputant.n.01'), Synset('dissenter.n.01'), Synset('divider.n.02'), Synset('domestic_partner.n.01'), Synset('double.n.03'), Synset('dresser.n.02'), Synset('dribbler.n.02'), Synset('drug_user.n.01'), Synset('dyslectic.n.01'), Synset('ectomorph.n.01'), Synset('effecter.n.01'), Synset('elizabethan.n.01'), Synset('emotional_person.n.01'), Synset('endomorph.n.01'), Synset('engineer.n.01'), Synset('enjoyer.n.01'), Synset('enrollee.n.01'), Synset('entertainer.n.01'), Synset('ethnic.n.01'), Synset('experimenter.n.02'), Synset('expert.n.01'), Synset('explorer.n.01'), Synset('extrovert.n.01'), Synset('face.n.05'), Synset('faddist.n.01'), Synset('faller.n.02'), Synset('fastener.n.01'), Synset('female.n.02'), Synset('fiduciary.n.01'), Synset('first-rater.n.01'), Synset('follower.n.01'), Synset('free_agent.n.02'), Synset('friend.n.01'), Synset('fugitive.n.01'), Synset('gainer.n.01'), Synset('gainer.n.02'), Synset('gambler.n.01'), Synset('gatekeeper.n.01'), Synset('gatherer.n.01'), Synset('gemini.n.01'), Synset('gentile.n.02'), Synset('good_guy.n.01'), Synset('good_person.n.01'), Synset('granter.n.01'), Synset('greeter.n.01'), Synset('grinner.n.01'), Synset('groaner.n.01'), Synset('grunter.n.01'), Synset('guesser.n.01'), Synset('handicapped_person.n.01'), Synset('hater.n.01'), Synset('heterosexual.n.01'), Synset('homosexual.n.01'), Synset('homunculus.n.02'), Synset('hope.n.04'), Synset('hoper.n.01'), Synset('huddler.n.02'), Synset('hugger.n.01'), Synset('immune.n.01'), Synset('individualist.n.01'), Synset('inhabitant.n.01'), Synset('innocent.n.01'), Synset('insured.n.01'), Synset('intellectual.n.01'), Synset('interpreter.n.02'), Synset('introvert.n.01'), Synset('jat.n.01'), Synset('jew.n.01'), Synset('jewel.n.02'), Synset('jumper.n.01'), Synset('junior.n.03'), Synset('juvenile.n.01'), Synset('killer.n.01'), Synset('kink.n.03'), Synset('kneeler.n.01'), Synset('knocker.n.02'), Synset('knower.n.01'), Synset('large_person.n.01'), Synset('latin.n.03'), Synset('laugher.n.01'), Synset('leader.n.01'), Synset('learner.n.01'), Synset('left-hander.n.02'), Synset('leo.n.01'), Synset('libra.n.01'), Synset('life.n.08'), Synset('lightning_rod.n.01'), Synset('linguist.n.02'), Synset('literate.n.01'), Synset('liver.n.03'), Synset('longer.n.01'), Synset('loose_cannon.n.01'), Synset('loved_one.n.01'), Synset('lover.n.01'), Synset('machine.n.02'), Synset('mailer.n.02'), Synset('malcontent.n.01'), Synset('male.n.02'), Synset('man.n.03'), Synset('man_jack.n.01'), Synset('manipulator.n.02'), Synset('married.n.01'), Synset('masturbator.n.01'), Synset('measurer.n.01'), Synset('mesomorph.n.01'), Synset('mestizo.n.01'), Synset('middlebrow.n.01'), Synset('miracle_man.n.01'), Synset('misogamist.n.01'), Synset('mixed-blood.n.01'), Synset('modern.n.01'), Synset('money_handler.n.01'), Synset('monolingual.n.01'), Synset('mother_hen.n.01'), Synset('mouse.n.03'), Synset('mutilator.n.01'), Synset('namer.n.01'), Synset('namesake.n.01'), Synset('national.n.01'), Synset('native.n.01'), Synset('native.n.02'), Synset('neglecter.n.01'), Synset('neighbor.n.01'), Synset('neutral.n.01'), Synset('nondescript.n.01'), Synset('nonmember.n.01'), Synset('nonparticipant.n.01'), Synset('nonpartisan.n.01'), Synset('nonperson.n.01'), Synset('nonreligious_person.n.01'), Synset('nonresident.n.01'), Synset('nonsmoker.n.01'), Synset('nonworker.n.01'), Synset('nude.n.03'), Synset('nurser.n.01'), Synset('occultist.n.01'), Synset('optimist.n.01'), Synset('orphan.n.02'), Synset('ostrich.n.01'), Synset('ouster.n.01'), Synset('outcaste.n.01'), Synset('outdoorsman.n.01'), Synset('owner.n.02'), Synset('pamperer.n.01'), Synset('pansexual.n.01'), Synset('pardoner.n.01'), Synset('partner.n.03'), Synset('party.n.05'), Synset('passer.n.02'), Synset('peer.n.01'), Synset('perceiver.n.01'), Synset('percher.n.01'), Synset('person_of_color.n.01'), Synset('personage.n.01'), Synset('personification.n.01'), Synset('perspirer.n.01'), Synset('philosopher.n.02'), Synset('picker.n.01'), Synset('pisces.n.02'), Synset('pisser.n.01'), Synset('planner.n.01'), Synset('player.n.04'), Synset('posturer.n.01'), Synset('powderer.n.01'), Synset('precursor.n.02'), Synset('preserver.n.03'), Synset('primitive.n.01'), Synset('propositus.n.01'), Synset('public_relations_person.n.01'), Synset('pursuer.n.02'), Synset('pussycat.n.01'), Synset('quarter.n.11'), Synset('quitter.n.01'), Synset('radical.n.03'), Synset('realist.n.02'), Synset('rectifier.n.02'), Synset('redhead.n.01'), Synset('registrant.n.01'), Synset('relative.n.01'), Synset('reliever.n.02'), Synset('religious_person.n.01'), Synset('repeater.n.01'), Synset('rescuer.n.02'), Synset('rester.n.01'), Synset('restrainer.n.02'), Synset('revenant.n.01'), Synset('rich_person.n.01'), Synset('right-hander.n.02'), Synset('riser.n.01'), Synset('romper.n.01'), Synset('roundhead.n.01'), Synset('ruler.n.01'), Synset('rusher.n.03'), Synset('sagittarius.n.01'), Synset('scientist.n.01'), Synset('scorpio.n.01'), Synset('scratcher.n.02'), Synset('second-rater.n.01'), Synset('seeder.n.01'), Synset('seeker.n.01'), Synset('segregate.n.01'), Synset('self.n.02'), Synset('sensualist.n.01'), Synset('sentimentalist.n.01'), Synset('sex_object.n.01'), Synset('sex_symbol.n.01'), Synset('shaker.n.01'), Synset('showman.n.01'), Synset('signer.n.02'), Synset('simpleton.n.01'), Synset('six-footer.n.01'), Synset('skidder.n.01'), Synset('slav.n.01'), Synset('slave.n.01'), Synset('slave.n.03'), Synset('sleepyhead.n.01'), Synset('sloucher.n.01'), Synset('small_person.n.01'), Synset('smasher.n.01'), Synset('smiler.n.01'), Synset('sneezer.n.01'), Synset('sniffer.n.01'), Synset('sniffler.n.01'), Synset('snuffer.n.02'), Synset('snuffler.n.01'), Synset('socializer.n.01'), Synset('sort.n.03'), Synset('sounding_board.n.01'), Synset('sphinx.n.01'), Synset('spitter.n.01'), Synset('sport.n.04'), Synset('sprawler.n.01'), Synset('spurner.n.01'), Synset('squinter.n.01'), Synset('stifler.n.01'), Synset('stigmatic.n.01'), Synset('stooper.n.02'), Synset('stranger.n.02'), Synset('struggler.n.01'), Synset('subject.n.06'), Synset('supernumerary.n.01'), Synset('surrenderer.n.01'), Synset('survivalist.n.01'), Synset('survivor.n.02'), Synset('suspect.n.01'), Synset('tagger.n.01'), Synset('tagger.n.02'), Synset('tapper.n.02'), Synset('taurus.n.02'), Synset('tempter.n.01'), Synset('termer.n.01'), Synset('terror.n.02'), Synset('testator.n.01'), Synset('thin_person.n.01'), Synset('third-rater.n.01'), Synset('thrower.n.02'), Synset('tiger.n.01'), Synset('totemist.n.01'), Synset('toucher.n.01'), Synset('transfer.n.02'), Synset('transsexual.n.02'), Synset('transvestite.n.01'), Synset('traveler.n.01'), Synset('trier.n.02'), Synset('turner.n.07'), Synset('tyrant.n.03'), Synset('undoer.n.02'), Synset('unfortunate.n.01'), Synset('unskilled_person.n.01'), Synset('unwelcome_person.n.01'), Synset('user.n.01'), Synset('vanisher.n.01'), Synset('victim.n.02'), Synset('victorian.n.01'), Synset('virgo.n.01'), Synset('visionary.n.01'), Synset('visually_impaired_person.n.01'), Synset('waiter.n.02'), Synset('waker.n.02'), Synset('walk-in.n.01'), Synset('wanter.n.01'), Synset('ward.n.01'), Synset('warrior.n.01'), Synset('watcher.n.03'), Synset('weakling.n.01'), Synset('weasel.n.01'), Synset('white.n.01'), Synset('wiggler.n.01'), Synset('winker.n.01'), Synset('withholder.n.01'), Synset('witness.n.05'), Synset('worker.n.01'), Synset('worldling.n.01'), Synset('yawner.n.01')]]], [[[Synset('addict.v.01'), Synset('inure.v.01'), Synset('teach.v.02')]]], [[[Synset('artificial_insemination.n.01')]]], [[[Synset('mirror.v.02')]]], [[[Synset('australopithecine.n.01'), Synset('dryopithecine.n.01'), Synset('homo.n.02'), Synset('javanthropus.n.01'), Synset('pithecanthropus.n.01'), Synset('sinanthropus.n.01'), Synset('sivapithecus.n.01')]]], [[[Synset('acquaint.v.03'), Synset('advise.v.02'), Synset('announce.v.01'), Synset('denounce.v.04'), Synset('disabuse.v.01'), Synset('explain.v.01'), Synset('familiarize.v.01'), Synset('fill_in.v.01'), Synset('indicate.v.02'), Synset('indicate.v.03'), Synset('indicate.v.05'), Synset('inform.v.03'), Synset('inoculate.v.01'), Synset('instruct.v.03'), Synset('introduce.v.01'), Synset('misinform.v.01'), Synset('nark.v.02'), Synset('narrate.v.01'), Synset('prompt.v.03'), Synset('regret.v.04'), Synset('remonstrate.v.02'), Synset('report.v.01'), Synset('report.v.04'), Synset('report.v.05'), Synset('teach.v.01'), Synset('tell.v.02'), Synset('tell.v.03'), Synset('testify.v.02'), Synset('undeceive.v.01'), Synset('understate.v.01'), Synset('update.v.02'), Synset('volunteer.v.01'), Synset('warn.v.01'), Synset('warn.v.04'), Synset('wise_up.v.02')]]], [], [[[Synset('calibrate.v.03'), Synset('caliper.v.01'), Synset('shoot.v.18'), Synset('triangulate.v.02')]]], [[[Synset('barrels.n.01'), Synset('batch.n.02'), Synset('battalion.n.02'), Synset('billyo.n.01'), Synset('boatload.n.01'), Synset('busload.n.01'), Synset('chunk.n.02'), Synset('infinitude.n.01'), Synset('maximum.n.01'), Synset('mile.n.03'), Synset('million.n.02'), Synset('much.n.01'), Synset('myriad.n.01'), Synset('ocean.n.02'), Synset('ream.n.01'), Synset('small_fortune.n.01'), Synset('tons.n.01')]]], [[[Synset('advance.v.02'), Synset('feed_back.v.02'), Synset('move.v.16'), Synset('proposition.v.01'), Synset('recommend.v.01'), Synset('submit.v.02')]]], [], [[[Synset('count.v.08'), Synset('credit.v.04'), Synset('lean.v.04')]]]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'path_similarity'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-f122054530aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistOfSentences_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mresult\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mSynset_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlistOfSentences_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlistOfSentences_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-f122054530aa>\u001b[0m in \u001b[0;36mSynset_similarity\u001b[1;34m(synsets1, synsets2)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mss1\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynsets1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mss2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynsets2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m             \u001b[0mbest1_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mss1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mss2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbest1_score\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mbest_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest1_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'path_similarity'"
     ]
    }
   ],
   "source": [
    "#Expandword_wordNet similarity??\n",
    "def Synset_similarity(synsets1, synsets2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    best_score = [0.0]\n",
    "    for ss1 in synsets1:\n",
    "        for ss2 in synsets2:\n",
    "            best1_score=ss1.path_similarity(ss2)\n",
    "        if best1_score is not None:\n",
    "            best_score.append(best1_score)\n",
    "        max1=max(best_score)\n",
    "        if best_score is not None:\n",
    "            score += max1\n",
    "        if max1 is not 0.0:\n",
    "            count += 1\n",
    "        best_score=[0.0]\n",
    "    print(score/count)      \n",
    "   \n",
    "    # Average the values\n",
    "    score /= count\n",
    "    return score\n",
    "\n",
    "#print(listOfSentences_1)\n",
    "result=Synset_similarity(listOfSentences_1,listOfSentences_2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Similarly, use Wikipedia dump files in order to design a program that search the Wikipedia documents for each Sentence. The similarity between the sentences is therefore measured as the number of common Wikipedia documents output by the queries (S1 and S2) over the total number of documents output by the two queries. Repeat the process of calculating the semantic similarity for your set of chosen academic examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the model is working. Only works in my computer because of size issues. May need you check the method of calculation correct or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-05 15:40:23,747 : INFO : loading LsiModel object from ./wikiresults/lsi.lsi_model\n",
      "2019-01-05 15:40:23,819 : INFO : loading id2word recursively from ./wikiresults/lsi.lsi_model.id2word.* with mmap=None\n",
      "2019-01-05 15:40:23,819 : INFO : setting ignored attribute projection to None\n",
      "2019-01-05 15:40:23,819 : INFO : setting ignored attribute dispatcher to None\n",
      "2019-01-05 15:40:23,819 : INFO : loaded ./wikiresults/lsi.lsi_model\n",
      "2019-01-05 15:40:23,823 : INFO : loading LsiModel object from ./wikiresults/lsi.lsi_model.projection\n",
      "2019-01-05 15:40:23,823 : INFO : loading u from ./wikiresults/lsi.lsi_model.projection.u.npy with mmap=None\n",
      "2019-01-05 15:40:24,088 : INFO : loaded ./wikiresults/lsi.lsi_model.projection\n",
      "2019-01-05 15:40:24,125 : INFO : loading MatrixSimilarity object from ./wikiresults/lsi_index.index\n",
      "2019-01-05 15:40:24,125 : INFO : loading index from ./wikiresults/lsi_index.index.index.npy with mmap=None\n",
      "2019-01-05 15:40:36,706 : INFO : loaded ./wikiresults/lsi_index.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikiSim= (0.5053311031324974, 0.49466889686750254)\n"
     ]
    }
   ],
   "source": [
    "# Wikipedia based similarity\n",
    "\"\"\" Compute the similarty between defined two sentences using Wikipedia dump files \"\"\"\n",
    "import gensim\n",
    "#from gensim.corpora import WikiCorpus, MmCorpus\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "id2word = gensim.corpora.Dictionary.load_from_text ('./wikiresults/results_wordids.txt.bz2')\n",
    "#mm = gensim.corpora.MmCorpus('./wikiresults/results_tfidf.mm')-22.4GB\n",
    "#print(mm)\n",
    "\n",
    "# load LSI model\n",
    "\n",
    "model_lsi = LsiModel.load('./wikiresults/lsi.lsi_model')\n",
    "\n",
    "# load index of wikicorpus documents\n",
    "\n",
    "index = similarities.MatrixSimilarity.load('./wikiresults/lsi_index.index')\n",
    "\n",
    "# Wiki_similarity\n",
    "def Wiki_similarity(sentence1,sentence2):\n",
    "    \"\"\" compute wiki based similarity using Wikipedia dump files \"\"\"\n",
    "    def cosine_similarity(sentence):\n",
    "        vec_bow = id2word.doc2bow(gensim.utils.simple_preprocess(sentence)) #or item.lower().split()\n",
    "        vec_lsi = model_lsi[vec_bow]\n",
    "        sims = index[vec_lsi]\n",
    "        return sims\n",
    "    # only search the similarity value >0\n",
    "    s_1 = list([index for index in cosine_similarity(sentence1) if index > 0])\n",
    "    #print(s_1)\n",
    "    s_2 = list([index for index in cosine_similarity(sentence2) if index > 0])\n",
    "    #print(s_2)\n",
    "\n",
    "# calculate the similary based document numbers\n",
    "    DS1 = len(s_1)/(len(s_1)+len(s_2))\n",
    "    DS2 = len(s_2)/(len(s_1)+len(s_2))\n",
    "    return DS1, DS2\n",
    "\n",
    "# test\n",
    "sentence1 = 'AI is our friend and it has been friendly.'\n",
    "sentence2= 'AI and humans have always been friendly.'\n",
    "\n",
    "Simresult = Wiki_similarity(sentence1,sentence2)\n",
    "print('wikiSim=',Simresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use a publicly available database of your choice in order to test the usefulness of this similarity measure (Snippets and Wikipedia based similarity) and compare the results with some state of art measures mentioned in the literature employing your chosen publicly database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: not sure what should i do about this task......???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "Videos:\n",
      " GOT7 \"A\" M/V\n",
      "Peek a Boo Song | +More Nursery Rhymes & Kids Songs - Cocomelon (ABCkidTV)\n",
      "A Lovely Day | Episode Compilation | Mr Bean Official Cartoon\n",
      "Luan Santana | \"A\" (Video Oficial) - Live-Móvel\n",
      "Faith Evans feat. Stevie J – \"A Minute\" [Official Music Video]\n",
      "I WANT A BABY NOW PRANK ON GIRLFRIEND! (CUTE COUPLE GOALS)\n",
      "Coldplay - Adventure Of A Lifetime (Official Video)\n",
      "The Gummy Bear Song - Long English Version\n",
      "Phonics Song with TWO Words - A For Apple - ABC Alphabet Songs with Sounds for Children \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " Kendrick Lamar - i (Official Video)\n",
      "TAEYEON 태연 'I (feat. Verbal Jint)' MV\n",
      "Kanye West & Lil Pump ft. Adele Givens - \"I Love It\" (Official Music Video)\n",
      "I Hindi Movie 2015\n",
      "Auli'i Cravalho - How Far I'll Go\n",
      "Cardi B, Bad Bunny & J Balvin - I Like It [Official Music Video]\n",
      "The Gummy Bear Song - Long English Version\n",
      "I Tamil Full Movie (2015) - Vikram, Amy Jackson - Shankar's I (Ai)\n",
      "Bruno Mars - That’s What I Like [Official Video]\n",
      "Pitbull - I Know You Want Me (Calle Ocho) OFFICIAL VIDEO (Ultra Music) \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " K/DA - POP/STARS (ft Madison Beer, (G)I-DLE, Jaira Burns) | Official Music Video - League of Legends\n",
      "Marshmello ft. Bastille - Happier (Official Music Video)\n",
      "Marvel Studios' Avengers - Official Trailer\n",
      "Real Life Trick Shots | Dude Perfect\n",
      "Real Life Trick Shots 2 | Dude Perfect\n",
      "Steve Aoki - Waste It On Me feat. BTS (Lyric Video) [Ultra Music]\n",
      "Annoying Customers\n",
      "Lauv - I Like Me Better [Official Audio]\n",
      "Ryan Reynolds & Jake Gyllenhaal Answer the Web's Most Searched Questions | WIRED\n",
      "Ice Cube, Kevin Hart, And Conan Share A Lyft Car \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " GOT7 \"A\" M/V\n",
      "Peek a Boo Song | +More Nursery Rhymes & Kids Songs - Cocomelon (ABCkidTV)\n",
      "A Lovely Day | Episode Compilation | Mr Bean Official Cartoon\n",
      "Luan Santana | \"A\" (Video Oficial) - Live-Móvel\n",
      "Faith Evans feat. Stevie J – \"A Minute\" [Official Music Video]\n",
      "I WANT A BABY NOW PRANK ON GIRLFRIEND! (CUTE COUPLE GOALS)\n",
      "Coldplay - Adventure Of A Lifetime (Official Video)\n",
      "The Gummy Bear Song - Long English Version\n",
      "Phonics Song with TWO Words - A For Apple - ABC Alphabet Songs with Sounds for Children \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " Daniel Wellington Launch | MIHLALI N\n",
      "[N’-76] TEN’s ‘Taki Taki’ & ‘HUMBLE.’ Choreography Behind\n",
      "NTV - Canlı Yayın ᴴᴰ\n",
      "【DEEMO(switch版)＋雑談】音楽を聴いてまったり雑談(n*´ω`*n)【アイドル部】\n",
      "[모트라인] N의 감성을 입혔다! 현대 i30 N라인 리뷰 1부 (일반 도로 주행)\n",
      "Migos vs. Bone thugs n harmony getting out of control full tea\n",
      "HBB SYAIKHON (WAN SECHAN) HABIB MAJDZUB  N MAJELIS AL-BAHAR DUKUNG PRABOWO-SANDI;PILPRES 2019;JOKOWI\n",
      "'S' और 'N' की जोड़ी किसके साथ बनती है 😍| 'S' AND 'N' People Match With Whom\n",
      "\"TROUVER LA LETTRE N SOUS UN LAC GELÉ\" DEFI SEMAINE 4 sur FORTNITE BATTLE ROYALE ! \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " Ludacris - Vitamin D (feat. Ty Dolla $ign) [Official Video]\n",
      "Jaś Fasola :D\n",
      "franceinfo - DIRECT TV - actualité france et monde, interviews, documentaires et analyses\n",
      "12 Façons Risquées de Gagner Beaucoup D'argent\n",
      "CGI Animated Short : \"The D in David\" - by Michelle Yi & Yaron Farkash\n",
      "Ludacris - Vitamin D ft. Ty Dolla Sign\n",
      "My Super D: The Last Fight\n",
      "Letter D Song (Classic)\n",
      "I GAVE SOMMER RAY THE D - IMPAULSIVE EP. 3 \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " K/DA - POP/STARS (ft Madison Beer, (G)I-DLE, Jaira Burns) | Official Music Video - League of Legends\n",
      "Marshmello ft. Bastille - Happier (Official Music Video)\n",
      "Marvel Studios' Avengers - Official Trailer\n",
      "Real Life Trick Shots | Dude Perfect\n",
      "Real Life Trick Shots 2 | Dude Perfect\n",
      "Steve Aoki - Waste It On Me feat. BTS (Lyric Video) [Ultra Music]\n",
      "Annoying Customers\n",
      "Lauv - I Like Me Better [Official Audio]\n",
      "Ryan Reynolds & Jake Gyllenhaal Answer the Web's Most Searched Questions | WIRED\n",
      "Ice Cube, Kevin Hart, And Conan Share A Lyft Car \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " H Saison 2 - Ep. 01 - Une Histoire D'intelligence\n",
      "Syömiskisa Bull Mentula vs H  Kanerva vs M  Putkonen\n",
      "Messi Vs Elche (H) Liga 2014/15 - English Commentary HD 720p\n",
      "#Learncolors WithKinetic Sand Street Vehicles W MCqueen Cars #h - #NurseryRhymes Song Fun Toys\n",
      "h\n",
      "Tool - H.\n",
      "Sau khi h.ỏ.a th.i.ê.u xong dầu mỡ từ người quá cố mang đi đâu? Câu trả lời khiến tỉ người khiếp sợ \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " H.E.R. - U\n",
      "Luv U: Liza meets Luv U students\n",
      "Ariana Grande - thank u, next\n",
      "Kendrick Lamar - God Is Gangsta\n",
      "POSLE RUCKA - Kriticna godina za Kosovo - Da li ce biti sukoba u 2019. - TV Happy 26.12.2018\n",
      "Gaab part. Rodriguinho e Thomaz - U (Áudio Oficial)\n",
      "Letter U Song (Classic)\n",
      "Ranisha - Loving U & And I Am Telling U  | Minggu 8 | #Mentor7\n",
      "Gareth Emery feat. Bo Bruce - U (Official Video) \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " PSY - GENTLEMAN M/V (MattyBRaps Cover)\n",
      "The Gummy Bear Song - Long English Version\n",
      "TWICE \"LIKEY\" M/V\n",
      "DJ Khaled - I'm The One ft. Justin Bieber, Quavo, Chance the Rapper, Lil Wayne\n",
      "Honey, I'm Home .. Household Hacks | Try These DIY Life Hacks & DIY Projects by Blossom\n",
      "M - PRINCESS PRINCESS（フル）\n",
      "THE MUFFIN SONG (asdfmovie feat. Schmoyoho)\n",
      "PSY - GANGNAM STYLE(강남스타일) M/V\n",
      "Je m'appelle Funny Bear - Full French Version - Gummy Bear Song \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " GOT7 \"A\" M/V\n",
      "Peek a Boo Song | +More Nursery Rhymes & Kids Songs - Cocomelon (ABCkidTV)\n",
      "A Lovely Day | Episode Compilation | Mr Bean Official Cartoon\n",
      "Luan Santana | \"A\" (Video Oficial) - Live-Móvel\n",
      "Faith Evans feat. Stevie J – \"A Minute\" [Official Music Video]\n",
      "I WANT A BABY NOW PRANK ON GIRLFRIEND! (CUTE COUPLE GOALS)\n",
      "Coldplay - Adventure Of A Lifetime (Official Video)\n",
      "The Gummy Bear Song - Long English Version\n",
      "Phonics Song with TWO Words - A For Apple - ABC Alphabet Songs with Sounds for Children \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " QUAND TU N'AS PAS DE CERVEAU ! #11 😂\n",
      "Zap Koreus n°307\n",
      "NTV - Canlı Yayın ᴴᴰ\n",
      "[N’-76] TEN’s ‘Taki Taki’ & ‘HUMBLE.’ Choreography Behind\n",
      "JE N'EN PEUX PLUS !\n",
      "【DEEMO(switch版)＋雑談】音楽を聴いてまったり雑談(n*´ω`*n)【アイドル部】\n",
      "\"Il n'y a aucune possibilité pour Macky Sall & ses magistrats d’invalider la candidature de Khalifa\"\n",
      "'S' और 'N' की जोड़ी किसके साथ बनती है 😍| 'S' AND 'N' People Match With Whom\n",
      "Daniel Wellington Launch | MIHLALI N \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " M & S 2018年12月29日直播\n",
      "Inge sa chce vyspať s Michalom (PANELÁK)\n",
      "Wegen S*x im Auto Rücken gebrochen? | 112 - Rettung in letzter Minute | SAT.1 TV\n",
      "Los Angeles Fashion Week S/S 19  - Art Hearts Fashion - The Black Tape Project | FashionTV | FTV\n",
      "Search the letter 'S' in Wailing Woods – LOCATION WEEK 4 CHALLENGE Fortnite Season 7\n",
      "OTECKOVIA - Lucia s Vladom sa už nedokážu ovládať\n",
      "Ceremony held to reconnect N and S Korea - BBC News\n",
      "BAFASHWE NIMBWA BARIGUKORA IBIDAKORWA MU BIHURU\n",
      "Blend S『ブレンド・S』OP / Opening Full - Bon Appétit♡S (ぼなぺてぃーと♡S)\n",
      "DIE ANTWOORD - BABY'S ON FIRE (OFFICIAL) \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " K/DA - POP/STARS (ft Madison Beer, (G)I-DLE, Jaira Burns) | Official Music Video - League of Legends\n",
      "Marshmello ft. Bastille - Happier (Official Music Video)\n",
      "Marvel Studios' Avengers - Official Trailer\n",
      "Real Life Trick Shots | Dude Perfect\n",
      "Real Life Trick Shots 2 | Dude Perfect\n",
      "Steve Aoki - Waste It On Me feat. BTS (Lyric Video) [Ultra Music]\n",
      "Annoying Customers\n",
      "Lauv - I Like Me Better [Official Audio]\n",
      "Ryan Reynolds & Jake Gyllenhaal Answer the Web's Most Searched Questions | WIRED\n",
      "Ice Cube, Kevin Hart, And Conan Share A Lyft Car \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " H Saison 2 - Ep. 01 - Une Histoire D'intelligence\n",
      "Syömiskisa Bull Mentula vs H  Kanerva vs M  Putkonen\n",
      "Messi Vs Elche (H) Liga 2014/15 - English Commentary HD 720p\n",
      "#Learncolors WithKinetic Sand Street Vehicles W MCqueen Cars #h - #NurseryRhymes Song Fun Toys\n",
      "h\n",
      "Tool - H.\n",
      "Sau khi h.ỏ.a th.i.ê.u xong dầu mỡ từ người quá cố mang đi đâu? Câu trả lời khiến tỉ người khiếp sợ \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " GOT7 \"A\" M/V\n",
      "Peek a Boo Song | +More Nursery Rhymes & Kids Songs - Cocomelon (ABCkidTV)\n",
      "A Lovely Day | Episode Compilation | Mr Bean Official Cartoon\n",
      "Faith Evans feat. Stevie J – \"A Minute\" [Official Music Video]\n",
      "I WANT A BABY NOW PRANK ON GIRLFRIEND! (CUTE COUPLE GOALS)\n",
      "The Gummy Bear Song - Long English Version\n",
      "Luan Santana | \"A\" (Video Oficial) - Live-Móvel\n",
      "Phonics Song with TWO Words - A For Apple - ABC Alphabet Songs with Sounds for Children\n",
      "Letter A Song (Classic) \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " 박재범 Jay Park - 'V' (Official MV) [ENG, CHN]\n",
      "Россия 24. Последние новости России и мира\n",
      "Sky News - Live\n",
      "BTS (방탄소년단) LOVE YOURSELF 轉 Tear 'Singularity' Comeback Trailer\n",
      "[YTN LIVE] 대한민국 24시간 뉴스채널 YTN\n",
      "V for Vendetta: The Revolutionary Speech (HD)\n",
      "ABC \"V\" TV SERIES PILOT PROMO TRAILER\n",
      "PSY - GANGNAM STYLE(강남스타일) M/V\n",
      "Маша и Медведь (Masha and The Bear) - Маша плюс каша (17 Серия) \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "Videos:\n",
      " RUSH E\n",
      "''E'' MEME COMPILATION\n",
      "Lord Farquaad 'E' Memes Explained\n",
      "[2018 LoL KeSPA Cup] 2Round Ro8 Group C, D (AF-GRF, DWG-SKT)\n",
      "e\n",
      "Matt Mason - \"E\" - (OFFICIAL Lyric video)\n",
      "E\n",
      "E \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " K/DA - POP/STARS (ft Madison Beer, (G)I-DLE, Jaira Burns) | Official Music Video - League of Legends\n",
      "Marshmello ft. Bastille - Happier (Official Music Video)\n",
      "Marvel Studios' Avengers - Official Trailer\n",
      "Real Life Trick Shots | Dude Perfect\n",
      "Real Life Trick Shots 2 | Dude Perfect\n",
      "Steve Aoki - Waste It On Me feat. BTS (Lyric Video) [Ultra Music]\n",
      "Annoying Customers\n",
      "Lauv - I Like Me Better [Official Audio]\n",
      "Ryan Reynolds & Jake Gyllenhaal Answer the Web's Most Searched Questions | WIRED\n",
      "Ice Cube, Kevin Hart, And Conan Share A Lyft Car \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " GOT7 \"A\" M/V\n",
      "Peek a Boo Song | +More Nursery Rhymes & Kids Songs - Cocomelon (ABCkidTV)\n",
      "A Lovely Day | Episode Compilation | Mr Bean Official Cartoon\n",
      "Luan Santana | \"A\" (Video Oficial) - Live-Móvel\n",
      "Faith Evans feat. Stevie J – \"A Minute\" [Official Music Video]\n",
      "I WANT A BABY NOW PRANK ON GIRLFRIEND! (CUTE COUPLE GOALS)\n",
      "Coldplay - Adventure Of A Lifetime (Official Video)\n",
      "The Gummy Bear Song - Long English Version\n",
      "Phonics Song with TWO Words - A For Apple - ABC Alphabet Songs with Sounds for Children \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " ANH THÍCH KIỂU NÀO? | TẬP FULL Phần 2 l LOVE GAME | THỬ THÁCH HẸN HÒ\n",
      "FRANCE 24 en Direct – Info et actualités internationales en continu 24h/24\n",
      "ABC Song l How To Make Rainbow Color Foam Jiggly Clay Slime l kid songs\n",
      "KID Song l How To Make Colors Spoon Ice Jelly Slime DIY Clay Play l kid songs \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " Colours for Kids to Learn with Farm Vehicles #w | Colors Magic Liquids & Water Pipe for Children\n",
      "Sylwester w Kabaretowym Klubie Dwójki (KMN, Limo, Kabaret Młodych Panów)\n",
      "Tana Mongeau - W (Official Music Video)\n",
      "Emma Pretend Play w BIG Rapunzel Doll & Kids Make Up Hair Salon Toys\n",
      "Emma & Jannie Pretend Play w/ Squishy Hamburger Fast Food Drive Thru\n",
      "SENDING SELFIES TO STRANGERS W/ CORINNA KOPF!!\n",
      "Learn Colors Learn Eggs Baby Boong Animals W Cartoon Baby Nursery Rhymes For Kids\n",
      "[W] Kiss Compilation♥(Reviewing 'W' at once)\n",
      "Are Frozen Elsa & Spiderman BREAKING UP? w/ Maleficent Joker Hulk Spidergirl Anna Toys Superhero IRL \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " GOT7 \"A\" M/V\n",
      "Peek a Boo Song | +More Nursery Rhymes & Kids Songs - Cocomelon (ABCkidTV)\n",
      "A Lovely Day | Episode Compilation | Mr Bean Official Cartoon\n",
      "Luan Santana | \"A\" (Video Oficial) - Live-Móvel\n",
      "Faith Evans feat. Stevie J – \"A Minute\" [Official Music Video]\n",
      "I WANT A BABY NOW PRANK ON GIRLFRIEND! (CUTE COUPLE GOALS)\n",
      "Coldplay - Adventure Of A Lifetime (Official Video)\n",
      "The Gummy Bear Song - Long English Version\n",
      "Phonics Song with TWO Words - A For Apple - ABC Alphabet Songs with Sounds for Children \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " Lele Pons y Anwar Jibawi | Recopilacion de Vines\n",
      "Tom y Jerry en Español | Compilación clásica de dibujos animados | Tom, Jerry y Spike | WB Kids\n",
      "Masha y el Oso - El Hallazgo 🐧 (Capítulo 23)\n",
      "Tú Y Yo - Adexe & Nau (Videoclip Oficial)\n",
      "Teresa y Robinson se van \"con calma\" | Las Vega's\n",
      "Ricardo Montaner - ¿Qué Vas a Hacer? (Official Video - Protagonizado por Lali y J Balvin)\n",
      "ANTEK KRÓLIKOWSKI MNIE ZABIJE | Magic of Y #Iluzjonista\n",
      "Luis Fonsi - Despacito ft. Daddy Yankee\n",
      "CRUZE CABALLO FRISIAN Y YEGUA QUARABE\n",
      "Luis Miguel - \"Y\" (Video Oficial) \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " M & S 2018年12月29日直播\n",
      "Inge sa chce vyspať s Michalom (PANELÁK)\n",
      "Wegen S*x im Auto Rücken gebrochen? | 112 - Rettung in letzter Minute | SAT.1 TV\n",
      "Los Angeles Fashion Week S/S 19  - Art Hearts Fashion - The Black Tape Project | FashionTV | FTV\n",
      "Search the letter 'S' in Wailing Woods – LOCATION WEEK 4 CHALLENGE Fortnite Season 7\n",
      "OTECKOVIA - Lucia s Vladom sa už nedokážu ovládať\n",
      "Ceremony held to reconnect N and S Korea - BBC News\n",
      "BAFASHWE NIMBWA BARIGUKORA IBIDAKORWA MU BIHURU\n",
      "Blend S『ブレンド・S』OP / Opening Full - Bon Appétit♡S (ぼなぺてぃーと♡S)\n",
      "DIE ANTWOORD - BABY'S ON FIRE (OFFICIAL) \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " K/DA - POP/STARS (ft Madison Beer, (G)I-DLE, Jaira Burns) | Official Music Video - League of Legends\n",
      "Marshmello ft. Bastille - Happier (Official Music Video)\n",
      "Marvel Studios' Avengers - Official Trailer\n",
      "Real Life Trick Shots | Dude Perfect\n",
      "Real Life Trick Shots 2 | Dude Perfect\n",
      "Steve Aoki - Waste It On Me feat. BTS (Lyric Video) [Ultra Music]\n",
      "Annoying Customers\n",
      "Lauv - I Like Me Better [Official Audio]\n",
      "Ryan Reynolds & Jake Gyllenhaal Answer the Web's Most Searched Questions | WIRED\n",
      "Ice Cube, Kevin Hart, And Conan Share A Lyft Car \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " Plan B - Fanatica Sensual Official Video\n",
      "Cher Lloyd - Want U Back (MattyBRaps Cover)\n",
      "Plan B - Choca\n",
      "Plan B - Candy\n",
      "Plan B   Fanatica Sensual  - Alvin y las ardillas \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " RUSH E\n",
      "''E'' MEME COMPILATION\n",
      "Matt Mason - \"E\" - (OFFICIAL Lyric video)\n",
      "[2018 LoL KeSPA Cup] 2Round Ro8 Group C, D (AF-GRF, DWG-SKT)\n",
      "e\n",
      "Lord Farquaad 'E' Memes Explained\n",
      "Netanyahu chega ao Brasil e Bolsonaro promete grande parceria com Israel\n",
      "E \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " RUSH E\n",
      "''E'' MEME COMPILATION\n",
      "Lord Farquaad 'E' Memes Explained\n",
      "[2018 LoL KeSPA Cup] 2Round Ro8 Group C, D (AF-GRF, DWG-SKT)\n",
      "Matt Mason - \"E\" - (OFFICIAL Lyric video)\n",
      "e\n",
      "E\n",
      "Rob Kardashian Makes Rare Appearance | E! News \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " QUAND TU N'AS PAS DE CERVEAU ! #11 😂\n",
      "Zap Koreus n°307\n",
      "NTV - Canlı Yayın ᴴᴰ\n",
      "[N’-76] TEN’s ‘Taki Taki’ & ‘HUMBLE.’ Choreography Behind\n",
      "JE N'EN PEUX PLUS !\n",
      "【DEEMO(switch版)＋雑談】音楽を聴いてまったり雑談(n*´ω`*n)【アイドル部】\n",
      "\"Il n'y a aucune possibilité pour Macky Sall & ses magistrats d’invalider la candidature de Khalifa\"\n",
      "'S' और 'N' की जोड़ी किसके साथ बनती है 😍| 'S' AND 'N' People Match With Whom\n",
      "Daniel Wellington Launch | MIHLALI N \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " K/DA - POP/STARS (ft Madison Beer, (G)I-DLE, Jaira Burns) | Official Music Video - League of Legends\n",
      "Marshmello ft. Bastille - Happier (Official Music Video)\n",
      "Marvel Studios' Avengers - Official Trailer\n",
      "Real Life Trick Shots | Dude Perfect\n",
      "Real Life Trick Shots 2 | Dude Perfect\n",
      "Steve Aoki - Waste It On Me feat. BTS (Lyric Video) [Ultra Music]\n",
      "Annoying Customers\n",
      "Lauv - I Like Me Better [Official Audio]\n",
      "Ryan Reynolds & Jake Gyllenhaal Answer the Web's Most Searched Questions | WIRED\n",
      "Ice Cube, Kevin Hart, And Conan Share A Lyft Car \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " Crazy Frog - Axel F\n",
      "Funny indian Videos P Whatsapp Status With F ull of Comedy I Bet , You Cant Hold Your Laugh!\n",
      "i \"F\" emoted on every kill in fortnite... (new toxic emote)\n",
      "Paloma Ford - W.E.T. (Official Video)\n",
      "F%CK ThAnoS\n",
      "波音搶訂單! 五角大廈傳要買12架F-15X戰機 《9點換日線》2018.12.28\n",
      "【MAD】「F」マキシマム ザ ホルモン × DRAGON BALL Z 【再アップ】 \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " r/iamverysmart Best Posts #5\n",
      "r/woooosh Best Posts #2\n",
      "r/assholedesign Top Posts of All Time\n",
      "r/comedycemetery Top Posts\n",
      "r/ihadastroke Best Posts\n",
      "HtFR (feat.大喜) - 《R.》 歌詞版\n",
      "r/iamverybadass Top Posts of All Time\n",
      "r/gatekeeping | REAL MOMS ONLY | Reddit Cringe \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " Kendrick Lamar - i (Official Video)\n",
      "TAEYEON 태연 'I (feat. Verbal Jint)' MV\n",
      "Kanye West & Lil Pump ft. Adele Givens - \"I Love It\" (Official Music Video)\n",
      "I Hindi Movie 2015\n",
      "Auli'i Cravalho - How Far I'll Go\n",
      "Cardi B, Bad Bunny & J Balvin - I Like It [Official Music Video]\n",
      "The Gummy Bear Song - Long English Version\n",
      "I Tamil Full Movie (2015) - Vikram, Amy Jackson - Shankar's I (Ai)\n",
      "Bruno Mars - That’s What I Like [Official Video]\n",
      "G-Eazy & Halsey - Him & I (Official Music Video) \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " RUSH E\n",
      "''E'' MEME COMPILATION\n",
      "Lord Farquaad 'E' Memes Explained\n",
      "[2018 LoL KeSPA Cup] 2Round Ro8 Group C, D (AF-GRF, DWG-SKT)\n",
      "Matt Mason - \"E\" - (OFFICIAL Lyric video)\n",
      "e\n",
      "E\n",
      "Rob Kardashian Makes Rare Appearance | E! News \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " Daniel Wellington Launch | MIHLALI N\n",
      "[N’-76] TEN’s ‘Taki Taki’ & ‘HUMBLE.’ Choreography Behind\n",
      "NTV - Canlı Yayın ᴴᴰ\n",
      "【DEEMO(switch版)＋雑談】音楽を聴いてまったり雑談(n*´ω`*n)【アイドル部】\n",
      "[모트라인] N의 감성을 입혔다! 현대 i30 N라인 리뷰 1부 (일반 도로 주행)\n",
      "Migos vs. Bone thugs n harmony getting out of control full tea\n",
      "HBB SYAIKHON (WAN SECHAN) HABIB MAJDZUB  N MAJELIS AL-BAHAR DUKUNG PRABOWO-SANDI;PILPRES 2019;JOKOWI\n",
      "'S' और 'N' की जोड़ी किसके साथ बनती है 😍| 'S' AND 'N' People Match With Whom\n",
      "\"TROUVER LA LETTRE N SOUS UN LAC GELÉ\" DEFI SEMAINE 4 sur FORTNITE BATTLE ROYALE ! \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " Ludacris - Vitamin D (feat. Ty Dolla $ign) [Official Video]\n",
      "Jaś Fasola :D\n",
      "franceinfo - DIRECT TV - actualité france et monde, interviews, documentaires et analyses\n",
      "12 Façons Risquées de Gagner Beaucoup D'argent\n",
      "CGI Animated Short : \"The D in David\" - by Michelle Yi & Yaron Farkash\n",
      "Ludacris - Vitamin D ft. Ty Dolla Sign\n",
      "My Super D: The Last Fight\n",
      "Letter D Song (Classic)\n",
      "I GAVE SOMMER RAY THE D - IMPAULSIVE EP. 3 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "Videos:\n",
      " ANH THÍCH KIỂU NÀO? | TẬP FULL Phần 2 l LOVE GAME | THỬ THÁCH HẸN HÒ\n",
      "FRANCE 24 en Direct – Info et actualités internationales en continu 24h/24\n",
      "ABC Song l How To Make Rainbow Color Foam Jiggly Clay Slime l kid songs\n",
      "KID Song l How To Make Colors Spoon Ice Jelly Slime DIY Clay Play l kid songs \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " Lele Pons y Anwar Jibawi | Recopilacion de Vines\n",
      "Tom y Jerry en Español | Compilación clásica de dibujos animados | Tom, Jerry y Spike | WB Kids\n",
      "Masha y el Oso - El Hallazgo 🐧 (Capítulo 23)\n",
      "Tú Y Yo - Adexe & Nau (Videoclip Oficial)\n",
      "Teresa y Robinson se van \"con calma\" | Las Vega's\n",
      "Ricardo Montaner - ¿Qué Vas a Hacer? (Official Video - Protagonizado por Lali y J Balvin)\n",
      "ANTEK KRÓLIKOWSKI MNIE ZABIJE | Magic of Y #Iluzjonista\n",
      "Luis Fonsi - Despacito ft. Daddy Yankee\n",
      "CRUZE CABALLO FRISIAN Y YEGUA QUARABE\n",
      "Luis Miguel - \"Y\" (Video Oficial) \n",
      "\n",
      "1000000\n",
      "Videos:\n",
      " Full Stop Punctuation\n",
      "Full Stop Punctuation\n",
      "Full Stop Punctuation.\n",
      "Full stop Punctuation\n",
      "comedy - little funny girl full stop punctuation\n",
      "He Only Wanted Onions - A Full Stop Punctuation Production \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LET'S TRY YOUTUBE SNIPPETS\n",
    "\n",
    "# Please check in this code\n",
    "# python search.py / --q = surfing / --max-results = 10 / totalResults\n",
    "\n",
    "import argparse\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import json\n",
    "\n",
    "# NOTE: To use the sample, you must provide a developer key obtained in the Google APIs Console. \n",
    "# Search for DEVELOPER_KEY in this code to find the correct place to provide that key..\n",
    "# Please ensure that you have enabled the YouTube Data API for your project from your Google account.\n",
    "\n",
    "DEVELOPER_KEY = 'AIzaSyBN0zRiSDC_IdQrYWQaTcbCheyKLRopqOA'\n",
    "YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
    "YOUTUBE_API_VERSION = 'v3'\n",
    "\n",
    "# This code collects only ids and snippets of videos \n",
    "# (as chennels and playlists didn't include any suitable information for us)\n",
    "\n",
    "def youtube_search(searchItem, maxResults):\n",
    "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "    # Call the search.list method to retrieve results matching the specified query term (at the end of the code).\n",
    "    search_response = youtube.search().list(q=searchItem, part='id,snippet', maxResults=maxResults).execute()\n",
    "\n",
    "    videos = []     #  channels = []  #  playlists = []\n",
    "\n",
    "    # Add each result to the appropriate list, and then display the lists of matching videos. (channels, and playlists)\n",
    "    \n",
    "#print(str(search_response.get('pageInfo')), '\\n\\n')\n",
    "#print(search_response)\n",
    "    \n",
    "    info = search_response.get('pageInfo', [])\n",
    "    total = info.get('totalResults')\n",
    "    print(total)\n",
    "    for search_result in search_response.get(\"items\", []):\n",
    "        if search_result['id']['kind'] == 'youtube#video':\n",
    "              videos.append('%s' % (search_result['snippet']['title']))\n",
    "\n",
    "        # Code below saved for later use\n",
    "        #                              search_result['id']['videoId']))\n",
    "        #      elif search_result['id']['kind'] == 'youtube#channel':\n",
    "        #            channels.append('%s (%s)' % (search_result['snippet']['title'],\n",
    "        #                                 search_result['id']['channelId']))\n",
    "        #      elif search_result['id']['kind'] == 'youtube#playlist':\n",
    "        #            playlists.append('%s (%s)' % (search_result['snippet']['title'],\n",
    "        #                                  search_result['id']['playlistId']))\n",
    "\n",
    "    print ('Videos:\\n', '\\n'.join(videos), '\\n')\n",
    "         #   print ('Channels:\\n', '\\n'.join(channels), '\\n')\n",
    "         #   print ('Playlists:\\n', '\\n'.join(playlists), '\\n')\n",
    "\n",
    "sentence = \"AI and humans have always been friendly.\"\n",
    "\n",
    "for youtube_doc in sentence:\n",
    "    try:\n",
    "                        # youtube_search(args)\n",
    "                        # searchItem = \"How to build up super snippets for new web content?\"\n",
    "        maxResults = 10\n",
    "        youtube_search(youtube_doc, maxResults)\n",
    "    except HttpError as e:\n",
    "        print ('An HTTP error %d occurred:\\n%s' % (e.resp.status, e.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Design a simple GUI interface that allows you to demonstrate your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:generally no error happen. No input variable yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import * \n",
    "\n",
    "mainWindow = Tk()\n",
    "\n",
    "mainWindow.title(\"Welcome to project number 18\")\n",
    "\n",
    "mainWindow.geometry('350x200')\n",
    "\n",
    "# State\n",
    "\n",
    "numberOfEnteredSentences = 0\n",
    "\n",
    "S1 = []\n",
    "\n",
    "S2 = []\n",
    "\n",
    "\n",
    "# Insert sentences layout string variables\n",
    "\n",
    "instructionsLabelStringVariable = StringVar()\n",
    "\n",
    "instructionsLabelStringVariable.set(\"Enter first sentence\")\n",
    "\n",
    "resultLabelStringVariable = StringVar()\n",
    "\n",
    "sentenceInputEntryStringVariable = StringVar() # This variable will contain what ever the user has typed into the input field\n",
    "\n",
    "result = StringVar()\n",
    "\n",
    "\n",
    "# Results layout string variables\n",
    "\n",
    "resultsFirstSentenceStringVariable = StringVar()\n",
    "\n",
    "resultsFirstSentenceStringVariable.set(\"First sentence\")\n",
    "\n",
    "resultsSecondSentenceStringVariable = StringVar()\n",
    "\n",
    "resultsSecondSentenceStringVariable.set(\"Second sentence\")\n",
    "\n",
    "resultsFirstSnippetsStringVariable = StringVar()\n",
    "\n",
    "resultsFirstSnippetsStringVariable.set(\"Here be first snippet\")\n",
    "\n",
    "resultsSecondSnippetsStringVariable = StringVar()\n",
    "\n",
    "resultsSecondSnippetsStringVariable.set(\"Here be second snippet\")\n",
    "\n",
    "resultsWordNetSimilarityStringVariable  = StringVar()\n",
    "\n",
    "resultsWordNetSimilarityStringVariable.set(\"Over 9000\")\n",
    "\n",
    "resultsWikiSimilarityStringVariable  = StringVar()\n",
    "\n",
    "resultsWikiSimilarityStringVariable.set(\"Over 9001\")\n",
    "\n",
    "# Callbacks\n",
    "\n",
    "def submitCallback(): # This function will get called when user presses the \n",
    "\n",
    "    global numberOfEnteredSentences, S1, S2\n",
    "\n",
    "    numberOfEnteredSentences = numberOfEnteredSentences + 1\n",
    "\n",
    "    enteredSentence = sentenceInputEntryStringVariable.get()\n",
    "\n",
    "    if numberOfEnteredSentences == 1:\n",
    "\n",
    "        S1 = enteredSentence\n",
    "\n",
    "    elif numberOfEnteredSentences == 2:\n",
    "\n",
    "        S2 = enteredSentence\n",
    "\n",
    "\n",
    "    sentenceInputEntryStringVariable.set(\"\") # Clear the input field\n",
    "\n",
    "    instructionsLabelStringVariable.set(\"Enter second sentence\") # Update instructions\n",
    "\n",
    "    if numberOfEnteredSentences == 2:\n",
    "\n",
    "# Line below is mock functionality: we should query the api here and then update the result resultLabelStringVariable\n",
    "\n",
    "        result.set(\"test: \")\n",
    "\n",
    "        resultLabelStringVariable.set(\"blaablaa\")\n",
    "\n",
    "        removeInsertSentencesLayout()\n",
    "\n",
    "        displayResultsLayout()\n",
    "\n",
    "# Insert sentences layout elements\n",
    "\n",
    "instructionLabel = Label(mainWindow, text=\"Enter the first sentence:\", textvariable=instructionsLabelStringVariable, font=(\"Arial Bold\", 20))\n",
    "\n",
    "resultLabel2 = Label(mainWindow, textvariable=result)\n",
    "\n",
    "resultLabel = Label(mainWindow, text=\"Result:\", textvariable=resultLabelStringVariable)\n",
    "\n",
    "sentenceInputEntry = Entry(mainWindow, textvariable=sentenceInputEntryStringVariable)\n",
    "\n",
    "submitButton = Button(mainWindow, text=\"Submit\", width=10, command=submitCallback)\n",
    "\n",
    "\n",
    "def displayInsertSentencesLayout():\t\n",
    "\n",
    "    instructionLabel.grid(column=0, row=0)\n",
    "\n",
    "    #instructionLabel.pack()\n",
    "\n",
    "    sentenceInputEntry.grid(column=1, row=0)\n",
    "\n",
    "    sentenceInputEntry.focus()\n",
    "\n",
    "    #sentenceInputEntry.pack()\n",
    "\n",
    "    #resultLabel2.pack()\n",
    "\n",
    "    resultLabel2.grid(column=2, row=0)\n",
    "\n",
    "    resultLabel.grid(column=3, row=0)\n",
    "\n",
    "    #resultLabel.pack()\n",
    "\n",
    "    submitButton.grid(column=4, row=0)\n",
    "\n",
    "    #submitButton.pack()\n",
    "\n",
    "def removeInsertSentencesLayout():\n",
    "\n",
    "    instructionLabel.pack_forget()\n",
    "\n",
    "    sentenceInputEntry.pack_forget()\n",
    "\n",
    "    resultLabel2.pack_forget()\n",
    "\n",
    "    resultLabel.pack_forget()\n",
    "\n",
    "    submitButton.pack_forget()\n",
    "\n",
    "\n",
    "def displayResultsLayout():\n",
    "\n",
    "    # Update labels\n",
    "\n",
    "    resultsFirstSentenceStringVariable.set(S1)\n",
    "\n",
    "    resultsSecondSentenceStringVariable.set(S2)\n",
    "\n",
    "\n",
    "    # Layout elements\n",
    "\n",
    "    Label(mainWindow, text=\"S1\", font=(\"Arial Bold\", 20)).grid(row=0, column=0)\n",
    "\n",
    "    Label(mainWindow, text=\"S2\", font=(\"Arial Bold\", 20)).grid(row=0, column=1)\n",
    "\n",
    "    Label(mainWindow, textvariable=resultsFirstSentenceStringVariable).grid(row=1, column=0)\n",
    "\n",
    "    Label(mainWindow, textvariable=resultsSecondSentenceStringVariable).grid(row=1, column=1)\n",
    "\n",
    "    Label(mainWindow, text=\"Snippets\", font=(\"Arial Bold\", 20)).grid(row=2, column=0)\n",
    "\n",
    "    Label(mainWindow, textvariable=resultsFirstSnippetsStringVariable).grid(row=3, column=0)\n",
    "\n",
    "    Label(mainWindow, textvariable=resultsSecondSnippetsStringVariable).grid(row=3, column=1)\n",
    "\n",
    "    Label(mainWindow, text=\"WordNet similarity\", font=(\"Arial Bold\", 20)).grid(row=4, column=0)\n",
    "\n",
    "    Label(mainWindow, textvariable=resultsWordNetSimilarityStringVariable).grid(row=5, column=0)\n",
    "\n",
    "    Label(mainWindow, text=\"Wiki similarity\", font=(\"Arial Bold\", 20)).grid(row=6, column=0)\n",
    "\n",
    "    Label(mainWindow, textvariable=resultsWordNetSimilarityStringVariable).grid(row=7, column=0)\n",
    "\n",
    "displayInsertSentencesLayout()\n",
    "\n",
    "mainWindow.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
